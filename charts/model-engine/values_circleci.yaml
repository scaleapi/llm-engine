# This is a YAML-formatted file.

celery_enable_sha256: null
celery_broker_type_redis: null
debug_mode: null

#uncomment below if using a container image that requires nonroot (ex. chainguard)
# podSecurityContext:
#   runAsUser: 65532     
#   runAsGroup: 65532    
#   runAsNonRoot: true
#   fsGroup: 65532  

# containerSecurityContext:
#   allowPrivilegeEscalation: false
#   readOnlyRootFilesystem: false
#   capabilities:
#     drop:
#       - ALL  

replicaCount:
  gateway: 1
  cacher: 1
  builder: 1

balloons:
  - acceleratorName: nvidia-ampere-a10
    replicaCount: 0
  - acceleratorName: nvidia-ampere-a100
    replicaCount: 0
  - acceleratorName: cpu
    replicaCount: 0
  - acceleratorName: nvidia-tesla-t4
    replicaCount: 0
  - acceleratorName: nvidia-hopper-h100
    replicaCount: 0

# tag needs to be set dynamically every time. Usually it is set to the SHA1 hash of the git
# commit from which the image was built.
# tag:
context: circleci

# Enable SHA256 checksums for Celery S3 backend to avoid MD5 issues
celery_enable_sha256: true

image:
  gatewayRepository: model-engine
  builderRepository: model-engine
  cacherRepository: model-engine
  forwarderRepository: model-engine
  pullPolicy: IfNotPresent

# serviceIdentifier:

secrets:
  kubernetesDatabaseSecretName: model-engine-postgres-credentials


service:
  type: ClusterIP
  port: 80

virtualservice:
  enabled: true
  annotations: { }
  hostDomains:
    - example.com
  gateways:
    - default/internal-gateway

hostDomain:
  prefix: http://

destinationrule:
  enabled: true
  annotations: { }

autoscaling:
  horizontal:
    enabled: false
    minReplicas: 1
    maxReplicas: 10
    targetConcurrency: 30
  vertical:
    enabled: false
    minAllowed:
      cpu: 100m
      memory: 128Mi
    maxAllowed:
      cpu: 10
      memory: 8Gi
    updateMode: Auto
  prewarming:
    enabled: false

celery_autoscaler:
  enabled: false

podDisruptionBudget:
  enabled: true
  minAvailable: 1

resources:
  requests:
    cpu: 2

nodeSelector: null

balloonNodeSelector: null

tolerations: [ ]

affinity: { }

config:
  values:
    infra:
      cloud_provider: aws
      k8s_cluster_name: minikube
      dns_host_domain: localhost
      default_region: us-west-2
      ml_account_id: "$CIRCLECI_AWS_ACCOUNT_ID"
      docker_repo_prefix: "$CIRCLECI_AWS_ACCOUNT_ID.dkr.ecr.us-west-2.amazonaws.com"
      redis_host: redis-message-broker-master.default
      s3_bucket: "$CIRCLECI_AWS_S3_BUCKET"
      profile_ml_worker: "default"
      profile_ml_inference_worker: "default"
    launch:
      # Endpoint config
      # K8s namespace the endpoints will be created in
      endpoint_namespace: model-engine
      model_primitive_host: none

      # Asynchronous endpoints
      sqs_profile: default
      sqs_queue_policy_template: >
        {
            "Version": "2012-10-17",
            "Id": "__default_policy_ID",
            "Statement": [
              {
                "Sid": "__owner_statement",
                "Effect": "Allow",
                "Principal": {
                  "AWS": "arn:aws:iam::$CIRCLECI_AWS_ACCOUNT_ID:root"
                },
                "Action": "sqs:*",
                "Resource": "arn:aws:sqs:us-west-2:$CIRCLECI_AWS_ACCOUNT_ID:${queue_name}"
              },
              {
                "Effect": "Allow",
                "Principal": {
                  "AWS": "arn:aws:iam::$CIRCLECI_AWS_ACCOUNT_ID:role/default"
                },
                "Action": "sqs:*",
                "Resource": "arn:aws:sqs:us-west-2:$CIRCLECI_AWS_ACCOUNT_ID:${queue_name}"
              },
              {
                "Effect": "Allow",
                "Principal": {
                  "AWS": "arn:aws:iam::$CIRCLECI_AWS_ACCOUNT_ID:role/ml_llm_engine"
                },
                "Action": "sqs:*",
                "Resource": "arn:aws:sqs:us-west-2:$CIRCLECI_AWS_ACCOUNT_ID:${queue_name}"
              }
            ]
          }
      sqs_queue_tag_template: >
        {
          "Spellbook-Serve-Endpoint-Id": "${endpoint_id}",
          "Spellbook-Serve-Endpoint-Name": "${endpoint_name}",
          "Spellbook-Serve-Endpoint-Created-By": "${endpoint_created_by}"
        }

      billing_queue_arn: none
      cache_redis_aws_url: redis://redis-message-broker-master.default/15
      cloud_file_llm_fine_tune_repository: "s3://$CIRCLECI_AWS_S3_BUCKET/fine_tune_repository"
      dd_trace_enabled: false
      istio_enabled: true
      sensitive_log_mode: false
      tgi_repository: "text-generation-inference"
      vllm_repository: "vllm"
      lightllm_repository: "lightllm"
      tensorrt_llm_repository: "tensorrt-llm"
      batch_inference_vllm_repository: "llm-engine/batch-infer-vllm"
      user_inference_base_repository: "launch/inference"
      user_inference_pytorch_repository: "hosted-model-inference/async-pytorch"
      user_inference_tensorflow_repository: "hosted-model-inference/async-tensorflow-cpu"
      docker_image_layer_cache_repository: "kaniko-cache"
      hf_user_fine_tuned_weights_prefix: "s3://$CIRCLECI_AWS_S3_BUCKET/model-weights"

# Service Account
serviceAccount:
  annotations:
    "eks.amazonaws.com/role-arn": arn:aws:iam::$CIRCLECI_AWS_ACCOUNT_ID:role/default
    "helm.sh/hook": pre-install,pre-upgrade
    "helm.sh/hook-weight": "-2"
  namespaces:
    - default
    - model-engine

aws:
  configMap:
    name: default-config
    create: false
    mountPath: /opt/.aws/config
  profileName: default
  s3WriteProfileName: default

forwarder:
  forceUseIPv4: true

triton:
  image:
    repository: nvidia/tritonserver
    tag: latest

serviceTemplate:
  securityContext:
    capabilities:
      drop:
        - all
  mountInfraConfig: true
  serviceAccountName: default
  awsConfigMapName: default-config

imageCache:
  devices:
    - name: cpu
      nodeSelector:
        cpu-only: "true"
    - name: a10
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-ampere-a10
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: a100
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-ampere-a100
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: t4
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-tesla-t4
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
    - name: h100
      nodeSelector:
        k8s.amazonaws.com/accelerator: nvidia-hopper-h100
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

celeryBrokerType: redis

datadog:
  enabled: false

recommendedHardware:
  byGpuMemoryGb:
    - gpu_memory_le: 24
      cpus: 10
      gpus: 1
      memory: 24Gi
      storage: 80Gi
      gpu_type: nvidia-ampere-a10
      nodes_per_worker: 1
    - gpu_memory_le: 48
      cpus: 20
      gpus: 2
      memory: 48Gi
      storage: 80Gi
      gpu_type: nvidia-ampere-a10
      nodes_per_worker: 1
    - gpu_memory_le: 96
      cpus: 40
      gpus: 4
      memory: 96Gi
      storage: 96Gi
      gpu_type: nvidia-ampere-a10
      nodes_per_worker: 1
    - gpu_memory_le: 180
      cpus: 20
      gpus: 2
      memory: 160Gi
      storage: 160Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 1
    - gpu_memory_le: 320
      cpus: 40
      gpus: 4
      memory: 320Gi
      storage: 320Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 1
    - gpu_memory_le: 640
      cpus: 80
      gpus: 8
      memory: 800Gi
      storage: 640Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 1
    - gpu_memory_le: 1280
      cpus: 80
      gpus: 8
      memory: 800Gi
      storage: 900Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 2
  byModelName:
    - name: llama-3-8b-instruct-262k
      cpus: 20
      gpus: 2
      memory: 40Gi
      storage: 40Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 1
    - name: deepseek-coder-v2
      cpus: 160
      gpus: 8
      memory: 800Gi
      storage: 640Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 1
    - name: deepseek-coder-v2-instruct
      cpus: 160
      gpus: 8
      memory: 800Gi
      storage: 640Gi
      gpu_type: nvidia-hopper-h100
      nodes_per_worker: 1