
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="The open source engine for fine-tuning large language models.">
      
      
      
      
        <link rel="prev" href="../../guides/self_hosting/">
      
      
        <link rel="next" href="../data_types/">
      
      
      <link rel="icon" href="https://raw.githubusercontent.com/scaleapi/llm-engine/main/docs/_static/favicon-32x32.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.11">
    
    
      
        <title>API Reference - LLM Engine</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.4af4bdda.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CIBM+Plex+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"IBM Plex Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../stylesheets/index.css">
    
      <link rel="stylesheet" href="../../assets/css/extra.css">
    
      <link rel="stylesheet" href="../../assets/css/neoteroi.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      
  


  
  

<script id="__analytics">function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-N54ZLW5PGC"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-N54ZLW5PGC",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-N54ZLW5PGC",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script>
  
    <script>"undefined"!=typeof __md_analytics&&__md_analytics()</script>
  

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#python-client-api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLM Engine" class="md-header__button md-logo" aria-label="LLM Engine" data-md-component="logo">
      
  <img src="https://raw.githubusercontent.com/scaleapi/llm-engine/main/docs/_static/favicon-32x32.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM Engine
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API Reference
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="white" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/scaleapi/llm-engine" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    llm-engine
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM Engine" class="md-nav__button md-logo" aria-label="LLM Engine" data-md-component="logo">
      
  <img src="https://raw.githubusercontent.com/scaleapi/llm-engine/main/docs/_static/favicon-32x32.png" alt="logo">

    </a>
    LLM Engine
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/scaleapi/llm-engine" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    llm-engine
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Introduction
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../getting_started/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Getting Started
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_zoo/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Model Zoo
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Guides
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Guides
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/completions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Completions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/fine_tuning/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Fine-tuning
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/rate_limits/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Rate limits
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../guides/self_hosting/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Self Hosting
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    API
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            API
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#llmengine.Completion" class="md-nav__link">
    <span class="md-ellipsis">
      Completion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Completion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.Completion.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Completion.acreate" class="md-nav__link">
    <span class="md-ellipsis">
      acreate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Completion.batch_create" class="md-nav__link">
    <span class="md-ellipsis">
      batch_create
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llmengine.FineTune" class="md-nav__link">
    <span class="md-ellipsis">
      FineTune
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FineTune">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.get" class="md-nav__link">
    <span class="md-ellipsis">
      get
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.get_events" class="md-nav__link">
    <span class="md-ellipsis">
      get_events
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.cancel" class="md-nav__link">
    <span class="md-ellipsis">
      cancel
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llmengine.Model" class="md-nav__link">
    <span class="md-ellipsis">
      Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.get" class="md-nav__link">
    <span class="md-ellipsis">
      get
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.download" class="md-nav__link">
    <span class="md-ellipsis">
      download
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llmengine.File" class="md-nav__link">
    <span class="md-ellipsis">
      File
    </span>
  </a>
  
    <nav class="md-nav" aria-label="File">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.File.upload" class="md-nav__link">
    <span class="md-ellipsis">
      upload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.get" class="md-nav__link">
    <span class="md-ellipsis">
      get
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.download" class="md-nav__link">
    <span class="md-ellipsis">
      download
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../data_types/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Data Type Reference
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../error_handling/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Error handling
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../integrations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Integrations
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../pricing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Pricing
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../contributing/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#llmengine.Completion" class="md-nav__link">
    <span class="md-ellipsis">
      Completion
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Completion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.Completion.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Completion.acreate" class="md-nav__link">
    <span class="md-ellipsis">
      acreate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Completion.batch_create" class="md-nav__link">
    <span class="md-ellipsis">
      batch_create
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llmengine.FineTune" class="md-nav__link">
    <span class="md-ellipsis">
      FineTune
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FineTune">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.get" class="md-nav__link">
    <span class="md-ellipsis">
      get
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.get_events" class="md-nav__link">
    <span class="md-ellipsis">
      get_events
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.FineTune.cancel" class="md-nav__link">
    <span class="md-ellipsis">
      cancel
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llmengine.Model" class="md-nav__link">
    <span class="md-ellipsis">
      Model
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.create" class="md-nav__link">
    <span class="md-ellipsis">
      create
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.get" class="md-nav__link">
    <span class="md-ellipsis">
      get
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.update" class="md-nav__link">
    <span class="md-ellipsis">
      update
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.Model.download" class="md-nav__link">
    <span class="md-ellipsis">
      download
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#llmengine.File" class="md-nav__link">
    <span class="md-ellipsis">
      File
    </span>
  </a>
  
    <nav class="md-nav" aria-label="File">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#llmengine.File.upload" class="md-nav__link">
    <span class="md-ellipsis">
      upload
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.get" class="md-nav__link">
    <span class="md-ellipsis">
      get
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.download" class="md-nav__link">
    <span class="md-ellipsis">
      download
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.list" class="md-nav__link">
    <span class="md-ellipsis">
      list
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llmengine.File.delete" class="md-nav__link">
    <span class="md-ellipsis">
      delete
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="python-client-api-reference">🐍 Python Client API Reference<a class="headerlink" href="#python-client-api-reference" title="Permanent link">&para;</a></h1>


<div class="doc doc-object doc-class">



<h2 id="llmengine.Completion" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Completion</span>


<a href="#llmengine.Completion" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="llmengine.api_engine.APIEngine">APIEngine</span></code></p>


        <p>Completion API. This API is used to generate text completions.</p>
<p>Language models are trained to understand natural language and predict text outputs as a response to
their inputs. The inputs are called <em>prompts</em> and the outputs are referred to as <em>completions</em>.
LLMs take the input prompts and chunk them into smaller units called <em>tokens</em> to process and generate
language. Tokens may include trailing spaces and even sub-words; this process is language dependent.</p>
<p>The Completion API can be run either synchronous or asynchronously (via Python <code>asyncio</code>).
For each of these modes, you can also choose whether to stream token responses or not.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="llmengine.Completion.create" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">create</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Completion.create" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>create(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    model: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    prompt: <span style="color: #008000">str</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    max_new_tokens: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">20</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    temperature: <span style="color: #008000">float</span> <span style="color: #666">=</span> <span style="color: #666">0.2</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    stop_sequences: Optional[List[<span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    return_token_log_probs: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    presence_penalty: Optional[<span style="color: #008000">float</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    frequency_penalty: Optional[<span style="color: #008000">float</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    top_k: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    top_p: Optional[<span style="color: #008000">float</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    include_stop_str_in_output: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    guided_json: Optional[Dict[<span style="color: #008000">str</span>, Any]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    guided_regex: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    guided_choice: Optional[List[<span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    guided_grammar: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    timeout: <span style="color: #008000">int</span> <span style="color: #666">=</span> COMPLETION_TIMEOUT,
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    stream: <span style="color: #008000">bool</span> <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    <span style="color: #666">**</span>kwargs
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>) <span style="color: #666">-&gt;</span> Union[
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    CompletionSyncResponse,
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    Iterator[CompletionStreamResponse],
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>]
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Creates a completion for the provided prompt and parameters synchronously.</p>
<p>This API can be used to get the LLM to generate a completion <em>synchronously</em>.
It takes as parameters the <code>model</code> (<a href="../../model_zoo">see Model Zoo</a>) and the <code>prompt</code>.
Optionally it takes <code>max_new_tokens</code>, <code>temperature</code>, <code>timeout</code> and <code>stream</code>.
It returns a
<a href="../../api/data_types/#llmengine.CompletionSyncResponse">CompletionSyncResponse</a>
if <code>stream=False</code> or an async iterator of
<a href="../../api/data_types/#llmengine.CompletionStreamResponse">CompletionStreamResponse</a>
with <code>request_id</code> and <code>outputs</code> fields.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the model to use. See <a href="../../model_zoo">Model Zoo</a> for a list of Models that are supported.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prompt</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The prompt to generate completions for, encoded as a string.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum number of tokens to generate in the completion.</p>
<p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See
<a href="../../model_zoo">Model Zoo</a> for information on each supported model's context length.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>temperature</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>What sampling temperature to use, in the range <code>[0, 1]</code>. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
When temperature is 0 <a href="https://huggingface.co/docs/transformers/generation_strategies#greedy-search">greedy search</a> is used.</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_sequences</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>One or more sequences where the API will stop generating tokens for the current completion.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_token_log_probs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="bool">bool</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to return the log probabilities of generated tokens.
When True, the response will include a list of tokens and their log probabilities.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>presence_penalty</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Only supported in vllm, lightllm
Penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
https://platform.openai.com/docs/guides/gpt/parameter-details
Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frequency_penalty</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Only supported in vllm, lightllm
Penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
https://platform.openai.com/docs/guides/gpt/parameter-details
Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>top_k</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Integer that controls the number of top tokens to consider.
Range: [1, infinity). -1 means consider all tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>top_p</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Float that controls the cumulative probability of the top tokens to consider.
Range: (0.0, 1.0]. 1.0 means consider all tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>include_stop_str_in_output</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="bool">bool</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include the stop sequence in the output. Default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_json</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will follow the JSON schema.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_regex</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will follow the regex pattern.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_choice</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will be exactly one of the choices.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_grammar</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will follow the context-free grammar provided.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>timeout</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p>
              </div>
            </td>
            <td>
                  <code><span title="llmengine.completion.COMPLETION_TIMEOUT">COMPLETION_TIMEOUT</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stream</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to stream the response. If true, the return type is an
<code>Iterator[CompletionStreamResponse]</code>. Otherwise, the return type is a <code>CompletionSyncResponse</code>.
When streaming, tokens will be sent as data-only <a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format">server-sent events</a>.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>response</code></td>            <td>
                  <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="llmengine.data_types.CompletionSyncResponse" href="../data_types/#llmengine.CompletionSyncResponse">CompletionSyncResponse</a>, <span title="typing.AsyncIterable">AsyncIterable</span>[<a class="autorefs autorefs-internal" title="llmengine.data_types.CompletionStreamResponse" href="../data_types/#llmengine.CompletionStreamResponse">CompletionStreamResponse</a>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The generated response (if <code>stream=False</code>) or iterator of response chunks (if <code>stream=True</code>)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.Completion.create--__tabbed_1_1" name="llmengine.Completion.create--__tabbed_1" type="radio" /><input id="llmengine.Completion.create--__tabbed_1_2" name="llmengine.Completion.create--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Completion.create--__tabbed_1_1">Synchronous completion without token streaming in Python</label><label for="llmengine.Completion.create--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> Completion<span style="color: #666">.</span>create(
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    prompt<span style="color: #666">=</span><span style="color: #BA2121">&quot;Hello, my name is&quot;</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    max_new_tokens<span style="color: #666">=10</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    temperature<span style="color: #666">=0.2</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>)
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;8bbd0e83-f94c-465b-a12b-aabad45750a9&quot;</span>,
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">        </span><span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;_______ and I am a _______&quot;</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">        </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">10</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">    </span>}
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>}
</code></pre></div>
</div>
</div>
</div>
<p>Token streaming can be used to reduce <em>perceived</em> latency for applications. Here is how applications can use streaming:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="llmengine.Completion.create--__tabbed_2_1" name="llmengine.Completion.create--__tabbed_2" type="radio" /><input id="llmengine.Completion.create--__tabbed_2_2" name="llmengine.Completion.create--__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="llmengine.Completion.create--__tabbed_2_1">Synchronous completion with token streaming in Python</label><label for="llmengine.Completion.create--__tabbed_2_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>stream <span style="color: #666">=</span> Completion<span style="color: #666">.</span>create(
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    prompt<span style="color: #666">=</span><span style="color: #BA2121">&quot;why is the sky blue?&quot;</span>,
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    max_new_tokens<span style="color: #666">=5</span>,
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    temperature<span style="color: #666">=0.2</span>,
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    stream<span style="color: #666">=</span><span style="color: #008000; font-weight: bold">True</span>,
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>)
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a><span style="color: #008000; font-weight: bold">for</span> response <span style="color: #A2F; font-weight: bold">in</span> stream:
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>    <span style="color: #008000; font-weight: bold">if</span> response<span style="color: #666">.</span>output:
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>        <span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ebbde00c-8c31-4c03-8306-24f37cd25fa2&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;\n&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">1</span><span style="color: #BBB"> </span>}<span style="color: #BBB"> </span>}
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ebbde00c-8c31-4c03-8306-24f37cd25fa2&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;I&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">2</span><span style="color: #BBB"> </span>}<span style="color: #BBB"> </span>}
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ebbde00c-8c31-4c03-8306-24f37cd25fa2&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot; don&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">3</span><span style="color: #BBB"> </span>}<span style="color: #BBB"> </span>}
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ebbde00c-8c31-4c03-8306-24f37cd25fa2&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;’&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">4</span><span style="color: #BBB"> </span>}<span style="color: #BBB"> </span>}
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ebbde00c-8c31-4c03-8306-24f37cd25fa2&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;t&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">true</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">5</span><span style="color: #BBB"> </span>}<span style="color: #BBB"> </span>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Completion.acreate" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">acreate</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-async"><code>async</code></small>
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Completion.acreate" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>acreate(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    model: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    prompt: <span style="color: #008000">str</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    max_new_tokens: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">20</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    temperature: <span style="color: #008000">float</span> <span style="color: #666">=</span> <span style="color: #666">0.2</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    stop_sequences: Optional[List[<span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    return_token_log_probs: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    presence_penalty: Optional[<span style="color: #008000">float</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    frequency_penalty: Optional[<span style="color: #008000">float</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    top_k: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    top_p: Optional[<span style="color: #008000">float</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    include_stop_str_in_output: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    guided_json: Optional[Dict[<span style="color: #008000">str</span>, Any]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    guided_regex: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    guided_choice: Optional[List[<span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    guided_grammar: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    timeout: <span style="color: #008000">int</span> <span style="color: #666">=</span> COMPLETION_TIMEOUT,
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    stream: <span style="color: #008000">bool</span> <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    <span style="color: #666">**</span>kwargs
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>) <span style="color: #666">-&gt;</span> Union[
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    CompletionSyncResponse,
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    AsyncIterable[CompletionStreamResponse],
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>]
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Creates a completion for the provided prompt and parameters asynchronously (with <code>asyncio</code>).</p>
<p>This API can be used to get the LLM to generate a completion <em>asynchronously</em>.
It takes as parameters the <code>model</code> (<a href="../../model_zoo">see Model Zoo</a>) and the <code>prompt</code>.
Optionally it takes <code>max_new_tokens</code>, <code>temperature</code>, <code>timeout</code> and <code>stream</code>.
It returns a
<a href="../../api/data_types/#llmengine.CompletionSyncResponse">CompletionSyncResponse</a>
if <code>stream=False</code> or an async iterator of
<a href="../../api/data_types/#llmengine.CompletionStreamResponse">CompletionStreamResponse</a>
with <code>request_id</code> and <code>outputs</code> fields.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the model to use. See <a href="../../model_zoo">Model Zoo</a> for a list of Models that are supported.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>prompt</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The prompt to generate completions for, encoded as a string.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_new_tokens</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum number of tokens to generate in the completion.</p>
<p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See
<a href="../../model_zoo">Model Zoo</a> for information on each supported model's context length.</p>
              </div>
            </td>
            <td>
                  <code>20</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>temperature</code>
            </td>
            <td>
                  <code><span title="float">float</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>What sampling temperature to use, in the range <code>[0, 1]</code>. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
When temperature is 0 <a href="https://huggingface.co/docs/transformers/generation_strategies#greedy-search">greedy search</a> is used.</p>
              </div>
            </td>
            <td>
                  <code>0.2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stop_sequences</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>One or more sequences where the API will stop generating tokens for the current completion.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>return_token_log_probs</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="bool">bool</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to return the log probabilities of generated tokens.
When True, the response will include a list of tokens and their log probabilities.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>presence_penalty</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Only supported in vllm, lightllm
Penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
https://platform.openai.com/docs/guides/gpt/parameter-details
Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>frequency_penalty</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Only supported in vllm, lightllm
Penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
https://platform.openai.com/docs/guides/gpt/parameter-details
Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>top_k</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="int">int</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Integer that controls the number of top tokens to consider.
Range: [1, infinity). -1 means consider all tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>top_p</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="float">float</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Float that controls the cumulative probability of the top tokens to consider.
Range: (0.0, 1.0]. 1.0 means consider all tokens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>include_stop_str_in_output</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="bool">bool</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include the stop sequence in the output. Default to False.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_json</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.Dict">Dict</span>[<span title="str">str</span>, <span title="typing.Any">Any</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will follow the JSON schema. For examples see https://json-schema.org/learn/miscellaneous-examples.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_regex</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will follow the regex pattern.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_choice</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="typing.List">List</span>[<span title="str">str</span>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will be exactly one of the choices.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>guided_grammar</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specified, the output will follow the context-free grammar provided.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>timeout</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p>
              </div>
            </td>
            <td>
                  <code><span title="llmengine.completion.COMPLETION_TIMEOUT">COMPLETION_TIMEOUT</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stream</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to stream the response. If true, the return type is an
<code>Iterator[CompletionStreamResponse]</code>. Otherwise, the return type is a <code>CompletionSyncResponse</code>.
When streaming, tokens will be sent as data-only <a href="https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format">server-sent events</a>.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>response</code></td>            <td>
                  <code><span title="typing.Union">Union</span>[<a class="autorefs autorefs-internal" title="llmengine.data_types.CompletionSyncResponse" href="../data_types/#llmengine.CompletionSyncResponse">CompletionSyncResponse</a>, <span title="typing.AsyncIterable">AsyncIterable</span>[<a class="autorefs autorefs-internal" title="llmengine.data_types.CompletionStreamResponse" href="../data_types/#llmengine.CompletionStreamResponse">CompletionStreamResponse</a>]]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The generated response (if <code>stream=False</code>) or iterator of response chunks (if <code>stream=True</code>)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.Completion.acreate--__tabbed_1_1" name="llmengine.Completion.acreate--__tabbed_1" type="radio" /><input id="llmengine.Completion.acreate--__tabbed_1_2" name="llmengine.Completion.acreate--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Completion.acreate--__tabbed_1_1">Asynchronous completion without token streaming in Python</label><label for="llmengine.Completion.acreate--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">import</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">asyncio</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000; font-weight: bold">async</span> <span style="color: #008000; font-weight: bold">def</span><span style="color: #BBB"> </span><span style="color: #00F">main</span>():
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    response <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">await</span> Completion<span style="color: #666">.</span>acreate(
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>        model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        prompt<span style="color: #666">=</span><span style="color: #BA2121">&quot;Hello, my name is&quot;</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        max_new_tokens<span style="color: #666">=10</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>        temperature<span style="color: #666">=0.2</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    )
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>asyncio<span style="color: #666">.</span>run(main())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;9cfe4d5a-f86f-4094-a935-87f871d90ec0&quot;</span>,
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">        </span><span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;_______ and I am a _______&quot;</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">        </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">10</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">    </span>}
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>}
</code></pre></div>
</div>
</div>
</div>
<p>Token streaming can be used to reduce <em>perceived</em> latency for applications. Here is how applications can use streaming:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="llmengine.Completion.acreate--__tabbed_2_1" name="llmengine.Completion.acreate--__tabbed_2" type="radio" /><input id="llmengine.Completion.acreate--__tabbed_2_2" name="llmengine.Completion.acreate--__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="llmengine.Completion.acreate--__tabbed_2_1">Asynchronous completion with token streaming in Python</label><label for="llmengine.Completion.acreate--__tabbed_2_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span style="color: #008000; font-weight: bold">import</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">asyncio</span>
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span style="color: #008000; font-weight: bold">async</span> <span style="color: #008000; font-weight: bold">def</span><span style="color: #BBB"> </span><span style="color: #00F">main</span>():
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    stream <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">await</span> Completion<span style="color: #666">.</span>acreate(
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>        model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>        prompt<span style="color: #666">=</span><span style="color: #BA2121">&quot;why is the sky blue?&quot;</span>,
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>        max_new_tokens<span style="color: #666">=5</span>,
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>        temperature<span style="color: #666">=0.2</span>,
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>        stream<span style="color: #666">=</span><span style="color: #008000; font-weight: bold">True</span>,
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>    )
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    <span style="color: #008000; font-weight: bold">async</span> <span style="color: #008000; font-weight: bold">for</span> response <span style="color: #A2F; font-weight: bold">in</span> stream:
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>        <span style="color: #008000; font-weight: bold">if</span> response<span style="color: #666">.</span>output:
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>            <span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>asyncio<span style="color: #666">.</span>run(main())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;9cfe4d5a-f86f-4094-a935-87f871d90ec0&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;\n&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">1</span>}}
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;9cfe4d5a-f86f-4094-a935-87f871d90ec0&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;I&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">2</span>}}
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;9cfe4d5a-f86f-4094-a935-87f871d90ec0&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot; think&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">3</span>}}
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;9cfe4d5a-f86f-4094-a935-87f871d90ec0&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot; the&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">false</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">4</span>}}
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>{<span style="color: #008000; font-weight: bold">&quot;request_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;9cfe4d5a-f86f-4094-a935-87f871d90ec0&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;output&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;text&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot; sky&quot;</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;finished&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">true</span>,<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">&quot;num_completion_tokens&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">5</span>}}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Completion.batch_create" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">batch_create</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Completion.batch_create" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>batch_create(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    output_data_path: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    model_config: CreateBatchCompletionsModelConfig,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    content: Optional[BatchCompletionContent] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    input_data_path: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    data_parallelism: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">1</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    max_runtime_sec: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">24</span> <span style="color: #666">*</span> <span style="color: #666">3600</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    labels: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    priority: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    use_v2: <span style="color: #008000">bool</span> <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    tool_config: Optional[ToolConfig] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    cpus: Optional[CpuSpecificationType] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    gpus: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    memory: Optional[StorageSpecificationType] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    gpu_type: Optional[GpuType] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    storage: Optional[StorageSpecificationType] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    <span style="color: #666">**</span>kwargs
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>) <span style="color: #666">-&gt;</span> Union[
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    CreateBatchCompletionsV1Response,
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>    CreateBatchCompletionsV2Response,
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>]
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Creates a batch completion for the provided input data. The job runs offline and does not depend on an existing model endpoint.</p>
<p>Prompts can be passed in from an input file, or as a part of the request.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>output_data_path</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The path to the output file. The output file will be a JSON file containing the completions.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model_config</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.CreateBatchCompletionsModelConfig" href="../data_types/#llmengine.CreateBatchCompletionsModelConfig">CreateBatchCompletionsModelConfig</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The model configuration to use for the batch completion.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>content</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<a class="autorefs autorefs-internal" title="llmengine.data_types.CreateBatchCompletionsRequestContent" href="../data_types/#llmengine.CreateBatchCompletionsRequestContent">CreateBatchCompletionsRequestContent</a>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The content to use for the batch completion. Either one of <code>content</code> or <code>input_data_path</code> must be provided.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>input_data_path</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="str">str</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The path to the input file. The input file should be a JSON file with data of type <code>BatchCompletionsRequestContent</code>. Either one of <code>content</code> or <code>input_data_path</code> must be provided.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>data_parallelism</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The number of parallel jobs to run. Data will be evenly distributed to the jobs. Defaults to 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>priority</code>
            </td>
            <td>
                  <code><span title="str">str</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Priority of the batch inference job. Default to None.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_runtime_sec</code>
            </td>
            <td>
                  <code><span title="int">int</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum runtime of the batch completion in seconds. Defaults to 24 hours.</p>
              </div>
            </td>
            <td>
                  <code>24 * 3600</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>use_v2</code>
            </td>
            <td>
                  <code><span title="bool">bool</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to use the v2 batch completion API. Defaults to False.</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>tool_config</code>
            </td>
            <td>
                  <code><span title="typing.Optional">Optional</span>[<span title="llmengine.data_types.ToolConfig">ToolConfig</span>]</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Configuration for tool use.
NOTE: this config is highly experimental and signature will change significantly in future iterations.
Currently only Python code evaluator is supported.
Python code context starts with "```python\n" and ends with "\n&gt;&gt;&gt;\n", data before "\n```\n" and content end will be replaced by the Python execution results.
Please format prompts accordingly and provide examples so LLMs could properly generate Python code.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>response</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.CreateBatchCompletionsResponse" href="../data_types/#llmengine.CreateBatchCompletionsResponse">CreateBatchCompletionsResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The response containing the job id.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:4"><input checked="checked" id="llmengine.Completion.batch_create--__tabbed_1_1" name="llmengine.Completion.batch_create--__tabbed_1" type="radio" /><input id="llmengine.Completion.batch_create--__tabbed_1_2" name="llmengine.Completion.batch_create--__tabbed_1" type="radio" /><input id="llmengine.Completion.batch_create--__tabbed_1_3" name="llmengine.Completion.batch_create--__tabbed_1" type="radio" /><input id="llmengine.Completion.batch_create--__tabbed_1_4" name="llmengine.Completion.batch_create--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Completion.batch_create--__tabbed_1_1">Batch completions with prompts in the request</label><label for="llmengine.Completion.batch_create--__tabbed_1_2">Batch completions with prompts in a file and with 2 parallel jobs</label><label for="llmengine.Completion.batch_create--__tabbed_1_3">Batch completions with prompts and use tool</label><label for="llmengine.Completion.batch_create--__tabbed_1_4">V2 Batch completions with prompts in the request</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine.data_types</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>response <span style="color: #666">=</span> Completion<span style="color: #666">.</span>batch_create(
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    output_data_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://my-path&quot;</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    model_config<span style="color: #666">=</span>CreateBatchCompletionsModelConfig(
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>        model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>        checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://checkpoint-path&quot;</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>        labels<span style="color: #666">=</span>{<span style="color: #BA2121">&quot;team&quot;</span>:<span style="color: #BA2121">&quot;my-team&quot;</span>, <span style="color: #BA2121">&quot;product&quot;</span>:<span style="color: #BA2121">&quot;my-product&quot;</span>}
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    ),
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    content<span style="color: #666">=</span>CreateBatchCompletionsRequestContent(
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>        prompts<span style="color: #666">=</span>[<span style="color: #BA2121">&quot;What is deep learning&quot;</span>, <span style="color: #BA2121">&quot;What is a neural network&quot;</span>],
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>        max_new_tokens<span style="color: #666">=10</span>,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>        temperature<span style="color: #666">=0.0</span>
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    )
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>)
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine.data_types</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #3D7B7B; font-style: italic"># Store CreateBatchCompletionsRequestContent data into input file &quot;s3://my-input-path&quot;</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>response <span style="color: #666">=</span> Completion<span style="color: #666">.</span>batch_create(
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    input_data_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://my-input-path&quot;</span>,
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>    output_data_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://my-output-path&quot;</span>,
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    model_config<span style="color: #666">=</span>CreateBatchCompletionsModelConfig(
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>        model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>        checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://checkpoint-path&quot;</span>,
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>        labels<span style="color: #666">=</span>{<span style="color: #BA2121">&quot;team&quot;</span>:<span style="color: #BA2121">&quot;my-team&quot;</span>, <span style="color: #BA2121">&quot;product&quot;</span>:<span style="color: #BA2121">&quot;my-product&quot;</span>}
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    ),
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>    data_parallelism<span style="color: #666">=2</span>
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>)
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Completion
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine.data_types</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent, ToolConfig
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span style="color: #3D7B7B; font-style: italic"># Store CreateBatchCompletionsRequestContent data into input file &quot;s3://my-input-path&quot;</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>response <span style="color: #666">=</span> Completion<span style="color: #666">.</span>batch_create(
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    input_data_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://my-input-path&quot;</span>,
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    output_data_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://my-output-path&quot;</span>,
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    model_config<span style="color: #666">=</span>CreateBatchCompletionsModelConfig(
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>        model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>        checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://checkpoint-path&quot;</span>,
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>        labels<span style="color: #666">=</span>{<span style="color: #BA2121">&quot;team&quot;</span>:<span style="color: #BA2121">&quot;my-team&quot;</span>, <span style="color: #BA2121">&quot;product&quot;</span>:<span style="color: #BA2121">&quot;my-product&quot;</span>}
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    ),
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>    data_parallelism<span style="color: #666">=2</span>,
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>    tool_config<span style="color: #666">=</span>ToolConfig(
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>        name<span style="color: #666">=</span><span style="color: #BA2121">&quot;code_evaluator&quot;</span>,
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>    )
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>)
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<p>```python
from llmengine import Completion
from llmengine.data_types import CreateBatchCompletionsModelConfig, FilteredChatCompletionV2Request</p>
<p>model_config = CreateBatchCompletionsModelConfig(
    model="gemma-2-2b-it",
    checkpoint_path="s3://path-to-checkpoint",
)</p>
<p>content = {
    "messages": [
        {
            "role": "user",
            "content": "What is a good place for travel in the US?",
        },
        {"role": "assistant", "content": "California."},
        {"role": "user", "content": "What can I do in California?"},
    ],
    "logprobs": True,
}</p>
<p>response = Completion.batch_create(
    output_data_path="testoutput",
    model_config=model_config,
    content=[FilteredChatCompletionV2Request(**content)],
    use_v2=True,
    labels={"team": "my-team", "product": "my-product"},
)</p>
<p>print(response.json())</p>
</div>
</div>
</div>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="llmengine.FineTune" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">FineTune</span>


<a href="#llmengine.FineTune" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="llmengine.api_engine.APIEngine">APIEngine</span></code></p>


        <p>FineTune API. This API is used to fine-tune models.</p>
<p>Fine-tuning is a process where the LLM is further trained on a task-specific dataset, allowing the model to adjust its parameters to better align with the task at hand. Fine-tuning is a supervised training phase, where prompt/response pairs are provided to optimize the performance of the LLM. LLM Engine currently uses <a href="https://arxiv.org/abs/2106.09685">LoRA</a> for fine-tuning. Support for additional fine-tuning methods is upcoming.</p>
<p>LLM Engine provides APIs to create fine-tunes on a base model with training &amp; validation datasets. APIs are also provided to list, cancel and retrieve fine-tuning jobs.</p>
<p>Creating a fine-tune will end with the creation of a Model, which you can view using <code>Model.get(model_name)</code> or delete using <code>Model.delete(model_name)</code>.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="llmengine.FineTune.create" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">create</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.FineTune.create" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>create(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    model: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    training_file: <span style="color: #008000">str</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    validation_file: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    hyperparameters: Optional[
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>        Dict[<span style="color: #008000">str</span>, Union[<span style="color: #008000">str</span>, <span style="color: #008000">int</span>, <span style="color: #008000">float</span>]]
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    ] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    wandb_config: Optional[Dict[<span style="color: #008000">str</span>, Any]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    suffix: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>) <span style="color: #666">-&gt;</span> CreateFineTuneResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Creates a job that fine-tunes a specified model with a given dataset.</p>
<p>This API can be used to fine-tune a model. The <em>model</em> is the name of base model
(<a href="../../model_zoo">Model Zoo</a> for available models) to fine-tune. The training
and validation files should consist of prompt and response pairs. <code>training_file</code>
and <code>validation_file</code> must be either publicly accessible HTTP or HTTPS URLs, or
file IDs of files uploaded to LLM Engine's <a href="./#llmengine.File">Files API</a> (these
will have the <code>file-</code> prefix). The referenced files must be CSV files that include
two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is
currently supported. At least 200 rows of data is recommended to start to see benefits from
fine-tuning. For sequences longer than the native <code>max_seq_length</code> of the model, the sequences
will be truncated.</p>
<p>A fine-tuning job can take roughly 30 minutes for a small dataset (~200 rows)
and several hours for larger ones.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The name of the base model to fine-tune. See <a href="../../model_zoo">Model Zoo</a> for the list of available models to fine-tune.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>training_file</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Publicly accessible URL or file ID referencing a CSV file for training. When no validation_file is provided, one will automatically be created using a 10% split of the training_file data.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>validation_file</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Publicly accessible URL or file ID referencing a CSV file for validation. The validation file is used to compute metrics which let LLM Engine pick the best fine-tuned checkpoint, which will be used for inference when fine-tuning is complete.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>hyperparameters</code>
            </td>
            <td>
                  <code>`Optional[Dict[str, Union[str, int, float, Dict[str, Any]]]]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A dict of hyperparameters to customize fine-tuning behavior.</p>
<p>Currently supported hyperparameters:</p>
<ul>
<li><code>lr</code>: Peak learning rate used during fine-tuning. It decays with a cosine schedule afterward. (Default: 2e-3)</li>
<li><code>warmup_ratio</code>: Ratio of training steps used for learning rate warmup. (Default: 0.03)</li>
<li><code>epochs</code>: Number of fine-tuning epochs. This should be less than 20. (Default: 5)</li>
<li><code>weight_decay</code>: Regularization penalty applied to learned weights. (Default: 0.001)</li>
<li><code>peft_config</code>: A dict of parameters for the PEFT algorithm. See <a href="https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.LoraConfig">LoraConfig</a> for more information.</li>
</ul>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>wandb_config</code>
            </td>
            <td>
                  <code>`Optional[Dict[str, Any]]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A dict of configuration parameters for Weights &amp; Biases. See <a href="https://docs.wandb.ai/ref/python/init">Weights &amp; Biases</a> for more information.
Set <code>hyperparameter["report_to"]</code> to <code>wandb</code> to enable automatic finetune metrics logging.
Must include <code>api_key</code> field which is the wandb API key.
Also supports setting <code>base_url</code> to use a custom Weights &amp; Biases server.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>suffix</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A string that will be added to your fine-tuned model name. If present, the entire fine-tuned model name
will be formatted like <code>"[model].[suffix].[YYMMDD-HHMMSS]"</code>. If absent, the
fine-tuned model name will be formatted <code>"[model].[YYMMDD-HHMMSS]"</code>.
For example, if <code>suffix</code> is <code>"my-experiment"</code>, the fine-tuned model name could be
<code>"llama-2-7b.my-experiment.230717-230150"</code>.
Note: <code>suffix</code> must be between 1 and 28 characters long, and can only contain alphanumeric characters and hyphens.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>CreateFineTuneResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.CreateFineTuneResponse" href="../data_types/#llmengine.CreateFineTuneResponse">CreateFineTuneResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains the ID of the created fine-tuning job</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Here is an example script to create a 5-row CSV of properly formatted data for fine-tuning
an airline question answering bot:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:1"><input checked="checked" id="llmengine.FineTune.create--__tabbed_1_1" name="llmengine.FineTune.create--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.FineTune.create--__tabbed_1_1">Formatting data in Python</label></div>
<div class="tabbed-content">
<div class="tabbed-block"></div>
</div>
</div>
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">import</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">csv</span>
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span style="color: #3D7B7B; font-style: italic"># Define data</span>
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>data <span style="color: #666">=</span> [
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>  (<span style="color: #BA2121">&quot;What is your policy on carry-on luggage?&quot;</span>, <span style="color: #BA2121">&quot;Our policy allows each passenger to bring one piece of carry-on luggage and one personal item such as a purse or briefcase. The maximum size for carry-on luggage is 22 x 14 x 9 inches.&quot;</span>),
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>  (<span style="color: #BA2121">&quot;How can I change my flight?&quot;</span>, <span style="color: #BA2121">&quot;You can change your flight through our website or mobile app. Go to &#39;Manage my booking&#39; section, enter your booking reference and last name, then follow the prompts to change your flight.&quot;</span>),
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>  (<span style="color: #BA2121">&quot;What meals are available on my flight?&quot;</span>, <span style="color: #BA2121">&quot;We offer a variety of meals depending on the flight&#39;s duration and route. These can range from snacks and light refreshments to full-course meals on long-haul flights. Specific meal options can be viewed during the booking process.&quot;</span>),
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>  (<span style="color: #BA2121">&quot;How early should I arrive at the airport before my flight?&quot;</span>, <span style="color: #BA2121">&quot;We recommend arriving at least two hours before domestic flights and three hours before international flights.&quot;</span>),
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>  <span style="color: #BA2121">&quot;Can I select my seat in advance?&quot;</span>, <span style="color: #BA2121">&quot;Yes, you can select your seat during the booking process or afterwards via the &#39;Manage my booking&#39; section on our website or mobile app.&quot;</span>),
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>  ]
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span style="color: #3D7B7B; font-style: italic"># Write data to a CSV file</span>
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a><span style="color: #008000; font-weight: bold">with</span> <span style="color: #008000">open</span>(<span style="color: #BA2121">&#39;customer_service_data.csv&#39;</span>, <span style="color: #BA2121">&#39;w&#39;</span>, newline<span style="color: #666">=</span><span style="color: #BA2121">&#39;&#39;</span>) <span style="color: #008000; font-weight: bold">as</span> file:
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    writer <span style="color: #666">=</span> csv<span style="color: #666">.</span>writer(file)
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    writer<span style="color: #666">.</span>writerow([<span style="color: #BA2121">&quot;prompt&quot;</span>, <span style="color: #BA2121">&quot;response&quot;</span>])
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    writer<span style="color: #666">.</span>writerows(data)
</code></pre></div>
<p>Currently, data needs to be uploaded to either a publicly accessible web URL or to LLM Engine's
private file server so that it can be read for fine-tuning. Publicly accessible HTTP and HTTPS
URLs are currently supported.</p>
<p>To privately share data with the LLM Engine API, use LLM Engine's <a href="../../api/python_client/#llmengine.File.upload">File.upload</a>
API. You can upload data in local file to LLM Engine's private file server and then use the
returned file ID to reference your data in the FineTune API. The file ID is generally in the
form of <code>file-&lt;random_string&gt;</code>, e.g. "file-7DLVeLdN2Ty4M2m".</p>
<p>Example code for fine-tuning:</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="llmengine.FineTune.create--__tabbed_2_1" name="llmengine.FineTune.create--__tabbed_2" type="radio" /><input id="llmengine.FineTune.create--__tabbed_2_2" name="llmengine.FineTune.create--__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="llmengine.FineTune.create--__tabbed_2_1">Fine-tuning in Python</label><label for="llmengine.FineTune.create--__tabbed_2_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> FineTune
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>response <span style="color: #666">=</span> FineTune<span style="color: #666">.</span>create(
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    training_file<span style="color: #666">=</span><span style="color: #BA2121">&quot;file-7DLVeLdN2Ty4M2m&quot;</span>,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>)
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a>{
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;fine_tune_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ft-cir3eevt71r003ks6il0&quot;</span>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.FineTune.get" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.FineTune.get" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>get(fine_tune_id: <span style="color: #008000">str</span>) <span style="color: #666">-&gt;</span> GetFineTuneResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Get status of a fine-tuning job.</p>
<p>This API can be used to get the status of an already running
fine-tuning job. It takes as a single parameter the <code>fine_tune_id</code>
and returns a
<a href="../../api/data_types/#llmengine.GetFineTuneResponse">GetFineTuneResponse</a>
object with the id and status (<code>PENDING</code>, <code>STARTED</code>,
<code>UNDEFINED</code>, <code>FAILURE</code> or <code>SUCCESS</code>).</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>fine_tune_id</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ID of the fine-tuning job</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>GetFineTuneResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.GetFineTuneResponse" href="../data_types/#llmengine.GetFineTuneResponse">GetFineTuneResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains the ID and status of the requested job</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.FineTune.get--__tabbed_1_1" name="llmengine.FineTune.get--__tabbed_1" type="radio" /><input id="llmengine.FineTune.get--__tabbed_1_2" name="llmengine.FineTune.get--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.FineTune.get--__tabbed_1_1">Getting status of fine-tuning in Python</label><label for="llmengine.FineTune.get--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> FineTune
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> FineTune<span style="color: #666">.</span>get(
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    fine_tune_id<span style="color: #666">=</span><span style="color: #BA2121">&quot;ft-cir3eevt71r003ks6il0&quot;</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>)
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;fine_tune_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ft-cir3eevt71r003ks6il0&quot;</span>,
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;status&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;STARTED&quot;</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.FineTune.get_events" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get_events</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.FineTune.get_events" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>get_events(fine_tune_id: <span style="color: #008000">str</span>) <span style="color: #666">-&gt;</span> GetFineTuneEventsResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Get events of a fine-tuning job.</p>
<p>This API can be used to get the list of detailed events for a fine-tuning job.
It takes the <code>fine_tune_id</code> as a parameter and returns a response object
which has a list of events that has happened for the fine-tuning job. Two events
are logged periodically: an evaluation of the training loss, and an
evaluation of the eval loss. This API will return all events for the fine-tuning job.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>fine_tune_id</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ID of the fine-tuning job</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>GetFineTuneEventsResponse</code></td>            <td>
                  <code><span title="llmengine.data_types.GetFineTuneEventsResponse">GetFineTuneEventsResponse</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains the list of events for the fine-tuning job</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.FineTune.get_events--__tabbed_1_1" name="llmengine.FineTune.get_events--__tabbed_1" type="radio" /><input id="llmengine.FineTune.get_events--__tabbed_1_2" name="llmengine.FineTune.get_events--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.FineTune.get_events--__tabbed_1_1">Getting events for  fine-tuning jobs in Python</label><label for="llmengine.FineTune.get_events--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> FineTune
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> FineTune<span style="color: #666">.</span>get_events(fine_tune_id<span style="color: #666">=</span><span style="color: #BA2121">&quot;ft-cir3eevt71r003ks6il0&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;events&quot;</span>:
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span>[
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;timestamp&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">1689665099.6704428</span>,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;message&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;{&#39;loss&#39;: 2.108, &#39;learning_rate&#39;: 0.002, &#39;epoch&#39;: 0.7}&quot;</span>,
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;level&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;info&quot;</span>
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;timestamp&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">1689665100.1966307</span>,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;message&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;{&#39;eval_loss&#39;: 1.67730712890625, &#39;eval_runtime&#39;: 0.2023, &#39;eval_samples_per_second&#39;: 24.717, &#39;eval_steps_per_second&#39;: 4.943, &#39;epoch&#39;: 0.7}&quot;</span>,
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;level&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;info&quot;</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;timestamp&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">1689665105.6544185</span>,
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;message&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;{&#39;loss&#39;: 1.8961, &#39;learning_rate&#39;: 0.0017071067811865474, &#39;epoch&#39;: 1.39}&quot;</span>,
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;level&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;info&quot;</span>
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;timestamp&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">1689665106.159139</span>,
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;message&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;{&#39;eval_loss&#39;: 1.513688564300537, &#39;eval_runtime&#39;: 0.2025, &#39;eval_samples_per_second&#39;: 24.696, &#39;eval_steps_per_second&#39;: 4.939, &#39;epoch&#39;: 1.39}&quot;</span>,
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;level&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;info&quot;</span>
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a><span style="color: #BBB">        </span>}
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a><span style="color: #BBB">    </span>]
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.FineTune.list" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.FineTune.list" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000">list</span>() <span style="color: #666">-&gt;</span> ListFineTunesResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List fine-tuning jobs.</p>
<p>This API can be used to list all the fine-tuning jobs.
It returns a list of pairs of <code>fine_tune_id</code> and <code>status</code> for
all existing jobs.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>ListFineTunesResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.ListFineTunesResponse" href="../data_types/#llmengine.ListFineTunesResponse">ListFineTunesResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains a list of all fine-tuning jobs and their statuses</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.FineTune.list--__tabbed_1_1" name="llmengine.FineTune.list--__tabbed_1" type="radio" /><input id="llmengine.FineTune.list--__tabbed_1_2" name="llmengine.FineTune.list--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.FineTune.list--__tabbed_1_1">Listing fine-tuning jobs in Python</label><label for="llmengine.FineTune.list--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> FineTune
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> FineTune<span style="color: #666">.</span>list()
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;jobs&quot;</span>:<span style="color: #BBB"> </span>[
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;fine_tune_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ft-cir3eevt71r003ks6il0&quot;</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;status&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;STARTED&quot;</span>
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;fine_tune_id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;ft_def456&quot;</span>,
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;status&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;SUCCESS&quot;</span>
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span style="color: #BBB">        </span>}
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span style="color: #BBB">    </span>]
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.FineTune.cancel" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">cancel</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.FineTune.cancel" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>cancel(fine_tune_id: <span style="color: #008000">str</span>) <span style="color: #666">-&gt;</span> CancelFineTuneResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Cancel a fine-tuning job.</p>
<p>This API can be used to cancel an existing fine-tuning job if
it's no longer required. It takes the <code>fine_tune_id</code> as a parameter
and returns a response object which has a <code>success</code> field
confirming if the cancellation was successful.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>fine_tune_id</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ID of the fine-tuning job</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>CancelFineTuneResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.CancelFineTuneResponse" href="../data_types/#llmengine.CancelFineTuneResponse">CancelFineTuneResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains whether the cancellation was successful</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.FineTune.cancel--__tabbed_1_1" name="llmengine.FineTune.cancel--__tabbed_1" type="radio" /><input id="llmengine.FineTune.cancel--__tabbed_1_2" name="llmengine.FineTune.cancel--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.FineTune.cancel--__tabbed_1_1">Cancelling fine-tuning job in Python</label><label for="llmengine.FineTune.cancel--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> FineTune
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> FineTune<span style="color: #666">.</span>cancel(fine_tune_id<span style="color: #666">=</span><span style="color: #BA2121">&quot;ft-cir3eevt71r003ks6il0&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;success&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">true</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="llmengine.Model" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">Model</span>


<a href="#llmengine.Model" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="llmengine.api_engine.APIEngine">APIEngine</span></code></p>


        <p>Model API. This API is used to get, list, and delete models. Models include both base
models built into LLM Engine, and fine-tuned models that you create through the
<a href="./#llmengine.fine_tuning.FineTune.create">FineTune.create()</a> API.</p>
<p>See <a href="../../model_zoo">Model Zoo</a> for the list of publicly available base models.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="llmengine.Model.create" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">create</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Model.create" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>create(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    name: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    model: <span style="color: #008000">str</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    inference_framework_image_tag: <span style="color: #008000">str</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    source: LLMSource <span style="color: #666">=</span> LLMSource<span style="color: #666">.</span>HUGGING_FACE,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    inference_framework: LLMInferenceFramework <span style="color: #666">=</span> LLMInferenceFramework<span style="color: #666">.</span>VLLM,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    num_shards: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">1</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    quantize: Optional[Quantization] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    checkpoint_path: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    max_model_len: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    cpus: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    memory: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    storage: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    gpus: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    nodes_per_worker: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">1</span>,
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    min_workers: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">0</span>,
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    max_workers: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">1</span>,
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    per_worker: <span style="color: #008000">int</span> <span style="color: #666">=</span> <span style="color: #666">2</span>,
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    endpoint_type: ModelEndpointType <span style="color: #666">=</span> ModelEndpointType<span style="color: #666">.</span>STREAMING,
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>    gpu_type: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>    high_priority: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    post_inference_hooks: Optional[
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>        List[PostInferenceHooks]
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>    ] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    default_callback_url: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    public_inference: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">True</span>,
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>    labels: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>    <span style="color: #666">**</span>extra_kwargs
<a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a>) <span style="color: #666">-&gt;</span> CreateLLMEndpointResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Create an LLM model. Note: This API is only available for self-hosted users.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>name</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the endpoint</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the base model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inference_framework_image_tag</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Image tag for the inference framework. Use "latest" for the most recent image</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>source</code>
            </td>
            <td>
                  <code>`LLMSource`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Source of the LLM. Currently only HuggingFace is supported</p>
              </div>
            </td>
            <td>
                  <code><span title="llmengine.data_types.LLMSource.HUGGING_FACE">HUGGING_FACE</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inference_framework</code>
            </td>
            <td>
                  <code>`LLMInferenceFramework`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Inference framework for the LLM. Current supported frameworks are
LLMInferenceFramework.DEEPSPEED, LLMInferenceFramework.TEXT_GENERATION_INFERENCE,
LLMInferenceFramework.VLLM and LLMInferenceFramework.LIGHTLLM</p>
              </div>
            </td>
            <td>
                  <code><span title="llmengine.data_types.LLMInferenceFramework.VLLM">VLLM</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_shards</code>
            </td>
            <td>
                  <code>`int`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of shards for the LLM. When bigger than 1, LLM will be sharded
to multiple GPUs. Number of GPUs must be equal or larger than num_shards.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantize</code>
            </td>
            <td>
                  <code>`Optional[Quantization]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Quantization method for the LLM. <code>text_generation_inference</code> supports <code>bitsandbytes</code> and <code>vllm</code> supports <code>awq</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>checkpoint_path</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Remote path to the checkpoint for the LLM. LLM engine must have permission to access the given path.
Can be either a folder or a tar file. Folder is preferred since we don't need to untar and model loads faster.
For model weights, safetensors are preferred but PyTorch checkpoints are also accepted (model loading will be longer).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_model_len</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Model context length. If unspecified, will be automatically derived from the model config.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cpus</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of cpus each node in the worker should get, e.g. 1, 2, etc. This must be greater
than or equal to 1. Recommendation is set it to 8 * GPU count. Can be inferred from the model size.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>memory</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Amount of memory each node in the worker should get, e.g. "4Gi", "512Mi", etc. This must
be a positive amount of memory. Recommendation is set it to 24Gi * GPU count.
Can be inferred from the model size.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>storage</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Amount of local ephemeral storage each node in the worker should get, e.g. "4Gi",
"512Mi", etc. This must be a positive amount of storage.
Recommendataion is 40Gi for 7B models, 80Gi for 13B models and 200Gi for 70B models.
Can be inferred from the model size.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gpus</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of gpus each node in the worker should get, e.g. 0, 1, etc. Can be inferred from the model size.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>nodes_per_worker</code>
            </td>
            <td>
                  <code>`int`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of nodes per worker. Used to request multinode serving. This must be greater than or equal to 1.
Controls how many nodes to dedicate to one instance of the model.
Specifically, if <code>nodes_per_worker</code> is set to greater than 1, the model will be sharded across
<code>nodes_per_worker</code> nodes (e.g. kubernetes pods). One of these nodes will be a "leader" node and receive requests.
LLM Engine will set up the inter-node communication.
Any compute resource requests (i.e. cpus, memory, storage) apply to each individual node, thus the total resources
allocated are multiplied by this number. This is useful for models that require more memory than a single node can provide.
Note: autoscaling is not supported for multinode serving.
Further note: if your model can fit on GPUs on only one machine, e.g. you have access to an 8xA100 machine and your model fits
on 8 A100s, it is recommended to set <code>nodes_per_worker</code> to 1 and the rest of the resources accordingly.
<code>nodes_per_worker &gt; 1</code> should only be set if you require more resources than a single machine can provide.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_workers</code>
            </td>
            <td>
                  <code>`int`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The minimum number of workers. Must be greater than or equal to 0. This
should be determined by computing the minimum throughput of your workload and
dividing it by the throughput of a single worker. When this number is 0,
max_workers must be 1, and the endpoint will autoscale between
0 and 1 pods. When this number is greater than 0, max_workers can be any number
greater or equal to min_workers.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_workers</code>
            </td>
            <td>
                  <code>`int`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum number of workers. Must be greater than or equal to 0,
and as well as greater than or equal to <code>min_workers</code>. This should be determined by
computing the maximum throughput of your workload and dividing it by the throughput
of a single worker</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>per_worker</code>
            </td>
            <td>
                  <code>`int`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum number of concurrent requests that an individual worker can
service. LLM engine automatically scales the number of workers for the endpoint so that
each worker is processing <code>per_worker</code> requests, subject to the limits defined by
<code>min_workers</code> and <code>max_workers</code>
- If the average number of concurrent requests per worker is lower than
<code>per_worker</code>, then the number of workers will be reduced. - Otherwise,
if the average number of concurrent requests per worker is higher than
<code>per_worker</code>, then the number of workers will be increased to meet the elevated
traffic.
Here is our recommendation for computing <code>per_worker</code>:
1. Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum
throughput requirements. 2. Determine a value for the maximum number of
concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing
this ensures that the number of workers will "climb" to <code>max_workers</code>.</p>
              </div>
            </td>
            <td>
                  <code>2</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>endpoint_type</code>
            </td>
            <td>
                  <code>`ModelEndpointType`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Currently only <code>"streaming"</code> endpoints are supported.</p>
              </div>
            </td>
            <td>
                  <code><span title="llmengine.data_types.ModelEndpointType.STREAMING">STREAMING</span></code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gpu_type</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specifying a non-zero number of gpus, this controls the type of gpu
requested. Can be inferred from the model size. Here are the supported values:</p>
<ul>
<li><code>nvidia-tesla-t4</code></li>
<li><code>nvidia-ampere-a10</code></li>
<li><code>nvidia-ampere-a100</code></li>
<li><code>nvidia-ampere-a100e</code></li>
<li><code>nvidia-hopper-h100</code></li>
<li><code>nvidia-hopper-h100-1g20gb</code> # 1 slice of MIG with 1g compute and 20GB memory</li>
<li><code>nvidia-hopper-h100-3g40gb</code> # 1 slice of MIG with 3g compute and 40GB memory</li>
</ul>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>high_priority</code>
            </td>
            <td>
                  <code>`Optional[bool]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created
endpoint to leverage the shared pool of prewarmed nodes for faster spinup time</p>
              </div>
            </td>
            <td>
                  <code>False</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>post_inference_hooks</code>
            </td>
            <td>
                  <code>`Optional[List[PostInferenceHooks]]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of hooks to trigger after inference tasks are served</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>default_callback_url</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The default callback url to use for sync completion requests.
This can be overridden in the task parameters for each individual task.
post_inference_hooks must contain "callback" for the callback to be triggered</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>public_inference</code>
            </td>
            <td>
                  <code>`Optional[bool]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, this endpoint will be available to all user IDs for
inference</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td>
                  <code>`Optional[Dict[str, str]]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>An optional dictionary of key/value pairs to associate with this endpoint</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:
    CreateLLMEndpointResponse: creation task ID of the created Model. Currently not used.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:4"><input checked="checked" id="llmengine.Model.create--__tabbed_1_1" name="llmengine.Model.create--__tabbed_1" type="radio" /><input id="llmengine.Model.create--__tabbed_1_2" name="llmengine.Model.create--__tabbed_1" type="radio" /><input id="llmengine.Model.create--__tabbed_1_3" name="llmengine.Model.create--__tabbed_1" type="radio" /><input id="llmengine.Model.create--__tabbed_1_4" name="llmengine.Model.create--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Model.create--__tabbed_1_1">Create Llama 2 70B model with hardware specs inferred in Python</label><label for="llmengine.Model.create--__tabbed_1_2">Create Llama 2 7B model with hardware specs specified in Python</label><label for="llmengine.Model.create--__tabbed_1_3">Create Llama 2 13B model in Python</label><label for="llmengine.Model.create--__tabbed_1_4">Create Llama 2 70B model with 8bit quantization in Python</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>create(
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    name<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-70b-test&quot;</span>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-70b&quot;</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    inference_framework_image_tag<span style="color: #666">=</span><span style="color: #BA2121">&quot;0.9.4&quot;</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    inference_framework<span style="color: #666">=</span>LLMInferenceFramework<span style="color: #666">.</span>TEXT_GENERATION_INFERENCE,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    num_shards<span style="color: #666">=4</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://path/to/checkpoint&quot;</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    min_workers<span style="color: #666">=0</span>,
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    max_workers<span style="color: #666">=1</span>,
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    per_worker<span style="color: #666">=10</span>,
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    endpoint_type<span style="color: #666">=</span>ModelEndpointType<span style="color: #666">.</span>STREAMING,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    public_inference<span style="color: #666">=</span><span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>)
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>create(
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>    name<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b-test&quot;</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>    inference_framework_image_tag<span style="color: #666">=</span><span style="color: #BA2121">&quot;0.2.1.post1&quot;</span>,
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>    inference_framework<span style="color: #666">=</span>LLMInferenceFramework<span style="color: #666">.</span>VLLM,
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a>    num_shards<span style="color: #666">=1</span>,
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>    checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://path/to/checkpoint&quot;</span>,
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a>    cpus<span style="color: #666">=8</span>,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>    memory<span style="color: #666">=</span><span style="color: #BA2121">&quot;24Gi&quot;</span>,
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    storage<span style="color: #666">=</span><span style="color: #BA2121">&quot;40Gi&quot;</span>,
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>    gpus<span style="color: #666">=1</span>,
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>    min_workers<span style="color: #666">=0</span>,
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>    max_workers<span style="color: #666">=1</span>,
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>    per_worker<span style="color: #666">=10</span>,
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>    endpoint_type<span style="color: #666">=</span>ModelEndpointType<span style="color: #666">.</span>STREAMING,
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>    gpu_type<span style="color: #666">=</span><span style="color: #BA2121">&quot;nvidia-ampere-a10&quot;</span>,
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>    public_inference<span style="color: #666">=</span><span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>)
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
<a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>create(
<a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a>    name<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-13b-test&quot;</span>
<a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-13b&quot;</span>,
<a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>    inference_framework_image_tag<span style="color: #666">=</span><span style="color: #BA2121">&quot;0.2.1.post1&quot;</span>,
<a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a>    inference_framework<span style="color: #666">=</span>LLMInferenceFramework<span style="color: #666">.</span>VLLM,
<a id="__codelineno-2-8" name="__codelineno-2-8" href="#__codelineno-2-8"></a>    num_shards<span style="color: #666">=2</span>,
<a id="__codelineno-2-9" name="__codelineno-2-9" href="#__codelineno-2-9"></a>    checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://path/to/checkpoint&quot;</span>,
<a id="__codelineno-2-10" name="__codelineno-2-10" href="#__codelineno-2-10"></a>    cpus<span style="color: #666">=16</span>,
<a id="__codelineno-2-11" name="__codelineno-2-11" href="#__codelineno-2-11"></a>    memory<span style="color: #666">=</span><span style="color: #BA2121">&quot;48Gi&quot;</span>,
<a id="__codelineno-2-12" name="__codelineno-2-12" href="#__codelineno-2-12"></a>    storage<span style="color: #666">=</span><span style="color: #BA2121">&quot;80Gi&quot;</span>,
<a id="__codelineno-2-13" name="__codelineno-2-13" href="#__codelineno-2-13"></a>    gpus<span style="color: #666">=2</span>,
<a id="__codelineno-2-14" name="__codelineno-2-14" href="#__codelineno-2-14"></a>    min_workers<span style="color: #666">=0</span>,
<a id="__codelineno-2-15" name="__codelineno-2-15" href="#__codelineno-2-15"></a>    max_workers<span style="color: #666">=1</span>,
<a id="__codelineno-2-16" name="__codelineno-2-16" href="#__codelineno-2-16"></a>    per_worker<span style="color: #666">=10</span>,
<a id="__codelineno-2-17" name="__codelineno-2-17" href="#__codelineno-2-17"></a>    endpoint_type<span style="color: #666">=</span>ModelEndpointType<span style="color: #666">.</span>STREAMING,
<a id="__codelineno-2-18" name="__codelineno-2-18" href="#__codelineno-2-18"></a>    gpu_type<span style="color: #666">=</span><span style="color: #BA2121">&quot;nvidia-ampere-a10&quot;</span>,
<a id="__codelineno-2-19" name="__codelineno-2-19" href="#__codelineno-2-19"></a>    public_inference<span style="color: #666">=</span><span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-2-20" name="__codelineno-2-20" href="#__codelineno-2-20"></a>)
<a id="__codelineno-2-21" name="__codelineno-2-21" href="#__codelineno-2-21"></a>
<a id="__codelineno-2-22" name="__codelineno-2-22" href="#__codelineno-2-22"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
<a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>create(
<a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a>    name<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-70b-test&quot;</span>
<a id="__codelineno-3-5" name="__codelineno-3-5" href="#__codelineno-3-5"></a>    model<span style="color: #666">=</span><span style="color: #BA2121">&quot;llama-2-70b&quot;</span>,
<a id="__codelineno-3-6" name="__codelineno-3-6" href="#__codelineno-3-6"></a>    inference_framework_image_tag<span style="color: #666">=</span><span style="color: #BA2121">&quot;0.9.4&quot;</span>,
<a id="__codelineno-3-7" name="__codelineno-3-7" href="#__codelineno-3-7"></a>    inference_framework<span style="color: #666">=</span>LLMInferenceFramework<span style="color: #666">.</span>TEXT_GENERATION_INFERENCE,
<a id="__codelineno-3-8" name="__codelineno-3-8" href="#__codelineno-3-8"></a>    num_shards<span style="color: #666">=4</span>,
<a id="__codelineno-3-9" name="__codelineno-3-9" href="#__codelineno-3-9"></a>    quantize<span style="color: #666">=</span><span style="color: #BA2121">&quot;bitsandbytes&quot;</span>,
<a id="__codelineno-3-10" name="__codelineno-3-10" href="#__codelineno-3-10"></a>    checkpoint_path<span style="color: #666">=</span><span style="color: #BA2121">&quot;s3://path/to/checkpoint&quot;</span>,
<a id="__codelineno-3-11" name="__codelineno-3-11" href="#__codelineno-3-11"></a>    cpus<span style="color: #666">=40</span>,
<a id="__codelineno-3-12" name="__codelineno-3-12" href="#__codelineno-3-12"></a>    memory<span style="color: #666">=</span><span style="color: #BA2121">&quot;96Gi&quot;</span>,
<a id="__codelineno-3-13" name="__codelineno-3-13" href="#__codelineno-3-13"></a>    storage<span style="color: #666">=</span><span style="color: #BA2121">&quot;200Gi&quot;</span>,
<a id="__codelineno-3-14" name="__codelineno-3-14" href="#__codelineno-3-14"></a>    gpus<span style="color: #666">=4</span>,
<a id="__codelineno-3-15" name="__codelineno-3-15" href="#__codelineno-3-15"></a>    min_workers<span style="color: #666">=0</span>,
<a id="__codelineno-3-16" name="__codelineno-3-16" href="#__codelineno-3-16"></a>    max_workers<span style="color: #666">=1</span>,
<a id="__codelineno-3-17" name="__codelineno-3-17" href="#__codelineno-3-17"></a>    per_worker<span style="color: #666">=10</span>,
<a id="__codelineno-3-18" name="__codelineno-3-18" href="#__codelineno-3-18"></a>    endpoint_type<span style="color: #666">=</span>ModelEndpointType<span style="color: #666">.</span>STREAMING,
<a id="__codelineno-3-19" name="__codelineno-3-19" href="#__codelineno-3-19"></a>    gpu_type<span style="color: #666">=</span><span style="color: #BA2121">&quot;nvidia-ampere-a10&quot;</span>,
<a id="__codelineno-3-20" name="__codelineno-3-20" href="#__codelineno-3-20"></a>    public_inference<span style="color: #666">=</span><span style="color: #008000; font-weight: bold">False</span>,
<a id="__codelineno-3-21" name="__codelineno-3-21" href="#__codelineno-3-21"></a>)
<a id="__codelineno-3-22" name="__codelineno-3-22" href="#__codelineno-3-22"></a>
<a id="__codelineno-3-23" name="__codelineno-3-23" href="#__codelineno-3-23"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Model.get" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Model.get" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>get(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    model: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>) <span style="color: #666">-&gt;</span> GetLLMEndpointResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Get information about an LLM model.</p>
<p>This API can be used to get information about a Model's source and inference framework.
For self-hosted users, it returns additional information about number of shards, quantization, infra settings, etc.
The function takes as a single parameter the name <code>model</code>
and returns a
<a href="../../api/data_types/#llmengine.GetLLMEndpointResponse">GetLLMEndpointResponse</a>
object.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>GetLLMEndpointResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.GetLLMEndpointResponse" href="../data_types/#llmengine.GetLLMEndpointResponse">GetLLMEndpointResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>object representing the LLM and configurations</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.Model.get--__tabbed_1_1" name="llmengine.Model.get--__tabbed_1" type="radio" /><input id="llmengine.Model.get--__tabbed_1_2" name="llmengine.Model.get--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Model.get--__tabbed_1_1">Accessing model in Python</label><label for="llmengine.Model.get--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>get(<span style="color: #BA2121">&quot;llama-2-7b.suffix.2023-07-18-12-00-00&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;llama-2-7b.suffix.2023-07-18-12-00-00&quot;</span>,
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;model_name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;source&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;hugging_face&quot;</span>,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;status&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;READY&quot;</span>,
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;inference_framework&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;text_generation_inference&quot;</span>,
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;inference_framework_tag&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;num_shards&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;quantize&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;spec&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Model.list" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Model.list" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000">list</span>(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>) <span style="color: #666">-&gt;</span> ListLLMEndpointsResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List LLM models available to call inference on.</p>
<p>This API can be used to list all available models, including both publicly
available models and user-created fine-tuned models.
It returns a list of
<a href="../../api/data_types/#llmengine.GetLLMEndpointResponse">GetLLMEndpointResponse</a>
objects for all models. The most important field is the model <code>name</code>.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>ListLLMEndpointsResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.ListLLMEndpointsResponse" href="../data_types/#llmengine.ListLLMEndpointsResponse">ListLLMEndpointsResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>list of models</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.Model.list--__tabbed_1_1" name="llmengine.Model.list--__tabbed_1" type="radio" /><input id="llmengine.Model.list--__tabbed_1_2" name="llmengine.Model.list--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Model.list--__tabbed_1_1">Listing available modes in Python</label><label for="llmengine.Model.list--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>list()
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;model_endpoints&quot;</span>:<span style="color: #BBB"> </span>[
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;llama-2-7b.suffix.2023-07-18-12-00-00&quot;</span>,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;model_name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;source&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;hugging_face&quot;</span>,
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;text_generation_inference&quot;</span>,
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework_tag&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;num_shards&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;quantize&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;spec&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;llama-2-7b&quot;</span>,
<a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;model_name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;source&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;hugging_face&quot;</span>,
<a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;text_generation_inference&quot;</span>,
<a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework_tag&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;num_shards&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;quantize&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-23" name="__codelineno-1-23" href="#__codelineno-1-23"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;spec&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>
<a id="__codelineno-1-24" name="__codelineno-1-24" href="#__codelineno-1-24"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-25" name="__codelineno-1-25" href="#__codelineno-1-25"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-26" name="__codelineno-1-26" href="#__codelineno-1-26"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-27" name="__codelineno-1-27" href="#__codelineno-1-27"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;llama-13b-deepspeed-sync&quot;</span>,
<a id="__codelineno-1-28" name="__codelineno-1-28" href="#__codelineno-1-28"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;model_name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-29" name="__codelineno-1-29" href="#__codelineno-1-29"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;source&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;hugging_face&quot;</span>,
<a id="__codelineno-1-30" name="__codelineno-1-30" href="#__codelineno-1-30"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;deepspeed&quot;</span>,
<a id="__codelineno-1-31" name="__codelineno-1-31" href="#__codelineno-1-31"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework_tag&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-32" name="__codelineno-1-32" href="#__codelineno-1-32"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;num_shards&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-33" name="__codelineno-1-33" href="#__codelineno-1-33"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;quantize&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-34" name="__codelineno-1-34" href="#__codelineno-1-34"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;spec&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>
<a id="__codelineno-1-35" name="__codelineno-1-35" href="#__codelineno-1-35"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-36" name="__codelineno-1-36" href="#__codelineno-1-36"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-37" name="__codelineno-1-37" href="#__codelineno-1-37"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-38" name="__codelineno-1-38" href="#__codelineno-1-38"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;falcon-40b&quot;</span>,
<a id="__codelineno-1-39" name="__codelineno-1-39" href="#__codelineno-1-39"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;model_name&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-40" name="__codelineno-1-40" href="#__codelineno-1-40"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;source&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;hugging_face&quot;</span>,
<a id="__codelineno-1-41" name="__codelineno-1-41" href="#__codelineno-1-41"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;text_generation_inference&quot;</span>,
<a id="__codelineno-1-42" name="__codelineno-1-42" href="#__codelineno-1-42"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;inference_framework_tag&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-43" name="__codelineno-1-43" href="#__codelineno-1-43"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;num_shards&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-44" name="__codelineno-1-44" href="#__codelineno-1-44"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;quantize&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>,
<a id="__codelineno-1-45" name="__codelineno-1-45" href="#__codelineno-1-45"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;spec&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">null</span>
<a id="__codelineno-1-46" name="__codelineno-1-46" href="#__codelineno-1-46"></a><span style="color: #BBB">        </span>}
<a id="__codelineno-1-47" name="__codelineno-1-47" href="#__codelineno-1-47"></a><span style="color: #BBB">    </span>]
<a id="__codelineno-1-48" name="__codelineno-1-48" href="#__codelineno-1-48"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Model.update" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">update</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Model.update" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>update(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    name: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    model: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    inference_framework_image_tag: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>    source: Optional[LLMSource] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>    num_shards: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>    quantize: Optional[Quantization] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a>    checkpoint_path: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a>    cpus: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    memory: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    storage: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a>    gpus: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>    min_workers: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a>    max_workers: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a>    per_worker: Optional[<span style="color: #008000">int</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>    endpoint_type: Optional[ModelEndpointType] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a>    gpu_type: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a>    high_priority: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>    post_inference_hooks: Optional[
<a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a>        List[PostInferenceHooks]
<a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a>    ] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>    default_callback_url: Optional[<span style="color: #008000">str</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a>    public_inference: Optional[<span style="color: #008000">bool</span>] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a>    labels: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>    <span style="color: #666">**</span>extra_kwargs
<a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a>) <span style="color: #666">-&gt;</span> UpdateLLMEndpointResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Update an LLM model. Note: This API is only available for self-hosted users.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>name</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the endpoint</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>model</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the base model</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>inference_framework_image_tag</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Image tag for the inference framework. Use "latest" for the most recent image</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>source</code>
            </td>
            <td>
                  <code>`Optional[LLMSource]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Source of the LLM. Currently only HuggingFace is supported</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>num_shards</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of shards for the LLM. When bigger than 1, LLM will be sharded
to multiple GPUs. Number of GPUs must be equal or larger than num_shards.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>quantize</code>
            </td>
            <td>
                  <code>`Optional[Quantization]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Quantization method for the LLM. <code>text_generation_inference</code> supports <code>bitsandbytes</code> and <code>vllm</code> supports <code>awq</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>checkpoint_path</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Remote path to the checkpoint for the LLM. LLM engine must have permission to access the given path.
Can be either a folder or a tar file. Folder is preferred since we don't need to untar and model loads faster.
For model weights, safetensors are preferred but PyTorch checkpoints are also accepted (model loading will be longer).</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>cpus</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of cpus each node in the worker should get, e.g. 1, 2, etc. This must be greater
than or equal to 1. Recommendation is set it to 8 * GPU count.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>memory</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Amount of memory each node in the worker should get, e.g. "4Gi", "512Mi", etc. This must
be a positive amount of memory. Recommendation is set it to 24Gi * GPU count.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>storage</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Amount of local ephemeral storage each node in the worker should get, e.g. "4Gi",
"512Mi", etc. This must be a positive amount of storage.
Recommendataion is 40Gi for 7B models, 80Gi for 13B models and 200Gi for 70B models.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gpus</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of gpus each node in the worker should get, e.g. 0, 1, etc.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>min_workers</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The minimum number of workers. Must be greater than or equal to 0. This
should be determined by computing the minimum throughput of your workload and
dividing it by the throughput of a single worker. When this number is 0,
max_workers must be 1, and the endpoint will autoscale between
0 and 1 pods. When this number is greater than 0, max_workers can be any number
greater or equal to min_workers.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>max_workers</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum number of workers. Must be greater than or equal to 0,
and as well as greater than or equal to <code>min_workers</code>. This should be determined by
computing the maximum throughput of your workload and dividing it by the throughput
of a single worker</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>per_worker</code>
            </td>
            <td>
                  <code>`Optional[int]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The maximum number of concurrent requests that an individual worker can
service. LLM engine automatically scales the number of workers for the endpoint so that
each worker is processing <code>per_worker</code> requests, subject to the limits defined by
<code>min_workers</code> and <code>max_workers</code>
- If the average number of concurrent requests per worker is lower than
<code>per_worker</code>, then the number of workers will be reduced. - Otherwise,
if the average number of concurrent requests per worker is higher than
<code>per_worker</code>, then the number of workers will be increased to meet the elevated
traffic.
Here is our recommendation for computing <code>per_worker</code>:
1. Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum
throughput requirements. 2. Determine a value for the maximum number of
concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing
this ensures that the number of workers will "climb" to <code>max_workers</code>.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>endpoint_type</code>
            </td>
            <td>
                  <code>`Optional[ModelEndpointType]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Currently only <code>"streaming"</code> endpoints are supported.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>gpu_type</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If specifying a non-zero number of gpus, this controls the type of gpu
requested. Here are the supported values:</p>
<ul>
<li><code>nvidia-tesla-t4</code></li>
<li><code>nvidia-ampere-a10</code></li>
<li><code>nvidia-ampere-a100</code></li>
<li><code>nvidia-ampere-a100e</code></li>
<li><code>nvidia-hopper-h100</code></li>
<li><code>nvidia-hopper-h100-1g20gb</code></li>
<li><code>nvidia-hopper-h100-3g40gb</code></li>
</ul>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>high_priority</code>
            </td>
            <td>
                  <code>`Optional[bool]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created
endpoint to leverage the shared pool of prewarmed nodes for faster spinup time</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>post_inference_hooks</code>
            </td>
            <td>
                  <code>`Optional[List[PostInferenceHooks]]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>List of hooks to trigger after inference tasks are served</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>default_callback_url</code>
            </td>
            <td>
                  <code>`Optional[str]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>The default callback url to use for sync completion requests.
This can be overridden in the task parameters for each individual task.
post_inference_hooks must contain "callback" for the callback to be triggered</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>public_inference</code>
            </td>
            <td>
                  <code>`Optional[bool]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If <code>True</code>, this endpoint will be available to all user IDs for
inference</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>labels</code>
            </td>
            <td>
                  <code>`Optional[Dict[str, str]]`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>An optional dictionary of key/value pairs to associate with this endpoint</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:
    UpdateLLMEndpointResponse: creation task ID of the updated Model. Currently not used.</p>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Model.delete" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">delete</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Model.delete" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>delete(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    model_endpoint_name: <span style="color: #008000">str</span>,
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>    request_headers: Optional[Dict[<span style="color: #008000">str</span>, <span style="color: #008000">str</span>]] <span style="color: #666">=</span> <span style="color: #008000; font-weight: bold">None</span>,
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>) <span style="color: #666">-&gt;</span> DeleteLLMEndpointResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Deletes an LLM model.</p>
<p>This API can be used to delete a fine-tuned model. It takes
as parameter the name of the <code>model</code> and returns a response
object which has a <code>deleted</code> field confirming if the deletion
was successful. If called on a base model included with LLM
Engine, an error will be thrown.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model_endpoint_name</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Name of the model endpoint to be deleted</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>response</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.DeleteLLMEndpointResponse" href="../data_types/#llmengine.DeleteLLMEndpointResponse">DeleteLLMEndpointResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>whether the model endpoint was successfully deleted</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.Model.delete--__tabbed_1_1" name="llmengine.Model.delete--__tabbed_1" type="radio" /><input id="llmengine.Model.delete--__tabbed_1_2" name="llmengine.Model.delete--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Model.delete--__tabbed_1_1">Deleting model in Python</label><label for="llmengine.Model.delete--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>delete(<span style="color: #BA2121">&quot;llama-2-7b.suffix.2023-07-18-12-00-00&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;deleted&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">true</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.Model.download" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">download</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.Model.download" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>download(
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>    model_name: <span style="color: #008000">str</span>, download_format: <span style="color: #008000">str</span> <span style="color: #666">=</span> <span style="color: #BA2121">&quot;hugging_face&quot;</span>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>) <span style="color: #666">-&gt;</span> ModelDownloadResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Download a fine-tuned model.</p>
<p>This API can be used to download the resulting model from a fine-tuning job.
It takes the <code>model_name</code> and <code>download_format</code> as parameter and returns a
response object which contains a dictonary of filename, url pairs associated
with the fine-tuned model. The user can then download these urls to obtain
the fine-tuned model. If called on a nonexistent model, an error will be thrown.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>model_name</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>name of the fine-tuned model</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>download_format</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>download format requested (default=hugging_face)</p>
              </div>
            </td>
            <td>
                  <code>&#39;hugging_face&#39;</code>
            </td>
          </tr>
      </tbody>
    </table>
        <p>Returns:
    DownloadModelResponse: an object that contains a dictionary of filenames, urls from which to download the model weights.
    The urls are presigned urls that grant temporary access and expire after an hour.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.Model.download--__tabbed_1_1" name="llmengine.Model.download--__tabbed_1" type="radio" /><input id="llmengine.Model.download--__tabbed_1_2" name="llmengine.Model.download--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.Model.download--__tabbed_1_1">Downloading model in Python</label><label for="llmengine.Model.download--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> Model
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> Model<span style="color: #666">.</span>download(<span style="color: #BA2121">&quot;llama-2-7b.suffix.2023-07-18-12-00-00&quot;</span>, download_format<span style="color: #666">=</span><span style="color: #BA2121">&quot;hugging_face&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;urls&quot;</span>:<span style="color: #BBB"> </span>{<span style="color: #008000; font-weight: bold">&quot;my_model_file&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;https://url-to-my-model-weights&quot;</span>}
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="llmengine.File" class="doc doc-heading">
            <span class="doc doc-object-name doc-class-name">File</span>


<a href="#llmengine.File" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents first">
            <p class="doc doc-class-bases">
              Bases: <code><span title="llmengine.api_engine.APIEngine">APIEngine</span></code></p>


        <p>File API. This API is used to upload private files to LLM engine so that fine-tunes can access them for training and validation data.</p>
<p>Functions are provided to upload, get, list, and delete files, as well as to get the contents of a file.</p>










  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="llmengine.File.upload" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">upload</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.File.upload" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>upload(file: BufferedReader) <span style="color: #666">-&gt;</span> UploadFileResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Uploads a file to LLM engine.</p>
<p>For use in <a href="./#llmengine.fine_tuning.FineTune.create">FineTune creation</a>, this should be a CSV file with two columns: <code>prompt</code> and <code>response</code>.
A maximum of 100,000 rows of data is currently supported.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>file</code>
            </td>
            <td>
                  <code>`BufferedReader`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A local file opened with <code>open(file_path, "r")</code></p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>UploadFileResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.UploadFileResponse" href="../data_types/#llmengine.UploadFileResponse">UploadFileResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains the ID of the uploaded file</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.File.upload--__tabbed_1_1" name="llmengine.File.upload--__tabbed_1" type="radio" /><input id="llmengine.File.upload--__tabbed_1_2" name="llmengine.File.upload--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.File.upload--__tabbed_1_1">Uploading file in Python</label><label for="llmengine.File.upload--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> File
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> File<span style="color: #666">.</span>upload(<span style="color: #008000">open</span>(<span style="color: #BA2121">&quot;training_dataset.csv&quot;</span>, <span style="color: #BA2121">&quot;r&quot;</span>))
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;file-abc123&quot;</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.File.get" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">get</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.File.get" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>get(file_id: <span style="color: #008000">str</span>) <span style="color: #666">-&gt;</span> GetFileResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Get file metadata, including filename and size.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>file_id</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ID of the file</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>GetFileResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.GetFileResponse" href="../data_types/#llmengine.GetFileResponse">GetFileResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains the ID, filename, and size of the requested file</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.File.get--__tabbed_1_1" name="llmengine.File.get--__tabbed_1" type="radio" /><input id="llmengine.File.get--__tabbed_1_2" name="llmengine.File.get--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.File.get--__tabbed_1_1">Getting metadata about file in Python</label><label for="llmengine.File.get--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> File
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> File<span style="color: #666">.</span>get(
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>    file_id<span style="color: #666">=</span><span style="color: #BA2121">&quot;file-abc123&quot;</span>,
<a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a>)
<a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a>
<a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;file-abc123&quot;</span>,
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;filename&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;training_dataset.csv&quot;</span>,
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;size&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">100</span>
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.File.download" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">download</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.File.download" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>download(file_id: <span style="color: #008000">str</span>) <span style="color: #666">-&gt;</span> GetFileContentResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Get contents of a file, as a string. (If the uploaded file is in binary, a string encoding will be returned.)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>file_id</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ID of the file</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>GetFileContentResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.GetFileContentResponse" href="../data_types/#llmengine.GetFileContentResponse">GetFileContentResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains the ID and content of the file</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.File.download--__tabbed_1_1" name="llmengine.File.download--__tabbed_1" type="radio" /><input id="llmengine.File.download--__tabbed_1_2" name="llmengine.File.download--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.File.download--__tabbed_1_1">Getting file content in Python</label><label for="llmengine.File.download--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> File
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> File<span style="color: #666">.</span>download(file_id<span style="color: #666">=</span><span style="color: #BA2121">&quot;file-abc123&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;file-abc123&quot;</span>,
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;content&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;Hello world!&quot;</span>
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.File.list" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">list</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.File.list" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000">list</span>() <span style="color: #666">-&gt;</span> ListFilesResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>List metadata about all files, e.g. their filenames and sizes.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>ListFilesResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.ListFilesResponse" href="../data_types/#llmengine.ListFilesResponse">ListFilesResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains a list of all files and their filenames and sizes</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.File.list--__tabbed_1_1" name="llmengine.File.list--__tabbed_1" type="radio" /><input id="llmengine.File.list--__tabbed_1_2" name="llmengine.File.list--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.File.list--__tabbed_1_1">Listing files in Python</label><label for="llmengine.File.list--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> File
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> File<span style="color: #666">.</span>list()
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;files&quot;</span>:<span style="color: #BBB"> </span>[
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;file-abc123&quot;</span>,
<a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;filename&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;training_dataset.csv&quot;</span>,
<a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;size&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">100</span>
<a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span style="color: #BBB">        </span>},
<a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span style="color: #BBB">        </span>{
<a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;id&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;file-def456&quot;</span>,
<a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;filename&quot;</span>:<span style="color: #BBB"> </span><span style="color: #BA2121">&quot;validation_dataset.csv&quot;</span>,
<a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span style="color: #BBB">            </span><span style="color: #008000; font-weight: bold">&quot;size&quot;</span>:<span style="color: #BBB"> </span><span style="color: #666">50</span>
<a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a><span style="color: #BBB">        </span>}
<a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a><span style="color: #BBB">    </span>]
<a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="llmengine.File.delete" class="doc doc-heading">
            <span class="doc doc-object-name doc-function-name">delete</span>


  <span class="doc doc-labels">
      <small class="doc doc-label doc-label-classmethod"><code>classmethod</code></small>
  </span>

<a href="#llmengine.File.delete" class="headerlink" title="Permanent link">&para;</a></h3>
<div class="doc-signature highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>delete(file_id: <span style="color: #008000">str</span>) <span style="color: #666">-&gt;</span> DeleteFileResponse
</code></pre></div>

    <div class="doc doc-contents ">

        <p>Deletes a file.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>file_id</code>
            </td>
            <td>
                  <code>`str`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>ID of the file</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
<th>Name</th>          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
<td><code>DeleteFileResponse</code></td>            <td>
                  <code><a class="autorefs autorefs-internal" title="llmengine.data_types.DeleteFileResponse" href="../data_types/#llmengine.DeleteFileResponse">DeleteFileResponse</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>an object that contains whether the deletion was successful</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="llmengine.File.delete--__tabbed_1_1" name="llmengine.File.delete--__tabbed_1" type="radio" /><input id="llmengine.File.delete--__tabbed_1_2" name="llmengine.File.delete--__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="llmengine.File.delete--__tabbed_1_1">Deleting file in Python</label><label for="llmengine.File.delete--__tabbed_1_2">Response in JSON</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span style="color: #008000; font-weight: bold">from</span><span style="color: #BBB"> </span><span style="color: #00F; font-weight: bold">llmengine</span><span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">import</span> File
<a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
<a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>response <span style="color: #666">=</span> File<span style="color: #666">.</span>delete(file_id<span style="color: #666">=</span><span style="color: #BA2121">&quot;file-abc123&quot;</span>)
<a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span style="color: #008000">print</span>(response<span style="color: #666">.</span>json())
</code></pre></div>
</div>
<div class="tabbed-block">
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%;"><span></span><code><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>{
<a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span style="color: #BBB">    </span><span style="color: #008000; font-weight: bold">&quot;deleted&quot;</span>:<span style="color: #BBB"> </span><span style="color: #008000; font-weight: bold">true</span>
<a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>}
</code></pre></div>
</div>
</div>
</div>


    </div>

</div>



  </div>

    </div>

</div>









  



  <form class="md-feedback" name="feedback" hidden>
    <fieldset>
      <legend class="md-feedback__title">
        Was this page helpful?
      </legend>
      <div class="md-feedback__inner">
        <div class="md-feedback__list">
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page was helpful" data-md-value="1">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m7 0c0 .8-.7 1.5-1.5 1.5S14 10.3 14 9.5 14.7 8 15.5 8s1.5.7 1.5 1.5m-5 7.73c-1.75 0-3.29-.73-4.19-1.81L9.23 14c.45.72 1.52 1.23 2.77 1.23s2.32-.51 2.77-1.23l1.42 1.42c-.9 1.08-2.44 1.81-4.19 1.81"/></svg>
            </button>
          
            <button class="md-feedback__icon md-icon" type="submit" title="This page could be improved" data-md-value="0">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 12a8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8 8 8 0 0 0 8-8m2 0a10 10 0 0 1-10 10A10 10 0 0 1 2 12 10 10 0 0 1 12 2a10 10 0 0 1 10 10m-6.5-4c.8 0 1.5.7 1.5 1.5s-.7 1.5-1.5 1.5-1.5-.7-1.5-1.5.7-1.5 1.5-1.5M10 9.5c0 .8-.7 1.5-1.5 1.5S7 10.3 7 9.5 7.7 8 8.5 8s1.5.7 1.5 1.5m2 4.5c1.75 0 3.29.72 4.19 1.81l-1.42 1.42C14.32 16.5 13.25 16 12 16s-2.32.5-2.77 1.23l-1.42-1.42C8.71 14.72 10.25 14 12 14"/></svg>
            </button>
          
        </div>
        <div class="md-feedback__note">
          
            <div data-md-value="1" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback!
            </div>
          
            <div data-md-value="0" hidden>
              
              
                
              
              
              
                
                
              
              Thanks for your feedback! Help us improve this page by using our <a href="..." target="_blank" rel="noopener">feedback form</a>.
            </div>
          
        </div>
      </div>
    </fieldset>
  </form>


                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["search.suggest", "search.highlight", "content.tabs.link", "content.code.annotate", "navigation.expand", "content.code.copy"], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.c8b220af.min.js"></script>
      
    
  </body>
</html>