{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#llm-engine","title":"LLM Engine","text":"<p>The open source engine for fine-tuning and serving large language models.</p> <p>LLM Engine is the easiest way to customize and serve LLMs.</p> <p>LLMs can be accessed via Scale's hosted version or by using the Helm charts in this repository to run model inference and fine-tuning in your own infrastructure.</p>"},{"location":"#quick-install","title":"Quick Install","text":"Install the python package <pre><code>pip install scale-llm-engine\n</code></pre>"},{"location":"#about","title":"About","text":"<p>Foundation models are emerging as the building blocks of AI. However, deploying these models to the  cloud and fine-tuning them is an expensive operation that require infrastructure and ML expertise.  It is also difficult to maintain over time as new models are released and new techniques for both inference and fine-tuning are made available.</p> <p>LLM Engine is a Python library and Helm chart that provides everything you need to serve and fine-tune foundation models, whether you use Scale's hosted infrastructure or do it in your own cloud infrastructure using Kubernetes.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>Ready-to-use APIs for your favorite models: Deploy and serve open source foundation models - including Llama-2, MPT, and Falcon. Use Scale-hosted models or deploy to your own infrastructure.</p> <p>Fine-tune the best open-source models: Fine-tune open-source foundation models like Llama-2, MPT, etc. with your own data for optimized performance.</p> <p>Optimized Inference: LLM Engine provides inference APIs for streaming responses and dynamically batching inputs for higher throughput and lower latency.</p> <p>Open-Source Integrations: Deploy any Hugging Face model with a single command.</p> <p>Deploying from any docker image: Turn any Docker image into an auto-scaling deployment with simple APIs.</p>"},{"location":"#features-coming-soon","title":"Features Coming Soon","text":"<p>Kubernetes Installation Enhancements: We are working hard to enhance the  installation and maintenance of inference and fine-tuning functionality on  your infrastructure. For now, our documentation covers experimental libraries to deploy language models on your infrastructure  and libraries to access Scale's hosted infrastructure.</p> <p>Fast Cold-Start Times: To prevent GPUs from idling, LLM Engine automatically scales your model to zero when it's not in use and scales up within seconds, even for large foundation models.</p> <p>Cost Optimization: Deploy AI models cheaper than commercial ones, including cold-start and warm-down times.</p>"},{"location":"contributing/","title":"Contributing to LLM Engine","text":""},{"location":"contributing/#updating-llm-engine-documentation","title":"Updating LLM Engine Documentation","text":"<p>LLM Engine leverages mkdocs to create beautiful, community-oriented documentation.</p>"},{"location":"contributing/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Clone/Fork the LLM Engine Repository. Our documentation lives in the <code>docs</code> folder.</p>"},{"location":"contributing/#step-2-install-the-dependencies","title":"Step 2: Install the Dependencies","text":"<p>Dependencies are located in <code>requirements-docs.txt</code>, go ahead and pip install those with </p> <pre><code>pip install -r requirements-docs.txt\n</code></pre>"},{"location":"contributing/#step-3-install-the-python-client-locally","title":"Step 3: Install the Python client locally","text":"<p>Our Python client API reference is autogenerated from our client. You can install the client in editable mode with</p> <pre><code>pip install -e clients/python\n</code></pre>"},{"location":"contributing/#step-4-run-locally","title":"Step 4: Run Locally","text":"<p>To run the documentation service locally, execute the following command:</p> <pre><code>mkdocs serve\n</code></pre> <p>This should kick off a locally running instance on http://127.0.0.1:8000/.</p> <p>As you edit the content in the <code>docs</code> folder, the site will be automatically reloaded on each file save.</p>"},{"location":"contributing/#step-5-editing-navigation-and-settings","title":"Step 5: Editing Navigation and Settings","text":"<p>If you are less familiar with <code>mkdocs</code>, in addition to the markdown content in the <code>docs</code> folder, there is a top-level  <code>mkdocs.yml</code> file as well that defines the navigation pane and other website settings. If you don't see your page where  you think it should be, double-check the .yml file.</p>"},{"location":"contributing/#step-6-building-and-deploying","title":"Step 6: Building and Deploying","text":"<p>CircleCI (via <code>.circleci/config.yml</code>) handles the building and deployment of our documentation service for us.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"getting_started/","title":"Getting Started","text":"<p>The fastest way to get started with LLM Engine is to use the Python client in this repository to  run inference and fine-tuning on Scale's infrastructure. This path does not require you to install  anything on your infrastructure, and Scale's free research preview gives you access to experimentation using open source LLMs.</p> <p>To start, install LLM Engine via pip:</p> pip <pre><code>pip install scale-llm-engine\n</code></pre>"},{"location":"getting_started/#scale-api-keys","title":"Scale API Keys","text":"<p>Next, you need a Scale Spellbook API key.</p>"},{"location":"getting_started/#retrieving-your-api-key","title":"Retrieving your API Key","text":"<p>To retrieve your API key, head to Scale Spellbook where you will get an API key on the settings page.</p> <p>Different API Keys for different Scale Products</p> <p>If you have leveraged Scale's platform for annotation work in the past, please note that your Spellbook API key will be different than the Scale Annotation API key. You will want to create a Spellbook API key before getting started.</p>"},{"location":"getting_started/#set-your-api-key","title":"Set your API Key","text":"<p>LLM Engine uses environment variables to access your API key.</p> <p>Set this API key as the <code>SCALE_API_KEY</code> environment variable by running the following command in your terminal before you run your python application.</p> <pre><code>export SCALE_API_KEY=\"[Your API key]\"\n</code></pre> <p>You can also add in the line above to your <code>.zshrc</code> or <code>.bash_profile</code> so it's automatically set for future sessions.</p> <p>Alternatively, you can also set your API key using either of the following patterns: <pre><code>llmengine.api_engine.api_key = \"abc\"\nllmengine.api_engine.set_api_key(\"abc\")\n</code></pre> These patterns are useful for Jupyter Notebook users to set API keys without the need for using <code>os.environ</code>.</p>"},{"location":"getting_started/#example-code","title":"Example Code","text":""},{"location":"getting_started/#sample-completion","title":"Sample Completion","text":"<p>With your API key set, you can now send LLM Engine requests using the Python client:</p> <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"I'm opening a pancake restaurant that specializes in unique pancake shapes, colors, and flavors. List 3 quirky names I could name my restaurant.\",\n    max_new_tokens=100,\n    temperature=0.2,\n)\n\nprint(response.output.text)\n</code></pre>"},{"location":"getting_started/#with-streaming","title":"With Streaming","text":"<pre><code>import sys\nfrom llmengine import Completion\n\nstream = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Give me a 200 word summary on the current economic events in the US.\",\n    max_new_tokens=1000,\n    temperature=0.2,\n    stream=True,\n)\n\nfor response in stream:\n    if response.output:\n        print(response.output.text, end=\"\")\n        sys.stdout.flush()\n    else: # an error occurred\nprint(response.error) # print the error message out \nbreak\n</code></pre>"},{"location":"integrations/","title":"Integrations","text":""},{"location":"integrations/#weights-biases","title":"Weights &amp; Biases","text":"<p>LLM Engine integrates with Weights &amp; Biases to track metrics during fine tuning. To enable:</p> <pre><code>from llmengine import FineTune\n\nresponse = FineTune.create(\n    model=\"llama-2-7b\",\n    training_file=\"s3://my-bucket/path/to/training-file.csv\",\n    validation_file=\"s3://my-bucket/path/to/validation-file.csv\",\n    hyperparameters={\"report_to\": \"wandb\"},\n    wandb_config={\"api_key\":\"key\", \"project\":\"fine-tune project\"}\n)\n</code></pre> <p>Configs to specify:</p> <ul> <li>(Required) Set <code>hyperparameters.report_to</code> to <code>wandb</code> to enables automatic metrics tracking.</li> <li>(Required) Set <code>wandb_config.api_key</code> to the API key.</li> <li>(Optional) Set <code>wandb_config.base_url</code> to use a custom Weights &amp; Biases server.</li> <li><code>wandb_config</code> also accepts keys from wandb.init().</li> </ul>"},{"location":"model_zoo/","title":"Public Model Zoo","text":"<p>Scale hosts the following models in the LLM Engine Model Zoo:</p> Model Name Inference APIs Available Fine-tuning APIs Available Inference Frameworks Available Inference max total tokens (prompt + response) <code>llama-7b</code> \u2705 \u2705 deepspeed, text-generation-inference 2048 <code>llama-2-7b</code> \u2705 \u2705 text-generation-inference, vllm 4096 <code>llama-2-7b-chat</code> \u2705 text-generation-inference, vllm 4096 <code>llama-2-13b</code> \u2705 text-generation-inference, vllm 4096 <code>llama-2-13b-chat</code> \u2705 text-generation-inference, vllm 4096 <code>llama-2-70b</code> \u2705 \u2705 text-generation-inference, vllm 4096 <code>llama-2-70b-chat</code> \u2705 text-generation-inference, vllm 4096 <code>llama-3-8b</code> \u2705 vllm 8192 <code>llama-3-8b-instruct</code> \u2705 vllm 8192 <code>llama-3-70b</code> \u2705 vllm 8192 <code>llama-3-70b-instruct</code> \u2705 vllm 8192 <code>llama-3-1-8b</code> \u2705 vllm 131072 <code>llama-3-1-8b-instruct</code> \u2705 vllm 131072 <code>llama-3-1-70b</code> \u2705 vllm 131072 <code>llama-3-1-70b-instruct</code> \u2705 vllm 131072 <code>falcon-7b</code> \u2705 text-generation-inference, vllm 2048 <code>falcon-7b-instruct</code> \u2705 text-generation-inference, vllm 2048 <code>falcon-40b</code> \u2705 text-generation-inference, vllm 2048 <code>falcon-40b-instruct</code> \u2705 text-generation-inference, vllm 2048 <code>mpt-7b</code> \u2705 deepspeed, text-generation-inference, vllm 2048 <code>mpt-7b-instruct</code> \u2705 \u2705 deepspeed, text-generation-inference, vllm 2048 <code>flan-t5-xxl</code> \u2705 deepspeed, text-generation-inference 2048 <code>mistral-7b</code> \u2705 \u2705 vllm 8000 <code>mistral-7b-instruct</code> \u2705 \u2705 vllm 8000 <code>mixtral-8x7b</code> \u2705 vllm 32768 <code>mixtral-8x7b-instruct</code> \u2705 vllm 32768 <code>mixtral-8x22b</code> \u2705 vllm 65536 <code>mixtral-8x22b-instruct</code> \u2705 vllm 65536 <code>codellama-7b</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-7b-instruct</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-13b</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-13b-instruct</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-34b</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-34b-instruct</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-70b</code> \u2705 vllm 16384 <code>codellama-70b-instruct</code> \u2705 vllm 4096 <code>zephyr-7b-alpha</code> \u2705 text-generation-inference, vllm 32768 <code>zephyr-7b-beta</code> \u2705 text-generation-inference, vllm 32768 <code>gemma-2b</code> \u2705 vllm 8192 <code>gemma-2b-instruct</code> \u2705 vllm 8192 <code>gemma-7b</code> \u2705 vllm 8192 <code>gemma-7b-instruct</code> \u2705 vllm 8192 <code>phi-3-mini-4k-instruct</code> \u2705 vllm 4096 <code>deepseek-coder-v2</code> \u2705 vllm 131072 <code>deepseek-coder-v2-instruct</code> \u2705 vllm 131072 <code>deepseek-coder-v2-lite</code> \u2705 vllm 131072 <code>deepseek-coder-v2-lite-instruct</code> \u2705 vllm 131072"},{"location":"model_zoo/#usage","title":"Usage","text":"<p>Each of these models can be used with the Completion API.</p> <p>The specified models can be fine-tuned with the FineTune API.</p> <p>More information about the models can be found using the Model API.</p>"},{"location":"pricing/","title":"Pricing","text":"<p>LLM Engine is an open-source project and free self-hosting will always be an option.</p> <p>A hosted option for LLM Engine is being offered initially as a free preview via Scale Spellbook.</p>"},{"location":"pricing/#self-hosted-models","title":"Self-Hosted Models","text":"<p>We are committed to supporting the open-source community. Self-hosting LLM Engine will remain free and open-source.</p> <p>We would love contributions from the community make this even more amazing!</p>"},{"location":"pricing/#hosted-models","title":"Hosted Models","text":"<p>Once the limited preview period has ended, billing for hosted models will be managed through the Scale Spellbook product.</p> <p>Scale Spellbook leverages usage-based spending, billed to a credit card. Details on usage-based pricing will be shared with everyone before completing the limited preview.</p>"},{"location":"api/data_types/","title":"\ud83d\udc0d Python Client Data Type Reference","text":""},{"location":"api/data_types/#llmengine.CompletionOutput","title":"CompletionOutput","text":"<p>             Bases: <code>BaseModel</code></p> <p>Represents the output of a completion request to a model.</p>"},{"location":"api/data_types/#llmengine.CompletionOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text of the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/data_types/#llmengine.CompletionOutput.num_completion_tokens","title":"num_completion_tokens  <code>instance-attribute</code>","text":"<pre><code>num_completion_tokens: int\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput","title":"CompletionStreamOutput","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text of the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.finished","title":"finished  <code>instance-attribute</code>","text":"<pre><code>finished: bool\n</code></pre> <p>Whether the completion is finished.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.num_completion_tokens","title":"num_completion_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_completion_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionSyncResponse","title":"CompletionSyncResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for a synchronous prompt completion.</p>"},{"location":"api/data_types/#llmengine.CompletionSyncResponse.request_id","title":"request_id  <code>instance-attribute</code>","text":"<pre><code>request_id: str\n</code></pre> <p>The unique ID of the corresponding Completion request. This <code>request_id</code> is generated on the server, and all logs  associated with the request are grouped by the <code>request_id</code>, which allows for easier troubleshooting of errors as follows:</p> <ul> <li>When running the Scale-hosted LLM Engine, please provide the <code>request_id</code> in any bug reports.</li> <li>When running the self-hosted LLM Engine, the <code>request_id</code> serves as a trace ID in your observability  provider.</li> </ul>"},{"location":"api/data_types/#llmengine.CompletionSyncResponse.output","title":"output  <code>instance-attribute</code>","text":"<pre><code>output: CompletionOutput\n</code></pre> <p>Completion output.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamResponse","title":"CompletionStreamResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for a stream prompt completion task.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamResponse.request_id","title":"request_id  <code>instance-attribute</code>","text":"<pre><code>request_id: str\n</code></pre> <p>The unique ID of the corresponding Completion request. This <code>request_id</code> is generated on the server, and all logs  associated with the request are grouped by the <code>request_id</code>, which allows for easier troubleshooting of errors as follows:</p> <ul> <li>When running the Scale-hosted LLM Engine, please provide the <code>request_id</code> in any bug reports.</li> <li>When running the self-hosted LLM Engine, the <code>request_id</code> serves as a trace ID in your observability  provider.</li> </ul>"},{"location":"api/data_types/#llmengine.CompletionStreamResponse.output","title":"output  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output: Optional[CompletionStreamOutput] = None\n</code></pre> <p>Completion output.</p>"},{"location":"api/data_types/#llmengine.CreateFineTuneResponse","title":"CreateFineTuneResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for creating a FineTune.</p>"},{"location":"api/data_types/#llmengine.CreateFineTuneResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(\n    ..., description=\"ID of the created fine-tuning job.\"\n)\n</code></pre> <p>The ID of the FineTune.</p>"},{"location":"api/data_types/#llmengine.GetFineTuneResponse","title":"GetFineTuneResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for retrieving a FineTune.</p>"},{"location":"api/data_types/#llmengine.GetFineTuneResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(..., description=\"ID of the requested job.\")\n</code></pre> <p>The ID of the FineTune.</p>"},{"location":"api/data_types/#llmengine.GetFineTuneResponse.fine_tuned_model","title":"fine_tuned_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fine_tuned_model: Optional[str] = Field(\n    default=None,\n    description=\"Name of the resulting fine-tuned model. This can be plugged into the Completion API once the fine-tune is complete\",\n)\n</code></pre> <p>The name of the resulting fine-tuned model. This can be plugged into the Completion API once the fine-tune is complete.</p>"},{"location":"api/data_types/#llmengine.ListFineTunesResponse","title":"ListFineTunesResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for listing FineTunes.</p>"},{"location":"api/data_types/#llmengine.ListFineTunesResponse.jobs","title":"jobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>jobs: List[GetFineTuneResponse] = Field(\n    ...,\n    description=\"List of fine-tuning jobs and their statuses.\",\n)\n</code></pre> <p>A list of FineTunes, represented as <code>GetFineTuneResponse</code>s.</p>"},{"location":"api/data_types/#llmengine.CancelFineTuneResponse","title":"CancelFineTuneResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for cancelling a FineTune.</p>"},{"location":"api/data_types/#llmengine.CancelFineTuneResponse.success","title":"success  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>success: bool = Field(\n    ..., description=\"Whether cancellation was successful.\"\n)\n</code></pre> <p>Whether the cancellation succeeded.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse","title":"GetLLMEndpointResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for retrieving a Model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(\n    description=\"The name of the model. Use this for making inference requests to the model.\"\n)\n</code></pre> <p>The name of the model. Use this for making inference requests to the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: LLMSource = Field(\n    description=\"The source of the model, e.g. Hugging Face.\"\n)\n</code></pre> <p>The source of the model, e.g. Hugging Face.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.inference_framework","title":"inference_framework  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inference_framework: LLMInferenceFramework = Field(\n    description=\"The inference framework used by the model.\"\n)\n</code></pre> <p>(For self-hosted users) The inference framework used by the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Optional[str] = Field(\n    default=None,\n    description=\"(For self-hosted users) The autogenerated ID of the model.\",\n)\n</code></pre> <p>(For self-hosted users) The autogenerated ID of the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: Optional[str] = Field(\n    default=None,\n    description=\"(For self-hosted users) For fine-tuned models, the base model. For base models, this will be the same as `name`.\",\n)\n</code></pre> <p>(For self-hosted users) For fine-tuned models, the base model. For base models, this will be the same as <code>name</code>.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: ModelEndpointStatus = Field(\n    description=\"The status of the model.\"\n)\n</code></pre> <p>The status of the model (can be one of \"READY\", \"UPDATE_PENDING\", \"UPDATE_IN_PROGRESS\", \"UPDATE_FAILED\", \"DELETE_IN_PROGRESS\").</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.inference_framework_tag","title":"inference_framework_tag  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inference_framework_tag: Optional[str] = Field(\n    default=None,\n    description=\"(For self-hosted users) The Docker image tag used to run the model.\",\n)\n</code></pre> <p>(For self-hosted users) The Docker image tag used to run the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.num_shards","title":"num_shards  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_shards: Optional[int] = Field(\n    default=None,\n    description=\"(For self-hosted users) The number of shards.\",\n)\n</code></pre> <p>(For self-hosted users) The number of shards.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.quantize","title":"quantize  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quantize: Optional[Quantization] = Field(\n    default=None,\n    description=\"(For self-hosted users) The quantization method.\",\n)\n</code></pre> <p>(For self-hosted users) The quantization method.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.spec","title":"spec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>spec: Optional[GetModelEndpointResponse] = Field(\n    default=None,\n    description=\"(For self-hosted users) Model endpoint details.\",\n)\n</code></pre> <p>(For self-hosted users) Model endpoint details.</p>"},{"location":"api/data_types/#llmengine.ListLLMEndpointsResponse","title":"ListLLMEndpointsResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for listing Models.</p>"},{"location":"api/data_types/#llmengine.ListLLMEndpointsResponse.model_endpoints","title":"model_endpoints  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_endpoints: List[GetLLMEndpointResponse] = Field(\n    ..., description=\"The list of models.\"\n)\n</code></pre> <p>A list of Models, represented as <code>GetLLMEndpointResponse</code>s.</p>"},{"location":"api/data_types/#llmengine.DeleteLLMEndpointResponse","title":"DeleteLLMEndpointResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for deleting a Model.</p>"},{"location":"api/data_types/#llmengine.DeleteLLMEndpointResponse.deleted","title":"deleted  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deleted: bool = Field(\n    ..., description=\"Whether deletion was successful.\"\n)\n</code></pre> <p>Whether the deletion succeeded.</p>"},{"location":"api/data_types/#llmengine.ModelDownloadRequest","title":"ModelDownloadRequest","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request object for downloading a model.</p>"},{"location":"api/data_types/#llmengine.ModelDownloadRequest.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str = Field(\n    ..., description=\"Name of the model to download.\"\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.ModelDownloadRequest.download_format","title":"download_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>download_format: Optional[str] = Field(\n    default=\"hugging_face\",\n    description=\"Desired return format for downloaded model weights (default=hugging_face).\",\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.ModelDownloadResponse","title":"ModelDownloadResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for downloading a model.</p>"},{"location":"api/data_types/#llmengine.ModelDownloadResponse.urls","title":"urls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>urls: Dict[str, str] = Field(\n    ...,\n    description=\"Dictionary of (file_name, url) pairs to download the model from.\",\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.UploadFileResponse","title":"UploadFileResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for uploading a file.</p>"},{"location":"api/data_types/#llmengine.UploadFileResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(..., description=\"ID of the uploaded file.\")\n</code></pre> <p>ID of the uploaded file.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse","title":"GetFileResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for retrieving a file.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(\n    ..., description=\"ID of the requested file.\"\n)\n</code></pre> <p>ID of the requested file.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse.filename","title":"filename  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filename: str = Field(..., description='File name.')\n</code></pre> <p>File name.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse.size","title":"size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>size: int = Field(\n    ..., description=\"Length of the file, in characters.\"\n)\n</code></pre> <p>Length of the file, in characters.</p>"},{"location":"api/data_types/#llmengine.GetFileContentResponse","title":"GetFileContentResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for retrieving a file's content.</p>"},{"location":"api/data_types/#llmengine.GetFileContentResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(\n    ..., description=\"ID of the requested file.\"\n)\n</code></pre> <p>ID of the requested file.</p>"},{"location":"api/data_types/#llmengine.GetFileContentResponse.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: str = Field(..., description='File content.')\n</code></pre> <p>File content.</p>"},{"location":"api/data_types/#llmengine.ListFilesResponse","title":"ListFilesResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for listing files.</p>"},{"location":"api/data_types/#llmengine.ListFilesResponse.files","title":"files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>files: List[GetFileResponse] = Field(\n    ..., description=\"List of file IDs, names, and sizes.\"\n)\n</code></pre> <p>List of file IDs, names, and sizes.</p>"},{"location":"api/data_types/#llmengine.DeleteFileResponse","title":"DeleteFileResponse","text":"<p>             Bases: <code>BaseModel</code></p> <p>Response object for deleting a file.</p>"},{"location":"api/data_types/#llmengine.DeleteFileResponse.deleted","title":"deleted  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deleted: bool = Field(\n    ..., description=\"Whether deletion was successful.\"\n)\n</code></pre> <p>Whether deletion was successful.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent","title":"CreateBatchCompletionsRequestContent","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.prompts","title":"prompts  <code>instance-attribute</code>","text":"<pre><code>prompts: List[str]\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.max_new_tokens","title":"max_new_tokens  <code>instance-attribute</code>","text":"<pre><code>max_new_tokens: int\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.temperature","title":"temperature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>temperature: float = Field(ge=0.0, le=1.0)\n</code></pre> <p>Temperature of the sampling. Setting to 0 equals to greedy sampling.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.stop_sequences","title":"stop_sequences  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop_sequences: Optional[List[str]] = None\n</code></pre> <p>List of sequences to stop the completion at.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.return_token_log_probs","title":"return_token_log_probs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>return_token_log_probs: Optional[bool] = False\n</code></pre> <p>Whether to return the log probabilities of the tokens.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.presence_penalty","title":"presence_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>presence_penalty: Optional[float] = Field(\n    default=None, ge=0.0, le=2.0\n)\n</code></pre> <p>Only supported in vllm, lightllm Penalize new tokens based on whether they appear in the text so far. 0.0 means no penalty</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.frequency_penalty","title":"frequency_penalty  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>frequency_penalty: Optional[float] = Field(\n    default=None, ge=0.0, le=2.0\n)\n</code></pre> <p>Only supported in vllm, lightllm Penalize new tokens based on their existing frequency in the text so far. 0.0 means no penalty</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.top_k","title":"top_k  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_k: Optional[int] = Field(default=None, ge=-1)\n</code></pre> <p>Controls the number of top tokens to consider. -1 means consider all tokens.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent.top_p","title":"top_p  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>top_p: Optional[float] = Field(default=None, gt=0.0, le=1.0)\n</code></pre> <p>Controls the cumulative probability of the top tokens to consider. 1.0 means consider all tokens.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig","title":"CreateBatchCompletionsModelConfig","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig.model","title":"model  <code>instance-attribute</code>","text":"<pre><code>model: str\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig.checkpoint_path","title":"checkpoint_path  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>checkpoint_path: Optional[str] = None\n</code></pre> <p>Path to the checkpoint to load the model from.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig.labels","title":"labels  <code>instance-attribute</code>","text":"<pre><code>labels: Dict[str, str]\n</code></pre> <p>Labels to attach to the batch inference job.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig.num_shards","title":"num_shards  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_shards: Optional[int] = 1\n</code></pre> <p>Suggested number of shards to distribute the model. When not specified, will infer the number of shards based on model config. System may decide to use a different number than the given value.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig.quantize","title":"quantize  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quantize: Optional[Quantization] = None\n</code></pre> <p>Whether to quantize the model.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig.seed","title":"seed  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>seed: Optional[int] = None\n</code></pre> <p>Random seed for the model.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest","title":"CreateBatchCompletionsRequest","text":"<p>             Bases: <code>BaseModel</code></p> <p>Request object for batch completions.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.input_data_path","title":"input_data_path  <code>instance-attribute</code>","text":"<pre><code>input_data_path: Optional[str]\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.output_data_path","title":"output_data_path  <code>instance-attribute</code>","text":"<pre><code>output_data_path: str\n</code></pre> <p>Path to the output file. The output file will be a JSON file of type List[CompletionOutput].</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: Optional[\n    CreateBatchCompletionsRequestContent\n] = None\n</code></pre> <p>Either <code>input_data_path</code> or <code>content</code> needs to be provided. When input_data_path is provided, the input file should be a JSON file of type BatchCompletionsRequestContent.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.model_config","title":"model_config  <code>instance-attribute</code>","text":"<pre><code>model_config: CreateBatchCompletionsModelConfig\n</code></pre> <p>Model configuration for the batch inference. Hardware configurations are inferred.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.data_parallelism","title":"data_parallelism  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>data_parallelism: Optional[int] = Field(\n    default=1, ge=1, le=64\n)\n</code></pre> <p>Number of replicas to run the batch inference. More replicas are slower to schedule but faster to inference.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.max_runtime_sec","title":"max_runtime_sec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_runtime_sec: Optional[int] = Field(\n    default=24 * 3600, ge=1, le=2 * 24 * 3600\n)\n</code></pre> <p>Maximum runtime of the batch inference in seconds. Default to one day.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest.tool_config","title":"tool_config  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tool_config: Optional[ToolConfig] = None\n</code></pre> <p>Configuration for tool use. NOTE: this config is highly experimental and signature will change significantly in future iterations.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsResponse","title":"CreateBatchCompletionsResponse","text":"<p>             Bases: <code>BaseModel</code></p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsResponse.job_id","title":"job_id  <code>instance-attribute</code>","text":"<pre><code>job_id: str\n</code></pre> <p>The ID of the batch completions job.</p>"},{"location":"api/error_handling/","title":"Error handling","text":"<p>LLM Engine uses conventional HTTP response codes to indicate the success or failure of an API request. In general: codes in the <code>2xx</code> range indicate success. Codes in the <code>4xx</code> range indicate indicate an error that failed given the  information provided (e.g. a given Model was not found, or an invalid temperature was specified). Codes in the <code>5xx</code>  range indicate an error with the LLM Engine servers.</p> <p>In the Python client, errors are presented via a set of corresponding Exception classes, which should be caught  and handled by the user accordingly.</p>"},{"location":"api/error_handling/#llmengine.errors.BadRequestError","title":"BadRequestError","text":"<pre><code>BadRequestError(message: str)\n</code></pre> <p>             Bases: <code>Exception</code></p> <p>Corresponds to HTTP 400. Indicates that the request had inputs that were invalid. The user should not attempt to retry the request without changing the inputs.</p>"},{"location":"api/error_handling/#llmengine.errors.UnauthorizedError","title":"UnauthorizedError","text":"<pre><code>UnauthorizedError(message: str)\n</code></pre> <p>             Bases: <code>Exception</code></p> <p>Corresponds to HTTP 401. This means that no valid API key was provided.</p>"},{"location":"api/error_handling/#llmengine.errors.NotFoundError","title":"NotFoundError","text":"<pre><code>NotFoundError(message: str)\n</code></pre> <p>             Bases: <code>Exception</code></p> <p>Corresponds to HTTP 404. This means that the resource (e.g. a Model, FineTune, etc.) could not be found. Note that this can also be returned in some cases where the object might exist, but the user does not have access to the object. This is done to avoid leaking information about the existence or nonexistence of said object that the user does not have access to.</p>"},{"location":"api/error_handling/#llmengine.errors.RateLimitExceededError","title":"RateLimitExceededError","text":"<pre><code>RateLimitExceededError(message: str)\n</code></pre> <p>             Bases: <code>Exception</code></p> <p>Corresponds to HTTP 429. Too many requests hit the API too quickly. We recommend an exponential backoff for retries.</p>"},{"location":"api/error_handling/#llmengine.errors.ServerError","title":"ServerError","text":"<pre><code>ServerError(status_code: int, message: str)\n</code></pre> <p>             Bases: <code>Exception</code></p> <p>Corresponds to HTTP 5xx errors on the server.</p>"},{"location":"api/langchain/","title":"\ud83e\udd9c Langchain","text":"<p>Coming soon!</p>"},{"location":"api/python_client/","title":"\ud83d\udc0d Python Client API Reference","text":""},{"location":"api/python_client/#llmengine.Completion","title":"Completion","text":"<p>             Bases: <code>APIEngine</code></p> <p>Completion API. This API is used to generate text completions.</p> <p>Language models are trained to understand natural language and predict text outputs as a response to their inputs. The inputs are called prompts and the outputs are referred to as completions. LLMs take the input prompts and chunk them into smaller units called tokens to process and generate language. Tokens may include trailing spaces and even sub-words; this process is language dependent.</p> <p>The Completion API can be run either synchronous or asynchronously (via Python <code>asyncio</code>). For each of these modes, you can also choose whether to stream token responses or not.</p>"},{"location":"api/python_client/#llmengine.Completion.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    model: str,\n    prompt: str,\n    max_new_tokens: int = 20,\n    temperature: float = 0.2,\n    stop_sequences: Optional[List[str]] = None,\n    return_token_log_probs: Optional[bool] = False,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    include_stop_str_in_output: Optional[bool] = None,\n    guided_json: Optional[Dict[str, Any]] = None,\n    guided_regex: Optional[str] = None,\n    guided_choice: Optional[List[str]] = None,\n    guided_grammar: Optional[str] = None,\n    timeout: int = COMPLETION_TIMEOUT,\n    stream: bool = False,\n) -&gt; Union[\n    CompletionSyncResponse,\n    Iterator[CompletionStreamResponse],\n]\n</code></pre> <p>Creates a completion for the provided prompt and parameters synchronously.</p> <p>This API can be used to get the LLM to generate a completion synchronously. It takes as parameters the <code>model</code> (see Model Zoo) and the <code>prompt</code>. Optionally it takes <code>max_new_tokens</code>, <code>temperature</code>, <code>timeout</code> and <code>stream</code>. It returns a CompletionSyncResponse if <code>stream=False</code> or an async iterator of CompletionStreamResponse with <code>request_id</code> and <code>outputs</code> fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use. See Model Zoo for a list of Models that are supported.</p> required <code>prompt</code> <code>str</code> <p>The prompt to generate completions for, encoded as a string.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion.</p> <p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See Model Zoo for information on each supported model's context length.</p> <code>20</code> <code>temperature</code> <code>float</code> <p>What sampling temperature to use, in the range <code>[0, 1]</code>. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. When temperature is 0 greedy search is used.</p> <code>0.2</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>One or more sequences where the API will stop generating tokens for the current completion.</p> <code>None</code> <code>return_token_log_probs</code> <code>Optional[bool]</code> <p>Whether to return the log probabilities of generated tokens. When True, the response will include a list of tokens and their log probabilities.</p> <code>False</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Integer that controls the number of top tokens to consider. Range: [1, infinity). -1 means consider all tokens.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Float that controls the cumulative probability of the top tokens to consider. Range: (0.0, 1.0]. 1.0 means consider all tokens.</p> <code>None</code> <code>include_stop_str_in_output</code> <code>Optional[bool]</code> <p>Whether to include the stop sequence in the output. Default to False.</p> <code>None</code> <code>guided_json</code> <code>Optional[Dict[str, Any]]</code> <p>If specified, the output will follow the JSON schema.</p> <code>None</code> <code>guided_regex</code> <code>Optional[str]</code> <p>If specified, the output will follow the regex pattern.</p> <code>None</code> <code>guided_choice</code> <code>Optional[List[str]]</code> <p>If specified, the output will be exactly one of the choices.</p> <code>None</code> <code>guided_grammar</code> <code>Optional[str]</code> <p>If specified, the output will follow the context-free grammar provided.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p> <code>COMPLETION_TIMEOUT</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamResponse]</code>. Otherwise, the return type is a <code>CompletionSyncResponse</code>. When streaming, tokens will be sent as data-only server-sent events.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncResponse, AsyncIterable[CompletionStreamResponse]]</code> <p>The generated response (if <code>stream=False</code>) or iterator of response chunks (if <code>stream=True</code>)</p> Synchronous completion without token streaming in PythonResponse in JSON <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n)\nprint(response.json())\n</code></pre> <pre><code>{\n    \"request_id\": \"8bbd0e83-f94c-465b-a12b-aabad45750a9\",\n    \"output\": {\n        \"text\": \"_______ and I am a _______\",\n        \"num_completion_tokens\": 10\n}\n}\n</code></pre> <p>Token streaming can be used to reduce perceived latency for applications. Here is how applications can use streaming:</p> Synchronous completion with token streaming in PythonResponse in JSON <pre><code>from llmengine import Completion\n\nstream = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"why is the sky blue?\",\n    max_new_tokens=5,\n    temperature=0.2,\n    stream=True,\n)\n\nfor response in stream:\n    if response.output:\n        print(response.json())\n</code></pre> <pre><code>{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"\\n\", \"finished\": false, \"num_completion_tokens\": 1 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"I\", \"finished\": false, \"num_completion_tokens\": 2 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \" don\", \"finished\": false, \"num_completion_tokens\": 3 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"\u2019\", \"finished\": false, \"num_completion_tokens\": 4 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"t\", \"finished\": true, \"num_completion_tokens\": 5 } }\n</code></pre>"},{"location":"api/python_client/#llmengine.Completion.acreate","title":"acreate  <code>async</code> <code>classmethod</code>","text":"<pre><code>acreate(\n    model: str,\n    prompt: str,\n    max_new_tokens: int = 20,\n    temperature: float = 0.2,\n    stop_sequences: Optional[List[str]] = None,\n    return_token_log_probs: Optional[bool] = False,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    include_stop_str_in_output: Optional[bool] = None,\n    guided_json: Optional[Dict[str, Any]] = None,\n    guided_regex: Optional[str] = None,\n    guided_choice: Optional[List[str]] = None,\n    guided_grammar: Optional[str] = None,\n    timeout: int = COMPLETION_TIMEOUT,\n    stream: bool = False,\n) -&gt; Union[\n    CompletionSyncResponse,\n    AsyncIterable[CompletionStreamResponse],\n]\n</code></pre> <p>Creates a completion for the provided prompt and parameters asynchronously (with <code>asyncio</code>).</p> <p>This API can be used to get the LLM to generate a completion asynchronously. It takes as parameters the <code>model</code> (see Model Zoo) and the <code>prompt</code>. Optionally it takes <code>max_new_tokens</code>, <code>temperature</code>, <code>timeout</code> and <code>stream</code>. It returns a CompletionSyncResponse if <code>stream=False</code> or an async iterator of CompletionStreamResponse with <code>request_id</code> and <code>outputs</code> fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use. See Model Zoo for a list of Models that are supported.</p> required <code>prompt</code> <code>str</code> <p>The prompt to generate completions for, encoded as a string.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion.</p> <p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See Model Zoo for information on each supported model's context length.</p> <code>20</code> <code>temperature</code> <code>float</code> <p>What sampling temperature to use, in the range <code>[0, 1]</code>. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. When temperature is 0 greedy search is used.</p> <code>0.2</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>One or more sequences where the API will stop generating tokens for the current completion.</p> <code>None</code> <code>return_token_log_probs</code> <code>Optional[bool]</code> <p>Whether to return the log probabilities of generated tokens. When True, the response will include a list of tokens and their log probabilities.</p> <code>False</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Integer that controls the number of top tokens to consider. Range: [1, infinity). -1 means consider all tokens.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Float that controls the cumulative probability of the top tokens to consider. Range: (0.0, 1.0]. 1.0 means consider all tokens.</p> <code>None</code> <code>include_stop_str_in_output</code> <code>Optional[bool]</code> <p>Whether to include the stop sequence in the output. Default to False.</p> <code>None</code> <code>guided_json</code> <code>Optional[Dict[str, Any]]</code> <p>If specified, the output will follow the JSON schema. For examples see https://json-schema.org/learn/miscellaneous-examples.</p> <code>None</code> <code>guided_regex</code> <code>Optional[str]</code> <p>If specified, the output will follow the regex pattern.</p> <code>None</code> <code>guided_choice</code> <code>Optional[List[str]]</code> <p>If specified, the output will be exactly one of the choices.</p> <code>None</code> <code>guided_grammar</code> <code>Optional[str]</code> <p>If specified, the output will follow the context-free grammar provided.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p> <code>COMPLETION_TIMEOUT</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamResponse]</code>. Otherwise, the return type is a <code>CompletionSyncResponse</code>. When streaming, tokens will be sent as data-only server-sent events.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncResponse, AsyncIterable[CompletionStreamResponse]]</code> <p>The generated response (if <code>stream=False</code>) or iterator of response chunks (if <code>stream=True</code>)</p> Asynchronous completion without token streaming in PythonResponse in JSON <pre><code>import asyncio\nfrom llmengine import Completion\n\nasync def main():\n    response = await Completion.acreate(\n        model=\"llama-2-7b\",\n        prompt=\"Hello, my name is\",\n        max_new_tokens=10,\n        temperature=0.2,\n    )\n    print(response.json())\n\nasyncio.run(main())\n</code></pre> <pre><code>{\n    \"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\",\n    \"output\": {\n        \"text\": \"_______ and I am a _______\",\n        \"num_completion_tokens\": 10\n}\n}\n</code></pre> <p>Token streaming can be used to reduce perceived latency for applications. Here is how applications can use streaming:</p> Asynchronous completion with token streaming in PythonResponse in JSON <pre><code>import asyncio\nfrom llmengine import Completion\n\nasync def main():\n    stream = await Completion.acreate(\n        model=\"llama-2-7b\",\n        prompt=\"why is the sky blue?\",\n        max_new_tokens=5,\n        temperature=0.2,\n        stream=True,\n    )\n\nasync for response in stream:\n        if response.output:\n            print(response.json())\n\nasyncio.run(main())\n</code></pre> <pre><code>{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \"\\n\", \"finished\": false, \"num_completion_tokens\": 1}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \"I\", \"finished\": false, \"num_completion_tokens\": 2}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \" think\", \"finished\": false, \"num_completion_tokens\": 3}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \" the\", \"finished\": false, \"num_completion_tokens\": 4}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \" sky\", \"finished\": true, \"num_completion_tokens\": 5}}\n</code></pre>"},{"location":"api/python_client/#llmengine.Completion.batch_create","title":"batch_create  <code>classmethod</code>","text":"<pre><code>batch_create(\n    output_data_path: str,\n    model_config: CreateBatchCompletionsModelConfig,\n    content: Optional[\n        CreateBatchCompletionsRequestContent\n    ] = None,\n    input_data_path: Optional[str] = None,\n    data_parallelism: int = 1,\n    max_runtime_sec: int = 24 * 3600,\n    tool_config: Optional[ToolConfig] = None,\n) -&gt; CreateBatchCompletionsResponse\n</code></pre> <p>Creates a batch completion for the provided input data. The job runs offline and does not depend on an existing model endpoint.</p> <p>Prompts can be passed in from an input file, or as a part of the request.</p> <p>Parameters:</p> Name Type Description Default <code>output_data_path</code> <code>str</code> <p>The path to the output file. The output file will be a JSON file containing the completions.</p> required <code>model_config</code> <code>CreateBatchCompletionsModelConfig</code> <p>The model configuration to use for the batch completion.</p> required <code>content</code> <code>Optional[CreateBatchCompletionsRequestContent]</code> <p>The content to use for the batch completion. Either one of <code>content</code> or <code>input_data_path</code> must be provided.</p> <code>None</code> <code>input_data_path</code> <code>Optional[str]</code> <p>The path to the input file. The input file should be a JSON file with data of type <code>BatchCompletionsRequestContent</code>. Either one of <code>content</code> or <code>input_data_path</code> must be provided.</p> <code>None</code> <code>data_parallelism</code> <code>int</code> <p>The number of parallel jobs to run. Data will be evenly distributed to the jobs. Defaults to 1.</p> <code>1</code> <code>max_runtime_sec</code> <code>int</code> <p>The maximum runtime of the batch completion in seconds. Defaults to 24 hours.</p> <code>24 * 3600</code> <code>tool_config</code> <code>Optional[ToolConfig]</code> <p>Configuration for tool use. NOTE: this config is highly experimental and signature will change significantly in future iterations. Currently only Python code evaluator is supported. Python code context starts with \"```python\\n\" and ends with \"\\n&gt;&gt;&gt;\\n\", data before \"\\n```\\n\" and content end will be replaced by the Python execution results. Please format prompts accordingly and provide examples so LLMs could properly generate Python code.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>CreateBatchCompletionsResponse</code> <p>The response containing the job id.</p> Batch completions with prompts in the requestBatch completions with prompts in a file and with 2 parallel jobsBatch completions with prompts and use tool <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\nresponse = Completion.batch_create(\n    output_data_path=\"s3://my-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    content=CreateBatchCompletionsRequestContent(\n        prompts=[\"What is deep learning\", \"What is a neural network\"],\n        max_new_tokens=10,\n        temperature=0.0\n    )\n)\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2\n)\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent, ToolConfig\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2,\n    tool_config=ToolConfig(\n        name=\"code_evaluator\",\n    )\n)\nprint(response.json())\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune","title":"FineTune","text":"<p>             Bases: <code>APIEngine</code></p> <p>FineTune API. This API is used to fine-tune models.</p> <p>Fine-tuning is a process where the LLM is further trained on a task-specific dataset, allowing the model to adjust its parameters to better align with the task at hand. Fine-tuning is a supervised training phase, where prompt/response pairs are provided to optimize the performance of the LLM. LLM Engine currently uses LoRA for fine-tuning. Support for additional fine-tuning methods is upcoming.</p> <p>LLM Engine provides APIs to create fine-tunes on a base model with training &amp; validation datasets. APIs are also provided to list, cancel and retrieve fine-tuning jobs.</p> <p>Creating a fine-tune will end with the creation of a Model, which you can view using <code>Model.get(model_name)</code> or delete using <code>Model.delete(model_name)</code>.</p>"},{"location":"api/python_client/#llmengine.FineTune.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    model: str,\n    training_file: str,\n    validation_file: Optional[str] = None,\n    hyperparameters: Optional[\n        Dict[str, Union[str, int, float]]\n    ] = None,\n    wandb_config: Optional[Dict[str, Any]] = None,\n    suffix: Optional[str] = None,\n) -&gt; CreateFineTuneResponse\n</code></pre> <p>Creates a job that fine-tunes a specified model with a given dataset.</p> <p>This API can be used to fine-tune a model. The model is the name of base model (Model Zoo for available models) to fine-tune. The training and validation files should consist of prompt and response pairs. <code>training_file</code> and <code>validation_file</code> must be either publicly accessible HTTP or HTTPS URLs, or file IDs of files uploaded to LLM Engine's Files API (these will have the <code>file-</code> prefix). The referenced files must be CSV files that include two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is currently supported. At least 200 rows of data is recommended to start to see benefits from fine-tuning. For sequences longer than the native <code>max_seq_length</code> of the model, the sequences will be truncated.</p> <p>A fine-tuning job can take roughly 30 minutes for a small dataset (~200 rows) and several hours for larger ones.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`</code> <p>The name of the base model to fine-tune. See Model Zoo for the list of available models to fine-tune.</p> required <code>training_file</code> <code>`str`</code> <p>Publicly accessible URL or file ID referencing a CSV file for training. When no validation_file is provided, one will automatically be created using a 10% split of the training_file data.</p> required <code>validation_file</code> <code>`Optional[str]`</code> <p>Publicly accessible URL or file ID referencing a CSV file for validation. The validation file is used to compute metrics which let LLM Engine pick the best fine-tuned checkpoint, which will be used for inference when fine-tuning is complete.</p> <code>None</code> <code>hyperparameters</code> <code>`Optional[Dict[str, Union[str, int, float, Dict[str, Any]]]]`</code> <p>A dict of hyperparameters to customize fine-tuning behavior.</p> <p>Currently supported hyperparameters:</p> <ul> <li><code>lr</code>: Peak learning rate used during fine-tuning. It decays with a cosine schedule afterward. (Default: 2e-3)</li> <li><code>warmup_ratio</code>: Ratio of training steps used for learning rate warmup. (Default: 0.03)</li> <li><code>epochs</code>: Number of fine-tuning epochs. This should be less than 20. (Default: 5)</li> <li><code>weight_decay</code>: Regularization penalty applied to learned weights. (Default: 0.001)</li> <li><code>peft_config</code>: A dict of parameters for the PEFT algorithm. See LoraConfig for more information.</li> </ul> <code>None</code> <code>wandb_config</code> <code>`Optional[Dict[str, Any]]`</code> <p>A dict of configuration parameters for Weights &amp; Biases. See Weights &amp; Biases for more information. Set <code>hyperparameter[\"report_to\"]</code> to <code>wandb</code> to enable automatic finetune metrics logging. Must include <code>api_key</code> field which is the wandb API key. Also supports setting <code>base_url</code> to use a custom Weights &amp; Biases server.</p> <code>None</code> <code>suffix</code> <code>`Optional[str]`</code> <p>A string that will be added to your fine-tuned model name. If present, the entire fine-tuned model name will be formatted like <code>\"[model].[suffix].[YYMMDD-HHMMSS]\"</code>. If absent, the fine-tuned model name will be formatted <code>\"[model].[YYMMDD-HHMMSS]\"</code>. For example, if <code>suffix</code> is <code>\"my-experiment\"</code>, the fine-tuned model name could be <code>\"llama-2-7b.my-experiment.230717-230150\"</code>. Note: <code>suffix</code> must be between 1 and 28 characters long, and can only contain alphanumeric characters and hyphens.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CreateFineTuneResponse</code> <code>CreateFineTuneResponse</code> <p>an object that contains the ID of the created fine-tuning job</p> <p>Here is an example script to create a 5-row CSV of properly formatted data for fine-tuning an airline question answering bot:</p> Formatting data in Python <pre><code>import csv\n# Define data\ndata = [\n  (\"What is your policy on carry-on luggage?\", \"Our policy allows each passenger to bring one piece of carry-on luggage and one personal item such as a purse or briefcase. The maximum size for carry-on luggage is 22 x 14 x 9 inches.\"),\n  (\"How can I change my flight?\", \"You can change your flight through our website or mobile app. Go to 'Manage my booking' section, enter your booking reference and last name, then follow the prompts to change your flight.\"),\n  (\"What meals are available on my flight?\", \"We offer a variety of meals depending on the flight's duration and route. These can range from snacks and light refreshments to full-course meals on long-haul flights. Specific meal options can be viewed during the booking process.\"),\n  (\"How early should I arrive at the airport before my flight?\", \"We recommend arriving at least two hours before domestic flights and three hours before international flights.\"),\n  \"Can I select my seat in advance?\", \"Yes, you can select your seat during the booking process or afterwards via the 'Manage my booking' section on our website or mobile app.\"),\n  ]\n\n# Write data to a CSV file\nwith open('customer_service_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"prompt\", \"response\"])\n    writer.writerows(data)\n</code></pre> <p>Currently, data needs to be uploaded to either a publicly accessible web URL or to LLM Engine's private file server so that it can be read for fine-tuning. Publicly accessible HTTP and HTTPS URLs are currently supported.</p> <p>To privately share data with the LLM Engine API, use LLM Engine's File.upload API. You can upload data in local file to LLM Engine's private file server and then use the returned file ID to reference your data in the FineTune API. The file ID is generally in the form of <code>file-&lt;random_string&gt;</code>, e.g. \"file-7DLVeLdN2Ty4M2m\".</p> <p>Example code for fine-tuning:</p> Fine-tuning in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.create(\n    model=\"llama-2-7b\",\n    training_file=\"file-7DLVeLdN2Ty4M2m\",\n)\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"fine_tune_id\": \"ft-cir3eevt71r003ks6il0\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(fine_tune_id: str) -&gt; GetFineTuneResponse\n</code></pre> <p>Get status of a fine-tuning job.</p> <p>This API can be used to get the status of an already running fine-tuning job. It takes as a single parameter the <code>fine_tune_id</code> and returns a GetFineTuneResponse object with the id and status (<code>PENDING</code>, <code>STARTED</code>, <code>UNDEFINED</code>, <code>FAILURE</code> or <code>SUCCESS</code>).</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>GetFineTuneResponse</code> <code>GetFineTuneResponse</code> <p>an object that contains the ID and status of the requested job</p> Getting status of fine-tuning in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.get(\n    fine_tune_id=\"ft-cir3eevt71r003ks6il0\",\n)\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"fine_tune_id\": \"ft-cir3eevt71r003ks6il0\",\n    \"status\": \"STARTED\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.get_events","title":"get_events  <code>classmethod</code>","text":"<pre><code>get_events(fine_tune_id: str) -&gt; GetFineTuneEventsResponse\n</code></pre> <p>Get events of a fine-tuning job.</p> <p>This API can be used to get the list of detailed events for a fine-tuning job. It takes the <code>fine_tune_id</code> as a parameter and returns a response object which has a list of events that has happened for the fine-tuning job. Two events are logged periodically: an evaluation of the training loss, and an evaluation of the eval loss. This API will return all events for the fine-tuning job.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>GetFineTuneEventsResponse</code> <code>GetFineTuneEventsResponse</code> <p>an object that contains the list of events for the fine-tuning job</p> Getting events for  fine-tuning jobs in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.get_events(fine_tune_id=\"ft-cir3eevt71r003ks6il0\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"events\":\n    [\n        {\n            \"timestamp\": 1689665099.6704428,\n            \"message\": \"{'loss': 2.108, 'learning_rate': 0.002, 'epoch': 0.7}\",\n            \"level\": \"info\"\n},\n        {\n            \"timestamp\": 1689665100.1966307,\n            \"message\": \"{'eval_loss': 1.67730712890625, 'eval_runtime': 0.2023, 'eval_samples_per_second': 24.717, 'eval_steps_per_second': 4.943, 'epoch': 0.7}\",\n            \"level\": \"info\"\n},\n        {\n            \"timestamp\": 1689665105.6544185,\n            \"message\": \"{'loss': 1.8961, 'learning_rate': 0.0017071067811865474, 'epoch': 1.39}\",\n            \"level\": \"info\"\n},\n        {\n            \"timestamp\": 1689665106.159139,\n            \"message\": \"{'eval_loss': 1.513688564300537, 'eval_runtime': 0.2025, 'eval_samples_per_second': 24.696, 'eval_steps_per_second': 4.939, 'epoch': 1.39}\",\n            \"level\": \"info\"\n}\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListFineTunesResponse\n</code></pre> <p>List fine-tuning jobs.</p> <p>This API can be used to list all the fine-tuning jobs. It returns a list of pairs of <code>fine_tune_id</code> and <code>status</code> for all existing jobs.</p> <p>Returns:</p> Name Type Description <code>ListFineTunesResponse</code> <code>ListFineTunesResponse</code> <p>an object that contains a list of all fine-tuning jobs and their statuses</p> Listing fine-tuning jobs in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.list()\nprint(response.json())\n</code></pre> <pre><code>{\n    \"jobs\": [\n        {\n            \"fine_tune_id\": \"ft-cir3eevt71r003ks6il0\",\n            \"status\": \"STARTED\"\n},\n        {\n            \"fine_tune_id\": \"ft_def456\",\n            \"status\": \"SUCCESS\"\n}\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.cancel","title":"cancel  <code>classmethod</code>","text":"<pre><code>cancel(fine_tune_id: str) -&gt; CancelFineTuneResponse\n</code></pre> <p>Cancel a fine-tuning job.</p> <p>This API can be used to cancel an existing fine-tuning job if it's no longer required. It takes the <code>fine_tune_id</code> as a parameter and returns a response object which has a <code>success</code> field confirming if the cancellation was successful.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>CancelFineTuneResponse</code> <code>CancelFineTuneResponse</code> <p>an object that contains whether the cancellation was successful</p> Cancelling fine-tuning job in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.cancel(fine_tune_id=\"ft-cir3eevt71r003ks6il0\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"success\": true\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model","title":"Model","text":"<p>             Bases: <code>APIEngine</code></p> <p>Model API. This API is used to get, list, and delete models. Models include both base models built into LLM Engine, and fine-tuned models that you create through the FineTune.create() API.</p> <p>See Model Zoo for the list of publicly available base models.</p>"},{"location":"api/python_client/#llmengine.Model.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    name: str,\n    model: str,\n    inference_framework_image_tag: str,\n    source: LLMSource = LLMSource.HUGGING_FACE,\n    inference_framework: LLMInferenceFramework = LLMInferenceFramework.VLLM,\n    num_shards: int = 1,\n    quantize: Optional[Quantization] = None,\n    checkpoint_path: Optional[str] = None,\n    cpus: Optional[int] = None,\n    memory: Optional[str] = None,\n    storage: Optional[str] = None,\n    gpus: Optional[int] = None,\n    min_workers: int = 0,\n    max_workers: int = 1,\n    per_worker: int = 2,\n    endpoint_type: ModelEndpointType = ModelEndpointType.STREAMING,\n    gpu_type: Optional[str] = None,\n    high_priority: Optional[bool] = False,\n    post_inference_hooks: Optional[\n        List[PostInferenceHooks]\n    ] = None,\n    default_callback_url: Optional[str] = None,\n    public_inference: Optional[bool] = True,\n    labels: Optional[Dict[str, str]] = None,\n) -&gt; CreateLLMEndpointResponse\n</code></pre> <p>Create an LLM model. Note: This API is only available for self-hosted users.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str`</code> <p>Name of the endpoint</p> required <code>model</code> <code>`str`</code> <p>Name of the base model</p> required <code>inference_framework_image_tag</code> <code>`str`</code> <p>Image tag for the inference framework. Use \"latest\" for the most recent image</p> required <code>source</code> <code>`LLMSource`</code> <p>Source of the LLM. Currently only HuggingFace is supported</p> <code>HUGGING_FACE</code> <code>inference_framework</code> <code>`LLMInferenceFramework`</code> <p>Inference framework for the LLM. Current supported frameworks are LLMInferenceFramework.DEEPSPEED, LLMInferenceFramework.TEXT_GENERATION_INFERENCE, LLMInferenceFramework.VLLM and LLMInferenceFramework.LIGHTLLM</p> <code>VLLM</code> <code>num_shards</code> <code>`int`</code> <p>Number of shards for the LLM. When bigger than 1, LLM will be sharded to multiple GPUs. Number of GPUs must be equal or larger than num_shards.</p> <code>1</code> <code>quantize</code> <code>`Optional[Quantization]`</code> <p>Quantization method for the LLM. <code>text_generation_inference</code> supports <code>bitsandbytes</code> and <code>vllm</code> supports <code>awq</code>.</p> <code>None</code> <code>checkpoint_path</code> <code>`Optional[str]`</code> <p>Remote path to the checkpoint for the LLM. LLM engine must have permission to access the given path. Can be either a folder or a tar file. Folder is preferred since we don't need to untar and model loads faster. For model weights, safetensors are preferred but PyTorch checkpoints are also accepted (model loading will be longer).</p> <code>None</code> <code>cpus</code> <code>`Optional[int]`</code> <p>Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1. Recommendation is set it to 8 * GPU count. Can be inferred from the model size.</p> <code>None</code> <code>memory</code> <code>`Optional[str]`</code> <p>Amount of memory each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory. Recommendation is set it to 24Gi * GPU count. Can be inferred from the model size.</p> <code>None</code> <code>storage</code> <code>`Optional[str]`</code> <p>Amount of local ephemeral storage each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage. Recommendataion is 40Gi for 7B models, 80Gi for 13B models and 200Gi for 70B models. Can be inferred from the model size.</p> <code>None</code> <code>gpus</code> <code>`Optional[int]`</code> <p>Number of gpus each worker should get, e.g. 0, 1, etc. Can be inferred from the model size.</p> <code>None</code> <code>min_workers</code> <code>`int`</code> <p>The minimum number of workers. Must be greater than or equal to 0. This should be determined by computing the minimum throughput of your workload and dividing it by the throughput of a single worker. When this number is 0, max_workers must be 1, and the endpoint will autoscale between 0 and 1 pods. When this number is greater than 0, max_workers can be any number greater or equal to min_workers.</p> <code>0</code> <code>max_workers</code> <code>`int`</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>. This should be determined by computing the maximum throughput of your workload and dividing it by the throughput of a single worker</p> <code>1</code> <code>per_worker</code> <code>`int`</code> <p>The maximum number of concurrent requests that an individual worker can service. LLM engine automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests, subject to the limits defined by <code>min_workers</code> and <code>max_workers</code> - If the average number of concurrent requests per worker is lower than <code>per_worker</code>, then the number of workers will be reduced. - Otherwise, if the average number of concurrent requests per worker is higher than <code>per_worker</code>, then the number of workers will be increased to meet the elevated traffic. Here is our recommendation for computing <code>per_worker</code>: 1. Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum throughput requirements. 2. Determine a value for the maximum number of concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing this ensures that the number of workers will \"climb\" to <code>max_workers</code>.</p> <code>2</code> <code>endpoint_type</code> <code>`ModelEndpointType`</code> <p>Currently only <code>\"streaming\"</code> endpoints are supported.</p> <code>STREAMING</code> <code>gpu_type</code> <code>`Optional[str]`</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Can be inferred from the model size. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> <li><code>nvidia-ampere-a100</code></li> <li><code>nvidia-ampere-a100e</code></li> <li><code>nvidia-hopper-h100</code></li> <li><code>nvidia-hopper-h100-1g20gb</code> # 1 slice of MIG with 1g compute and 20GB memory</li> <li><code>nvidia-hopper-h100-3g40gb</code> # 1 slice of MIG with 3g compute and 40GB memory</li> </ul> <code>None</code> <code>high_priority</code> <code>`Optional[bool]`</code> <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created endpoint to leverage the shared pool of prewarmed nodes for faster spinup time</p> <code>False</code> <code>post_inference_hooks</code> <code>`Optional[List[PostInferenceHooks]]`</code> <p>List of hooks to trigger after inference tasks are served</p> <code>None</code> <code>default_callback_url</code> <code>`Optional[str]`</code> <p>The default callback url to use for sync completion requests. This can be overridden in the task parameters for each individual task. post_inference_hooks must contain \"callback\" for the callback to be triggered</p> <code>None</code> <code>public_inference</code> <code>`Optional[bool]`</code> <p>If <code>True</code>, this endpoint will be available to all user IDs for inference</p> <code>True</code> <code>labels</code> <code>`Optional[Dict[str, str]]`</code> <p>An optional dictionary of key/value pairs to associate with this endpoint</p> <code>None</code> <p>Returns:     CreateLLMEndpointResponse: creation task ID of the created Model. Currently not used.</p> Create Llama 2 70B model with hardware specs inferred in PythonCreate Llama 2 7B model with hardware specs specified in PythonCreate Llama 2 13B model in PythonCreate Llama 2 70B model with 8bit quantization in Python <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-70b-test\"\n    model=\"llama-2-70b\",\n    inference_framework_image_tag=\"0.9.4\",\n    inference_framework=LLMInferenceFramework.TEXT_GENERATION_INFERENCE,\n    num_shards=4,\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-7b-test\"\n    model=\"llama-2-7b\",\n    inference_framework_image_tag=\"0.2.1.post1\",\n    inference_framework=LLMInferenceFramework.VLLM,\n    num_shards=1,\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    cpus=8,\n    memory=\"24Gi\",\n    storage=\"40Gi\",\n    gpus=1,\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    gpu_type=\"nvidia-ampere-a10\",\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-13b-test\"\n    model=\"llama-2-13b\",\n    inference_framework_image_tag=\"0.2.1.post1\",\n    inference_framework=LLMInferenceFramework.VLLM,\n    num_shards=2,\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    cpus=16,\n    memory=\"48Gi\",\n    storage=\"80Gi\",\n    gpus=2,\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    gpu_type=\"nvidia-ampere-a10\",\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-70b-test\"\n    model=\"llama-2-70b\",\n    inference_framework_image_tag=\"0.9.4\",\n    inference_framework=LLMInferenceFramework.TEXT_GENERATION_INFERENCE,\n    num_shards=4,\n    quantize=\"bitsandbytes\",\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    cpus=40,\n    memory=\"96Gi\",\n    storage=\"200Gi\",\n    gpus=4,\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    gpu_type=\"nvidia-ampere-a10\",\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(model: str) -&gt; GetLLMEndpointResponse\n</code></pre> <p>Get information about an LLM model.</p> <p>This API can be used to get information about a Model's source and inference framework. For self-hosted users, it returns additional information about number of shards, quantization, infra settings, etc. The function takes as a single parameter the name <code>model</code> and returns a GetLLMEndpointResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`</code> <p>Name of the model</p> required <p>Returns:</p> Name Type Description <code>GetLLMEndpointResponse</code> <code>GetLLMEndpointResponse</code> <p>object representing the LLM and configurations</p> Accessing model in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.get(\"llama-2-7b.suffix.2023-07-18-12-00-00\")\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": null,\n    \"name\": \"llama-2-7b.suffix.2023-07-18-12-00-00\",\n    \"model_name\": null,\n    \"source\": \"hugging_face\",\n    \"status\": \"READY\",\n    \"inference_framework\": \"text_generation_inference\",\n    \"inference_framework_tag\": null,\n    \"num_shards\": null,\n    \"quantize\": null,\n    \"spec\": null\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListLLMEndpointsResponse\n</code></pre> <p>List LLM models available to call inference on.</p> <p>This API can be used to list all available models, including both publicly available models and user-created fine-tuned models. It returns a list of GetLLMEndpointResponse objects for all models. The most important field is the model <code>name</code>.</p> <p>Returns:</p> Name Type Description <code>ListLLMEndpointsResponse</code> <code>ListLLMEndpointsResponse</code> <p>list of models</p> Listing available modes in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.list()\nprint(response.json())\n</code></pre> <pre><code>{\n    \"model_endpoints\": [\n        {\n            \"id\": null,\n            \"name\": \"llama-2-7b.suffix.2023-07-18-12-00-00\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"text_generation_inference\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n},\n        {\n            \"id\": null,\n            \"name\": \"llama-2-7b\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"text_generation_inference\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n},\n        {\n            \"id\": null,\n            \"name\": \"llama-13b-deepspeed-sync\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"deepspeed\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n},\n        {\n            \"id\": null,\n            \"name\": \"falcon-40b\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"text_generation_inference\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n}\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.update","title":"update  <code>classmethod</code>","text":"<pre><code>update(\n    name: str,\n    model: Optional[str] = None,\n    inference_framework_image_tag: Optional[str] = None,\n    source: Optional[LLMSource] = None,\n    num_shards: Optional[int] = None,\n    quantize: Optional[Quantization] = None,\n    checkpoint_path: Optional[str] = None,\n    cpus: Optional[int] = None,\n    memory: Optional[str] = None,\n    storage: Optional[str] = None,\n    gpus: Optional[int] = None,\n    min_workers: Optional[int] = None,\n    max_workers: Optional[int] = None,\n    per_worker: Optional[int] = None,\n    endpoint_type: Optional[ModelEndpointType] = None,\n    gpu_type: Optional[str] = None,\n    high_priority: Optional[bool] = None,\n    post_inference_hooks: Optional[\n        List[PostInferenceHooks]\n    ] = None,\n    default_callback_url: Optional[str] = None,\n    public_inference: Optional[bool] = None,\n    labels: Optional[Dict[str, str]] = None,\n) -&gt; UpdateLLMEndpointResponse\n</code></pre> <p>Update an LLM model. Note: This API is only available for self-hosted users.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str`</code> <p>Name of the endpoint</p> required <code>model</code> <code>`Optional[str]`</code> <p>Name of the base model</p> <code>None</code> <code>inference_framework_image_tag</code> <code>`Optional[str]`</code> <p>Image tag for the inference framework. Use \"latest\" for the most recent image</p> <code>None</code> <code>source</code> <code>`Optional[LLMSource]`</code> <p>Source of the LLM. Currently only HuggingFace is supported</p> <code>None</code> <code>num_shards</code> <code>`Optional[int]`</code> <p>Number of shards for the LLM. When bigger than 1, LLM will be sharded to multiple GPUs. Number of GPUs must be equal or larger than num_shards.</p> <code>None</code> <code>quantize</code> <code>`Optional[Quantization]`</code> <p>Quantization method for the LLM. <code>text_generation_inference</code> supports <code>bitsandbytes</code> and <code>vllm</code> supports <code>awq</code>.</p> <code>None</code> <code>checkpoint_path</code> <code>`Optional[str]`</code> <p>Remote path to the checkpoint for the LLM. LLM engine must have permission to access the given path. Can be either a folder or a tar file. Folder is preferred since we don't need to untar and model loads faster. For model weights, safetensors are preferred but PyTorch checkpoints are also accepted (model loading will be longer).</p> <code>None</code> <code>cpus</code> <code>`Optional[int]`</code> <p>Number of cpus each worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1. Recommendation is set it to 8 * GPU count.</p> <code>None</code> <code>memory</code> <code>`Optional[str]`</code> <p>Amount of memory each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory. Recommendation is set it to 24Gi * GPU count.</p> <code>None</code> <code>storage</code> <code>`Optional[str]`</code> <p>Amount of local ephemeral storage each worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage. Recommendataion is 40Gi for 7B models, 80Gi for 13B models and 200Gi for 70B models.</p> <code>None</code> <code>gpus</code> <code>`Optional[int]`</code> <p>Number of gpus each worker should get, e.g. 0, 1, etc.</p> <code>None</code> <code>min_workers</code> <code>`Optional[int]`</code> <p>The minimum number of workers. Must be greater than or equal to 0. This should be determined by computing the minimum throughput of your workload and dividing it by the throughput of a single worker. When this number is 0, max_workers must be 1, and the endpoint will autoscale between 0 and 1 pods. When this number is greater than 0, max_workers can be any number greater or equal to min_workers.</p> <code>None</code> <code>max_workers</code> <code>`Optional[int]`</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>. This should be determined by computing the maximum throughput of your workload and dividing it by the throughput of a single worker</p> <code>None</code> <code>per_worker</code> <code>`Optional[int]`</code> <p>The maximum number of concurrent requests that an individual worker can service. LLM engine automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests, subject to the limits defined by <code>min_workers</code> and <code>max_workers</code> - If the average number of concurrent requests per worker is lower than <code>per_worker</code>, then the number of workers will be reduced. - Otherwise, if the average number of concurrent requests per worker is higher than <code>per_worker</code>, then the number of workers will be increased to meet the elevated traffic. Here is our recommendation for computing <code>per_worker</code>: 1. Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum throughput requirements. 2. Determine a value for the maximum number of concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing this ensures that the number of workers will \"climb\" to <code>max_workers</code>.</p> <code>None</code> <code>endpoint_type</code> <code>`Optional[ModelEndpointType]`</code> <p>Currently only <code>\"streaming\"</code> endpoints are supported.</p> <code>None</code> <code>gpu_type</code> <code>`Optional[str]`</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> <li><code>nvidia-ampere-a100</code></li> <li><code>nvidia-ampere-a100e</code></li> <li><code>nvidia-hopper-h100</code></li> <li><code>nvidia-hopper-h100-1g20gb</code></li> <li><code>nvidia-hopper-h100-3g40gb</code></li> </ul> <code>None</code> <code>high_priority</code> <code>`Optional[bool]`</code> <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created endpoint to leverage the shared pool of prewarmed nodes for faster spinup time</p> <code>None</code> <code>post_inference_hooks</code> <code>`Optional[List[PostInferenceHooks]]`</code> <p>List of hooks to trigger after inference tasks are served</p> <code>None</code> <code>default_callback_url</code> <code>`Optional[str]`</code> <p>The default callback url to use for sync completion requests. This can be overridden in the task parameters for each individual task. post_inference_hooks must contain \"callback\" for the callback to be triggered</p> <code>None</code> <code>public_inference</code> <code>`Optional[bool]`</code> <p>If <code>True</code>, this endpoint will be available to all user IDs for inference</p> <code>None</code> <code>labels</code> <code>`Optional[Dict[str, str]]`</code> <p>An optional dictionary of key/value pairs to associate with this endpoint</p> <code>None</code> <p>Returns:     UpdateLLMEndpointResponse: creation task ID of the updated Model. Currently not used.</p>"},{"location":"api/python_client/#llmengine.Model.delete","title":"delete  <code>classmethod</code>","text":"<pre><code>delete(\n    model_endpoint_name: str,\n) -&gt; DeleteLLMEndpointResponse\n</code></pre> <p>Deletes an LLM model.</p> <p>This API can be used to delete a fine-tuned model. It takes as parameter the name of the <code>model</code> and returns a response object which has a <code>deleted</code> field confirming if the deletion was successful. If called on a base model included with LLM Engine, an error will be thrown.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint_name</code> <code>`str`</code> <p>Name of the model endpoint to be deleted</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>DeleteLLMEndpointResponse</code> <p>whether the model endpoint was successfully deleted</p> Deleting model in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.delete(\"llama-2-7b.suffix.2023-07-18-12-00-00\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"deleted\": true\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.download","title":"download  <code>classmethod</code>","text":"<pre><code>download(\n    model_name: str, download_format: str = \"hugging_face\"\n) -&gt; ModelDownloadResponse\n</code></pre> <p>Download a fine-tuned model.</p> <p>This API can be used to download the resulting model from a fine-tuning job. It takes the <code>model_name</code> and <code>download_format</code> as parameter and returns a response object which contains a dictonary of filename, url pairs associated with the fine-tuned model. The user can then download these urls to obtain the fine-tuned model. If called on a nonexistent model, an error will be thrown.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>`str`</code> <p>name of the fine-tuned model</p> required <code>download_format</code> <code>`str`</code> <p>download format requested (default=hugging_face)</p> <code>'hugging_face'</code> <p>Returns:     DownloadModelResponse: an object that contains a dictionary of filenames, urls from which to download the model weights.     The urls are presigned urls that grant temporary access and expire after an hour.</p> Downloading model in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.download(\"llama-2-7b.suffix.2023-07-18-12-00-00\", download_format=\"hugging_face\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"urls\": {\"my_model_file\": \"https://url-to-my-model-weights\"}\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File","title":"File","text":"<p>             Bases: <code>APIEngine</code></p> <p>File API. This API is used to upload private files to LLM engine so that fine-tunes can access them for training and validation data.</p> <p>Functions are provided to upload, get, list, and delete files, as well as to get the contents of a file.</p>"},{"location":"api/python_client/#llmengine.File.upload","title":"upload  <code>classmethod</code>","text":"<pre><code>upload(file: BufferedReader) -&gt; UploadFileResponse\n</code></pre> <p>Uploads a file to LLM engine.</p> <p>For use in FineTune creation, this should be a CSV file with two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is currently supported.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>`BufferedReader`</code> <p>A local file opened with <code>open(file_path, \"r\")</code></p> required <p>Returns:</p> Name Type Description <code>UploadFileResponse</code> <code>UploadFileResponse</code> <p>an object that contains the ID of the uploaded file</p> Uploading file in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.upload(open(\"training_dataset.csv\", \"r\"))\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": \"file-abc123\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(file_id: str) -&gt; GetFileResponse\n</code></pre> <p>Get file metadata, including filename and size.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>`str`</code> <p>ID of the file</p> required <p>Returns:</p> Name Type Description <code>GetFileResponse</code> <code>GetFileResponse</code> <p>an object that contains the ID, filename, and size of the requested file</p> Getting metadata about file in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.get(\n    file_id=\"file-abc123\",\n)\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": \"file-abc123\",\n    \"filename\": \"training_dataset.csv\",\n    \"size\": 100\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.download","title":"download  <code>classmethod</code>","text":"<pre><code>download(file_id: str) -&gt; GetFileContentResponse\n</code></pre> <p>Get contents of a file, as a string. (If the uploaded file is in binary, a string encoding will be returned.)</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>`str`</code> <p>ID of the file</p> required <p>Returns:</p> Name Type Description <code>GetFileContentResponse</code> <code>GetFileContentResponse</code> <p>an object that contains the ID and content of the file</p> Getting file content in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.download(file_id=\"file-abc123\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": \"file-abc123\",\n    \"content\": \"Hello world!\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListFilesResponse\n</code></pre> <p>List metadata about all files, e.g. their filenames and sizes.</p> <p>Returns:</p> Name Type Description <code>ListFilesResponse</code> <code>ListFilesResponse</code> <p>an object that contains a list of all files and their filenames and sizes</p> Listing files in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.list()\nprint(response.json())\n</code></pre> <pre><code>{\n    \"files\": [\n        {\n            \"id\": \"file-abc123\",\n            \"filename\": \"training_dataset.csv\",\n            \"size\": 100\n},\n        {\n            \"id\": \"file-def456\",\n            \"filename\": \"validation_dataset.csv\",\n            \"size\": 50\n}\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.delete","title":"delete  <code>classmethod</code>","text":"<pre><code>delete(file_id: str) -&gt; DeleteFileResponse\n</code></pre> <p>Deletes a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>`str`</code> <p>ID of the file</p> required <p>Returns:</p> Name Type Description <code>DeleteFileResponse</code> <code>DeleteFileResponse</code> <p>an object that contains whether the deletion was successful</p> Deleting file in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.delete(file_id=\"file-abc123\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"deleted\": true\n}\n</code></pre>"},{"location":"guides/completions/","title":"Completions","text":"<p>Language Models are trained to predict natural language and provide text outputs as a response to their inputs. The inputs are called prompts and outputs are referred to as completions. LLMs take the input prompts and chunk them into smaller units called tokens to process and generate language. Tokens may include trailing spaces and even sub-words. This process is language dependent.</p> <p>Scale's LLM Engine provides access to open source language models (see Model Zoo) that can be used for producing completions to prompts.</p>"},{"location":"guides/completions/#completion-api-call","title":"Completion API call","text":"<p>An example API call looks as follows:</p> Completion call in Python <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n)\n\nprint(response.json())\n# '{\"request_id\": \"c4bf0732-08e0-48a8-8b44-dfe8d4702fb0\", \"output\": {\"text\": \"________ and I am a ________\", \"num_completion_tokens\": 10}}'\nprint(response.output.text)\n# ________ and I am a ________\n</code></pre> <ul> <li>model: The LLM you want to use (see Model Zoo).</li> <li>prompt: The main input for the LLM to respond to.</li> <li>max_new_tokens: The maximum number of tokens to generate in the chat completion.</li> <li>temperature: The sampling temperature to use. Higher values make the output more random,   while lower values will make it more focused and deterministic.   When temperature is 0 greedy search is used.</li> </ul> <p>See the full Completion API reference documentation to learn more.</p>"},{"location":"guides/completions/#completion-api-response","title":"Completion API response","text":"<p>An example Completion API response looks as follows:</p> Response in JSONResponse in Python <pre><code>    &gt;&gt;&gt; print(response.json())\n    {\n      \"request_id\": \"c4bf0732-08e0-48a8-8b44-dfe8d4702fb0\",\n      \"output\": {\n        \"text\": \"_______ and I am a _______\",\n        \"num_completion_tokens\": 10\n      }\n    }\n</code></pre> <pre><code>    &gt;&gt;&gt; print(response.output.text)\n    _______ and I am a _______\n</code></pre>"},{"location":"guides/completions/#token-streaming","title":"Token streaming","text":"<p>The Completions API supports token streaming to reduce perceived latency for certain applications. When streaming, tokens will be sent as data-only server-side events.</p> <p>To enable token streaming, pass <code>stream=True</code> to either Completion.create or Completion.acreate.</p>"},{"location":"guides/completions/#streaming-error-handling","title":"Streaming Error Handling","text":"<p>Note: Error handling semantics are mixed for streaming calls: - Errors that arise before streaming begins are returned back to the user as <code>HTTP</code> errors with the appropriate status code. - Errors that arise after streaming begins within a <code>HTTP 200</code> response are returned back to the user as plain-text messages and currently need to be handled by the client. </p> <p>An example of token streaming using the synchronous Completions API looks as follows:</p> Token streaming with synchronous API in python <pre><code>import sys\nfrom llmengine import Completion\n\n# errors occurring before streaming begins will be thrown here\nstream = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Give me a 200 word summary on the current economic events in the US.\",\n    max_new_tokens=1000,\n    temperature=0.2,\n    stream=True,\n)\n\nfor response in stream:\n    if response.output:\n        print(response.output.text, end=\"\")\n        sys.stdout.flush()\n    else: # an error occurred after streaming began\nprint(response.error) # print the error message out \nbreak\n</code></pre>"},{"location":"guides/completions/#async-requests","title":"Async requests","text":"<p>The Python client supports <code>asyncio</code> for creating Completions. Use Completion.acreate instead of Completion.create to utilize async processing. The function signatures are otherwise identical.</p> <p>An example of async Completions looks as follows:</p> Completions with asynchronous API in python <pre><code>import asyncio\nfrom llmengine import Completion\n\nasync def main():\n    response = await Completion.acreate(\n        model=\"llama-2-7b\",\n        prompt=\"Hello, my name is\",\n        max_new_tokens=10,\n        temperature=0.2,\n    )\n    print(response.json())\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/completions/#batch-completions","title":"Batch completions","text":"<p>The Python client also supports batch completions. Batch completions supports distributing data to multiple workers to accelerate inference. It also tries to maximize throughput so the completions should finish quite a bit faster than hitting models through HTTP. Use Completion.batch_create to utilize batch completions.</p> <p>Some examples of batch completions:</p> Batch completions with prompts in the request <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\ncontent = CreateBatchCompletionsRequestContent(\n    prompts=[\"What is deep learning\", \"What is a neural network\"],\n    max_new_tokens=10,\n    temperature=0.0\n)\n\nresponse = Completion.batch_create(\n    output_data_path=\"s3://my-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    content=content\n)\nprint(response.job_id)\n</code></pre> Batch completions with prompts in a file and with 2 parallel jobs <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2\n)\nprint(response.job_id)\n</code></pre> Batch completions with prompts and use tool <p>For how to properly use the tool please see Completion.batch_create tool_config doc. <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent, ToolConfig\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2,\n    tool_config=ToolConfig(\n        name=\"code_evaluator\",\n    )\n)\nprint(response.json())\n</code></pre></p>"},{"location":"guides/completions/#guided-decoding","title":"Guided decoding","text":"<p>Guided decoding is supported by vLLM and backed by Outlines. It enforces certain token generation patterns by tinkering with the sampling logits.</p> Guided decoding with regex <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_regex=\"Sean.*\",\n)\n\nprint(response.json())\n# {\"request_id\":\"c19f0fae-317e-4f69-8e06-c04189299b9c\",\"output\":{\"text\":\"Sean. I'm a 2\",\"num_prompt_tokens\":6,\"num_completion_tokens\":10,\"tokens\":null}}\n</code></pre> Guided decoding with choice <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_choice=[\"Sean\", \"Brian\", \"Tim\"],\n)\n\nprint(response.json())\n# {\"request_id\":\"641e2af3-a3e3-4493-98b9-d38115ba0d22\",\"output\":{\"text\":\"Sean\",\"num_prompt_tokens\":6,\"num_completion_tokens\":4,\"tokens\":null}}\n</code></pre> Guided decoding with JSON schema <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_json={\"properties\":{\"myString\":{\"type\":\"string\"}},\"required\":[\"myString\"]},\n)\n\nprint(response.json())\n# {\"request_id\":\"5b184654-96b6-4932-9eb6-382a51fdb3d5\",\"output\":{\"text\":\"{\\\"myString\\\" : \\\"John Doe\",\"num_prompt_tokens\":6,\"num_completion_tokens\":10,\"tokens\":null}}\n</code></pre> Guided decoding with Context-Free Grammar <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_grammar=\"start: \\\"John\\\"\"\n)\n\nprint(response.json())\n# {\"request_id\": \"34621b44-c655-402c-a459-f108b3e49b12\", \"output\": {\"text\": \"John\", \"num_prompt_tokens\": 6, \"num_completion_tokens\": 4, \"tokens\": None}}\n</code></pre>"},{"location":"guides/completions/#which-model-should-i-use","title":"Which model should I use?","text":"<p>See the Model Zoo for more information on best practices for which model to use for Completions.</p>"},{"location":"guides/endpoint_creation/","title":"Endpoint creation","text":"<p>When creating a model endpoint, you can periodically poll the model status field to track the status of your model endpoint. In general, you'll need to wait after the  model creation step for the model endpoint to be ready and available for use. An example is provided below: </p> <pre><code>model_name = \"test_deploy\"\nmodel = Model.create(name=model_name, model=\"llama-2-7b\", inference_frame_image_tag=\"0.9.4\")\nresponse = Model.get(model_name)\nwhile response.status.name != \"READY\":\n    print(response.status.name)\n    time.sleep(60)\n    response = Model.get(model_name)\n</code></pre> <p>Once the endpoint status is ready, you can use your newly created model for inference.</p>"},{"location":"guides/fine_tuning/","title":"Fine-tuning","text":"<p>Learn how to customize your models on your data with fine-tuning. Or get started right away with our fine-tuning cookbook.</p>"},{"location":"guides/fine_tuning/#introduction","title":"Introduction","text":"<p>Fine-tuning helps improve model performance by training on specific examples of prompts and desired responses. LLMs are initially trained on data collected from the entire internet. With fine-tuning, LLMs can be optimized to perform better in a specific domain by learning from examples for that domain. Smaller LLMs that have been fine-tuned on a specific use case often outperform larger ones that were trained more generally.</p> <p>Fine-tuning allows for:</p> <ol> <li>Higher quality results than prompt engineering alone</li> <li>Cost savings through shorter prompts</li> <li>The ability to reach equivalent accuracy with a smaller model</li> <li>Lower latency at inference time</li> <li>The chance to show an LLM more examples than can fit in a single context window</li> </ol> <p>LLM Engine's fine-tuning API lets you fine-tune various open source LLMs on your own data and then make inference calls to the resulting LLM. For more specific details, see the fine-tuning API reference.</p>"},{"location":"guides/fine_tuning/#producing-high-quality-data-for-fine-tuning","title":"Producing high quality data for fine-tuning","text":"<p>The training data for fine-tuning should consist of prompt and response pairs.</p> <p>As a rule of thumb, you should expect to see linear improvements in your fine-tuned model's quality with each doubling of the dataset size. Having high-quality data is also essential to improving performance. For every linear increase in the error rate in your training data, you may encounter a roughly quadratic increase in your fine-tuned model's error rate.</p> <p>High quality data is critical to achieve improved model performance, and in several cases will require experts to generate and prepare data - the breadth and diversity of the data is highly critical. Scale's Data Engine can help prepare such high quality, diverse data sets - more information here.</p>"},{"location":"guides/fine_tuning/#preparing-data","title":"Preparing data","text":"<p>Your data must be formatted as a CSV file that includes two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is currently supported. At least 200 rows of data is recommended to start to see benefits from fine-tuning. LLM Engine supports fine-tuning with a training and validation dataset. If only a training dataset is provided, 10% of the data is randomly split to be used as validation.</p> <p>Here is an example script to create a 50-row CSV of properly formatted data for fine-tuning an airline question answering bot</p> Creating a sample dataset <pre><code>import csv\n# Define data\ndata = [\n    (\"What is your policy on carry-on luggage?\", \"Our policy allows each passenger to bring one piece of carry-on luggage and one personal item such as a purse or briefcase. The maximum size for carry-on luggage is 22 x 14 x 9 inches.\"),\n    (\"How can I change my flight?\", \"You can change your flight through our website or mobile app. Go to 'Manage my booking' section, enter your booking reference and last name, then follow the prompts to change your flight.\"),\n    (\"What meals are available on my flight?\", \"We offer a variety of meals depending on the flight's duration and route. These can range from snacks and light refreshments to full-course meals on long-haul flights. Specific meal options can be viewed during the booking process.\"),\n    (\"How early should I arrive at the airport before my flight?\", \"We recommend arriving at least two hours before domestic flights and three hours before international flights.\"),\n    (\"Can I select my seat in advance?\", \"Yes, you can select your seat during the booking process or afterwards via the 'Manage my booking' section on our website or mobile app.\"),\n    (\"What should I do if my luggage is lost?\", \"If your luggage is lost, please report this immediately at our 'Lost and Found' counter at the airport. We will assist you in tracking your luggage.\"),\n    (\"Do you offer special assistance for passengers with disabilities?\", \"Yes, we offer special assistance for passengers with disabilities. Please notify us of your needs at least 48 hours prior to your flight.\"),\n    (\"Can I bring my pet on the flight?\", \"Yes, we allow small pets in the cabin, and larger pets in the cargo hold. Please check our pet policy for more details.\"),\n    (\"What is your policy on flight cancellations?\", \"In case of flight cancellations, we aim to notify passengers as early as possible and offer either a refund or a rebooking on the next available flight.\"),\n    (\"Can I get a refund if I cancel my flight?\", \"Refunds depend on the type of ticket purchased. Please check our cancellation policy for details. Non-refundable tickets, however, are typically not eligible for refunds unless due to extraordinary circumstances.\"),\n    (\"How can I check-in for my flight?\", \"You can check-in for your flight either online, through our mobile app, or at the airport. Online and mobile app check-in opens 24 hours before departure and closes 90 minutes before.\"),\n    (\"Do you offer free meals on your flights?\", \"Yes, we serve free meals on all long-haul flights. For short-haul flights, we offer a complimentary drink and snack. Special meal requests should be made at least 48 hours before departure.\"),\n    (\"Can I use my electronic devices during the flight?\", \"Small electronic devices can be used throughout the flight in flight mode. Larger devices like laptops may be used above 10,000 feet.\"),\n    (\"How much baggage can I check-in?\", \"The checked baggage allowance depends on the class of travel and route. The details would be mentioned on your ticket, or you can check on our website.\"),\n    (\"How can I request for a wheelchair?\", \"To request a wheelchair or any other special assistance, please call our customer service at least 48 hours before your flight.\"),\n    (\"Do I get a discount for group bookings?\", \"Yes, we offer discounts on group bookings of 10 or more passengers. Please contact our group bookings team for more information.\"),\n    (\"Do you offer Wi-fi on your flights?\", \"Yes, we offer complimentary Wi-fi on select flights. You can check the availability during the booking process.\"),\n    (\"What is the minimum connecting time between flights?\", \"The minimum connecting time varies depending on the airport and whether your flight is international or domestic. Generally, it's recommended to allow at least 45-60 minutes for domestic connections and 60-120 minutes for international.\"),\n    (\"Do you offer duty-free shopping on international flights?\", \"Yes, we have a selection of duty-free items that you can pre-order on our website or purchase onboard on international flights.\"),\n    (\"Can I upgrade my ticket to business class?\", \"Yes, you can upgrade your ticket through the 'Manage my booking' section on our website or by contacting our customer service. The availability and costs depend on the specific flight.\"),\n    (\"Can unaccompanied minors travel on your flights?\", \"Yes, we do accommodate unaccompanied minors on our flights, with special services to ensure their safety and comfort. Please contact our customer service for more details.\"),\n    (\"What amenities do you provide in business class?\", \"In business class, you will enjoy additional legroom, reclining seats, premium meals, priority boarding and disembarkation, access to our business lounge, extra baggage allowance, and personalized service.\"),\n    (\"How much does extra baggage cost?\", \"Extra baggage costs vary based on flight route and the weight of the baggage. Please refer to our 'Extra Baggage' section on the website for specific rates.\"),\n    (\"Are there any specific rules for carrying liquids in carry-on?\", \"Yes, liquids carried in your hand luggage must be in containers of 100 ml or less and they should all fit into a single, transparent, resealable plastic bag of 20 cm x 20 cm.\"),\n    (\"What if I have a medical condition that requires special assistance during the flight?\", \"We aim to make the flight comfortable for all passengers. If you have a medical condition that may require special assistance, please contact our \u2018special services\u2019 team 48 hours before your flight.\"),\n    (\"What in-flight entertainment options are available?\", \"We offer a range of in-flight entertainment options including a selection of movies, TV shows, music, and games, available on your personal seat-back screen.\"),\n    (\"What types of payment methods do you accept?\", \"We accept credit/debit cards, PayPal, bank transfers, and various other forms of payment. The available options may vary depending on the country of departure.\"),\n    (\"How can I earn and redeem frequent flyer miles?\", \"You can earn miles for every journey you take with us or our partner airlines. These miles can be redeemed for flight tickets, upgrades, or various other benefits. To earn and redeem miles, you need to join our frequent flyer program.\"),\n    (\"Can I bring a stroller for my baby?\", \"Yes, you can bring a stroller for your baby. It can be checked in for free and will normally be given back to you at the aircraft door upon arrival.\"),\n    (\"What age does my child have to be to qualify as an unaccompanied minor?\", \"Children aged between 5 and 12 years who are traveling alone are considered unaccompanied minors. Our team provides special care for these children from departure to arrival.\"),\n    (\"What documents do I need to travel internationally?\", \"For international travel, you need a valid passport and may also require visas, depending on your destination and your country of residence. It's important to check the specific requirements before you travel.\"),\n    (\"What happens if I miss my flight?\", \"If you miss your flight, please contact our customer service immediately. Depending on the circumstances, you may be able to rebook on a later flight, but additional fees may apply.\"),\n    (\"Can I travel with my musical instrument?\", \"Yes, small musical instruments can be brought on board as your one carry-on item. Larger instruments must be transported in the cargo, or if small enough, a seat may be purchased for them.\"),\n    (\"Do you offer discounts for children or infants?\", \"Yes, children aged 2-11 traveling with an adult usually receive a discount on the fare. Infants under the age of 2 who do not occupy a seat can travel for a reduced fare or sometimes for free.\"),\n    (\"Is smoking allowed on your flights?\", \"No, all our flights are non-smoking for the comfort and safety of all passengers.\"),\n    (\"Do you have family seating?\", \"Yes, we offer the option to seat families together. You can select seats during booking or afterwards through the 'Manage my booking' section on the website.\"),\n    (\"Is there any discount for senior citizens?\", \"Some flights may offer a discount for senior citizens. Please check our website or contact customer service for accurate information.\"),\n    (\"What items are prohibited on your flights?\", \"Prohibited items include, but are not limited to, sharp objects, firearms, explosive materials, and certain chemicals. You can find a comprehensive list on our website under the 'Security Regulations' section.\"),\n    (\"Can I purchase a ticket for someone else?\", \"Yes, you can purchase a ticket for someone else. You'll need their correct name as it appears on their government-issued ID, and their correct travel dates.\"),\n    (\"What is the process for lost and found items on the plane?\", \"If you realize you forgot an item on the plane, report it as soon as possible to our lost and found counter. We will make every effort to locate and return your item.\"),\n    (\"Can I request a special meal?\", \"Yes, we offer a variety of special meals to accommodate dietary restrictions. Please request your preferred meal at least 48 hours prior to your flight.\"),\n    (\"Is there a weight limit for checked baggage?\", \"Yes, luggage weight limits depend on your ticket class and route. You can find the details on your ticket or by visiting our website.\"),\n    (\"Can I bring my sports equipment?\", \"Yes, certain types of sports equipment can be carried either as or in addition to your permitted baggage. Some equipment may require additional fees. It's best to check our policy on our website or contact us directly.\"),\n    (\"Do I need a visa to travel to certain countries?\", \"Yes, visa requirements depend on the country you are visiting and your nationality. We advise checking with the relevant embassy or consulate prior to travel.\"),\n    (\"How can I add extra baggage to my booking?\", \"You can add extra baggage to your booking through the 'Manage my booking' section on our website or by contacting our customer services.\"),\n    (\"Can I check-in at the airport?\", \"Yes, you can choose to check-in at the airport. However, we also offer online and mobile check-in, which may save you time.\"),\n    (\"How do I know if my flight is delayed or cancelled?\", \"In case of any changes to your flight, we will attempt to notify all passengers using the contact information given at the time of booking. You can also check your flight status on our website.\"),\n    (\"What is your policy on pregnant passengers?\", \"Pregnant passengers can travel up to the end of the 36th week for single pregnancies, and the end of the 32nd week for multiple pregnancies. We recommend consulting your doctor before any air travel.\"),\n    (\"Can children travel alone?\", \"Yes, children age 5 to 12 can travel alone as unaccompanied minors. We provide special care for these seats. Please contact our customer service for more information.\"),\n    (\"How can I pay for my booking?\", \"You can pay for your booking using a variety of methods including credit and debit cards, PayPal, or bank transfers. The options may vary depending on the country of departure.\"),\n]\n\n# Write data to a CSV file\nwith open('customer_service_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"prompt\", \"response\"])\n    writer.writerows(data)\n</code></pre>"},{"location":"guides/fine_tuning/#making-your-data-accessible-to-llm-engine","title":"Making your data accessible to LLM Engine","text":"<p>Currently, data needs to be uploaded to either a publicly accessible web URL or to LLM Engine's private file server so that it can be read for fine-tuning. Publicly accessible HTTP and HTTPS URLs are currently supported.</p> <p>To privately share data with the LLM Engine API, use LLM Engine's File.upload API. You can upload data in local file to LLM Engine's private file server and then use the returned file ID to reference your data in the FineTune API. The file ID is generally in the form of <code>file-&lt;random_string&gt;</code>, e.g. \"file-7DLVeLdN2Ty4M2m\".</p> Upload to LLM Engine's private file server <pre><code>from llmengine import File\n\nresponse = File.upload(open(\"customer_service_data.csv\", \"r\"))\nprint(response.json())\n</code></pre>"},{"location":"guides/fine_tuning/#launching-the-fine-tune","title":"Launching the fine-tune","text":"<p>Once you have uploaded your data, you can use the LLM Engine's FineTune.Create API to launch a fine-tune. You will need to specify which base model to fine-tune, the locations of the training file and optional validation data file, an optional set of hyperparameters to customize the fine-tuning behavior, and an optional suffix to append to the name of the fine-tune. For sequences longer than the native <code>max_seq_length</code> of the model, the sequences will be truncated.</p> <p>If you specify a suffix, the fine-tune will be named <code>model.suffix.&lt;timestamp&gt;</code>. If you do not, the fine-tune will be named <code>model.&lt;timestamp&gt;</code>. The timestamp will be the time the fine-tune was launched. Note: the suffix must only contain alphanumeric characters and hyphens, and be at most 28 characters long.</p> Hyper-parameters for fine-tune  - `lr`: Peak learning rate used during fine-tuning. It decays with a cosine schedule afterward. (Default: 2e-3) - `warmup_ratio`: Ratio of training steps used for learning rate warmup. (Default: 0.03) - `epochs`: Number of fine-tuning epochs. This should be less than 20. (Default: 5) - `weight_decay`: Regularization penalty applied to learned weights. (Default: 0.001)  Create a fine-tune in python <pre><code>from llmengine import FineTune\n\nresponse = FineTune.create(\n    model=\"llama-2-7b\",\n    training_file=\"file-AbCDeLdN2Ty4M2m\",\n    validation_file=\"file-ezSRpgtKQyItI26\",\n)\n\nprint(response.json())\n</code></pre> <p>See the Model Zoo to see which models have fine-tuning support.</p> <p>See Integrations to see how to track fine-tuning metrics.</p>"},{"location":"guides/fine_tuning/#monitoring-the-fine-tune","title":"Monitoring the fine-tune","text":"<p>Once the fine-tune is launched, you can also get the status of your fine-tune.  You can also list events that your fine-tune produces. <pre><code>from llmengine import FineTune\n\nfine_tune_id = \"ft-cabcdefghi1234567890\"\nfine_tune = FineTune.get(fine_tune_id)\nprint(fine_tune.status)  # BatchJobStatus.RUNNING\nprint(fine_tune.fine_tuned_model)  # \"llama-2-7b.700101-000000\nfine_tune_events = FineTune.get_events(fine_tune_id)\nfor event in fine_tune_events.events:\n    print(event)\n# Prints something like:\n# timestamp=1697590000.0 message=\"{'loss': 12.345, 'learning_rate': 0.0, 'epoch': 0.97}\" level='info'\n# timestamp=1697590000.0 message=\"{'eval_loss': 23.456, 'eval_runtime': 19.876, 'eval_samples_per_second': 4.9, 'eval_steps_per_second': 4.9, 'epoch': 0.97}\" level='info'\n# timestamp=1697590020.0 message=\"{'train_runtime': 421.234, 'train_samples_per_second': 2.042, 'train_steps_per_second': 0.042, 'total_flos': 123.45, 'train_loss': 34.567, 'epoch': 0.97}\" level='info'\n</code></pre></p> <p>The status of your fine-tune will give a high-level overview of the fine-tune's progress. The events of your fine-tune will give more detail, such as the training loss and validation loss at each epoch,  as well as any errors that may have occurred. If you encounter any errors with your fine-tune,  the events are a good place to start debugging. For example, if you see <code>Unable to read training or validation dataset</code>, you may need to make your files accessible to LLM Engine. If you see <code>Invalid value received for lora parameter 'lora_alpha'!</code>, you should check that your hyperparameters are valid.</p>"},{"location":"guides/fine_tuning/#making-inference-calls-to-your-fine-tune","title":"Making inference calls to your fine-tune","text":"<p>Once your fine-tune is finished, you will be able to start making inference requests to the model. You can use the <code>fine_tuned_model</code> returned from your FineTune.get API call to reference your fine-tuned model in the Completions API. Alternatively, you can list available LLMs with <code>Model.list</code> in order to find the name of your fine-tuned model. See the Completion API for more details. You can then use that name to direct your completion requests. You must wait until your fine-tune is complete before you can plug it into the Completions API. You can check the status of your fine-tune with FineTune.get.</p> Inference with a fine-tuned model in python <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b.airlines.2023-07-17-08-30-45\",\n    prompt=\"Do you offer in-flight Wi-fi?\",\n    max_new_tokens=100,\n    temperature=0.2,\n)\nprint(response.json())\n</code></pre>"},{"location":"guides/rate_limits/","title":"Overview","text":""},{"location":"guides/rate_limits/#what-are-rate-limits","title":"What are rate limits?","text":"<p>A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.</p>"},{"location":"guides/rate_limits/#how-do-i-know-if-i-am-rate-limited","title":"How do I know if I am rate limited?","text":"<p>Per standard HTTP practices, your request will receive a response with HTTP status code of <code>429</code>, <code>Too Many Requests</code>.</p>"},{"location":"guides/rate_limits/#what-are-the-rate-limits-for-our-api","title":"What are the rate limits for our API?","text":"<p>The LLM Engine API is currently in a preview mode, and therefore we currently do not have any advertised rate limits. As the API moves towards a production release, we will update this section with specific rate limits. For now, the API will return HTTP 429 on an as-needed basis.</p>"},{"location":"guides/rate_limits/#error-mitigation","title":"Error mitigation","text":""},{"location":"guides/rate_limits/#retrying-with-exponential-backoff","title":"Retrying with exponential backoff","text":"<p>One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits:</p> <ul> <li>Automatic retries means you can recover from rate limit errors without crashes or missing data</li> <li>Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail</li> <li>Adding random jitter to the delay helps retries from all hitting at the same time.</li> </ul> <p>Below are a few example solutions for Python that use exponential backoff.</p>"},{"location":"guides/rate_limits/#example-1-using-the-tenacity-library","title":"Example #1: Using the <code>tenacity</code> library","text":"<p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. To add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.</p> Exponential backoff in python <pre><code>import llmengine\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return llmengine.Completion.create(**kwargs)\n\ncompletion_with_backoff(model=\"llama-2-7b\", prompt=\"Why is the sky blue?\")\n</code></pre>"},{"location":"guides/rate_limits/#example-2-using-the-backoff-library","title":"Example #2: Using the <code>backoff</code> library","text":"<p>Backoff is another python library that provides function decorators which can be used to wrap a function such that it will be retried until some condition is met.</p> Decorators for backoff and retry in python <pre><code>import llmengine\nimport backoff\n@backoff.on_exception(backoff.expo, llmengine.errors.RateLimitExceededError)\ndef completion_with_backoff(**kwargs):\n    return llmengine.Completion.create(**kwargs)\n\ncompletions_with_backoff(model=\"llama-2-7b\", prompt=\"Why is the sky blue?\")\n</code></pre>"},{"location":"guides/self_hosting/","title":"Self Hosting [Experimental]","text":"<p>This guide is currently highly experimental. Instructions are subject to change as we improve support for self-hosting.</p> <p>We provide a Helm chart that deploys LLM Engine to an Elastic Kubernetes Cluster (EKS) in AWS. This Helm chart should be configured to connect to dependencies (such as a PostgreSQL database) that you may already have available in your environment.</p> <p>The only portions of the Helm chart that are production ready are the parts that configure and manage LLM Server itself (not PostgreSQL, IAM, etc.)</p> <p>We first go over required AWS dependencies that are required to exist before we can run <code>helm install</code> in your EKS cluster.</p>"},{"location":"guides/self_hosting/#aws-dependencies","title":"AWS Dependencies","text":"<p>This section describes assumptions about existing AWS resources required run to the LLM Engine Server</p>"},{"location":"guides/self_hosting/#eks","title":"EKS","text":"<p>The LLM Engine server must be deployed in an EKS cluster environment. Currently only versions <code>1.23+</code> are supported. Below are the assumed requirements for the EKS cluster: </p> <p>You will need to provision EKS node groups with GPUs to schedule model pods. These node groups must have the <code>node-lifecycle: normal</code> label on them. Additionally, they must have the <code>k8s.amazonaws.com/accelerator</code> label set appropriately depending on the instance type:</p> Instance family <code>k8s.amazonaws.com/accelerator</code> label g4dn nvidia-tesla-t4 g5 nvidia-tesla-a10 p4d nvidia-ampere-a100 p4de nvidia-ampere-a100e p5 nvidia-hopper-h100 <p>We also recommend setting the following taint on your GPU nodes to prevent pods requiring GPU resources from being scheduled on them: - { key = \"nvidia.com/gpu\", value = \"true\", effect = \"NO_SCHEDULE\" }</p>"},{"location":"guides/self_hosting/#postgresql","title":"PostgreSQL","text":"<p>The LLM Engine server requires a PostgreSQL database to back data. LLM Engine currently supports PostgreSQL version 14. Create a PostgreSQL database (e.g. AWS RDS PostgreSQL) if you do not have an existing one you wish to connect LLM Engine to.</p> <p>To enable LLM Engine to connect to the PostgreSQL engine, we create a Kubernetes secret with the PostgreSQL url. An example YAML is provided below: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: llm-engine-database-credentials  # this name will be an input to our Helm Chart\ndata:\n    database_url = \"postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&amp;...]\"\n</code></pre></p>"},{"location":"guides/self_hosting/#redis","title":"Redis","text":"<p>The LLM Engine server requires Redis for various caching/queue functionality. LLM Engine currently supports Redis version 6. Create a Redis cluster (e.g. AWS Elasticache for Redis) if you do not have an existing one you wish to connect LLM Engine to.</p> <p>To enable LLM Engine to connect redis, fill out the Helm chart values with the redis host and url.</p>"},{"location":"guides/self_hosting/#amazon-s3","title":"Amazon S3","text":"<p>You will need to have an S3 bucket for LLM Engine to store various assets (e.g model weigts, prediction restuls). The ARN of this bucket should be provided in the Helm chart values.</p>"},{"location":"guides/self_hosting/#amazon-ecr","title":"Amazon ECR","text":"<p>You will need to provide an ECR repository for the deployment to store model containers. The ARN of this repository should be provided in the Helm chart values.</p>"},{"location":"guides/self_hosting/#amazon-sqs","title":"Amazon SQS","text":"<p>LLM Engine utilizes Amazon SQS to keep track of jobs. LLM Engine will create and use SQS queues as needed.</p>"},{"location":"guides/self_hosting/#identity-and-access-management-iam","title":"Identity and Access Management (IAM)","text":"<p>The LLM Engine server will an IAM role to perform various AWS operations. This role will be assumed by the serviceaccount <code>llm-engine</code> in the <code>launch</code> namespace in the EKS cluster. The ARN of this role needs to be provided to the Helm chart, and the role needs to be provided the following permissions:</p> Action Resource <code>s3:Get*</code>, <code>s3:Put*</code> <code>${s3_bucket_arn}/*</code> <code>s3:List*</code> <code>${s3_bucket_arn}</code> <code>sqs:*</code> <code>arn:aws:sqs:${region}:${account_id}:llm-engine-endpoint-id-*</code> <code>sqs:ListQueues</code> <code>*</code> <code>ecr:BatchGetImage</code>, <code>ecr:DescribeImages</code>, <code>ecr:GetDownloadUrlForLayer</code>, <code>ecr:ListImages</code> <code>${ecr_repository_arn}</code>"},{"location":"guides/self_hosting/#helm-chart","title":"Helm Chart","text":"<p>Now that all dependencies have been installed and configured, we can run the provided Helm chart. The values in the Helm chart will need to correspond with the resources described in the Dependencies section. </p> <p>Ensure that Helm V3 is installed instructions and can connect to the EKS cluster. Users should be able to install the chart with <code>helm install llm-engine llm-engine -f llm-engine/values_sample.yaml -n &lt;DESIRED_NAMESPACE&gt;</code>. Below are the configurations to specify in the <code>values_sample.yaml</code> file. </p> Parameter Description Required tag The LLM Engine docker image tag Yes context A user-specified deployment tag No image.gatewayRepository The docker repository to pull the LLM Engine gateway image from Yes image.builderRepository The docker repository to pull the LLM Engine endpoint builder image from Yes image.cacherRepository The docker repository to pull the LLM Engine cacher image from Yes image.forwarderRepository The docker repository to pull the LLM Engine forwarder image from Yes image.pullPolicy The docker image pull policy No secrets.kubernetesDatabaseSecretName The name of the secret that contains the database credentials Yes serviceAccount.annotations.eks.amazonaws.com/role-arn The ARN of the IAM role that the service account will assume Yes service.type The service configuration for the main LLM Engine server No service.port The service configuration for the main LLM Engine server No replicaCount The amount of replica pods for each deployment No autoscaling The autoscaling configuration for LLM Engine server deployments No resources.requests.cpu The k8s resources for LLM Engine server deployments No nodeSelector The node selector for LLM Engine server deployments No tolerations The tolerations for LLM Engine server deployments No affinity The affinity for LLM Engine server deployments No aws.configMap.name The AWS configurations (by configMap) for LLM Engine server deployments No aws.configMap.create The AWS configurations (by configMap) for LLM Engine server deployments No aws.profileName The AWS configurations (by configMap) for LLM Engine server deployments No serviceTemplate.securityContext.capabilities.drop Additional flags for model endpoints No serviceTemplate.mountInfraConfig Additional flags for model endpoints No config.values.infra.k8s_cluster_name The name of the k8s cluster Yes config.values.infra.dns_host_domain The domain name of the k8s cluster Yes config.values.infra.default_region The default AWS region for various resources Yes config.values.infra.ml_account_id The AWS account ID for various resources Yes config.values.infra.docker_repo_prefix The prefix for AWS ECR repositories Yes config.values.infra.redis_host The hostname of the redis cluster you wish to connect Yes config.values.infra.s3_bucket The S3 bucket you wish to connect Yes config.values.llm_engine.endpoint_namespace K8s namespace the endpoints will be created in Yes config.values.llm_engine.cache_redis_aws_url The full url for the redis cluster you wish to connect No config.values.llm_engine.cache_redis_azure_host The redis cluster host when using cloud_provider azure No config.values.llm_engine.s3_file_llm_fine_tuning_job_repository The S3 URI for the S3 bucket/key that you wish to save fine-tuned assets Yes config.values.dd_trace_enabled Whether to enable datadog tracing, datadog must be installed in the cluster No"},{"location":"guides/self_hosting/#play-with-it","title":"Play With It","text":"<p>Once <code>helm install</code> succeeds, you can forward port <code>5000</code> from a <code>llm-engine</code> pod and test sending requests to it.</p> <p>First, see a list of pods in the namespace that you performed <code>helm install</code> in: <pre><code>$ kubectl get pods -n &lt;NAMESPACE_WHERE_LLM_ENGINE_IS_INSTALLED&gt;\nNAME                                           READY   STATUS             RESTARTS      AGE\nllm-engine-668679554-9q4wj                     1/1     Running            0             18m\nllm-engine-668679554-xfhxx                     1/1     Running            0             18m\nllm-engine-cacher-5f8b794585-fq7dj             1/1     Running            0             18m\nllm-engine-endpoint-builder-5cd6bf5bbc-sm254   1/1     Running            0             18m\nllm-engine-image-cache-a10-sw4pg               1/1     Running            0             18m \n</code></pre> Note the pod names you see may be different.</p> <p>Forward a port from a <code>llm-engine</code> pod: <pre><code>$ kubectl port-forward pod/llm-engine-&lt;REST_OF_POD_NAME&gt; 5000:5000 -n &lt;NAMESPACE_WHERE_LLM_ENGINE_IS_INSTALLED&gt;\n</code></pre></p> <p>Then, try sending a request to get LLM model endpoints for <code>test-user-id</code>: <pre><code>$ curl -X GET -H \"Content-Type: application/json\" -u \"test-user-id:\" \"http://localhost:5000/v1/llm/model-endpoints\"\n</code></pre></p> <p>You should get the following response: <pre><code>{\"model_endpoints\":[]}\n</code></pre></p> <p>Next, let's create a LLM endpoint using llama-7b: <pre><code>$ curl -X POST 'http://localhost:5000/v1/llm/model-endpoints' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"name\": \"llama-7b\",\n        \"model_name\": \"llama-7b\",\n        \"source\": \"hugging_face\",\n        \"inference_framework\": \"text_generation_inference\",\n        \"inference_framework_image_tag\": \"0.9.3\",\n        \"num_shards\": 4,\n        \"endpoint_type\": \"streaming\",\n        \"cpus\": 32,\n        \"gpus\": 4,\n        \"memory\": \"40Gi\",\n        \"storage\": \"40Gi\",\n        \"gpu_type\": \"nvidia-ampere-a10\",\n        \"min_workers\": 1,\n        \"max_workers\": 12,\n        \"per_worker\": 1,\n        \"labels\": {},\n        \"metadata\": {}\n    }' \\\n    -u test_user_id:\n</code></pre></p> <p>It should output something like: <pre><code>{\"endpoint_creation_task_id\":\"8d323344-b1b5-497d-a851-6d6284d2f8e4\"}\n</code></pre></p> <p>Wait a few minutes for the endpoint to be ready. You can tell that it's ready by listing pods and checking that all containers in the llm endpoint pod are ready: <pre><code>$ kubectl get pods -n &lt;endpoint_namespace specified in values_sample.yaml&gt;\nNAME                                                              READY   STATUS    RESTARTS        AGE\nllm-engine-endpoint-id-end-cismpd08agn003rr2kc0-7f86ff64f9qj9xp   2/2     Running   1 (4m41s ago)   7m26s\n</code></pre> Note the endpoint name could be different.</p> <p>Then, you can send an inference request to the endppoint: <pre><code>$ curl -X POST 'http://localhost:5000/v1/llm/completions-sync?model_endpoint_name=llama-7b' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"prompts\": [\"Tell me a joke about AI\"],\n        \"max_new_tokens\": 30,\n        \"temperature\": 0.1\n    }' \\\n    -u test-user-id:\n</code></pre></p> <p>You should get a response similar to: <pre><code>{\"status\":\"SUCCESS\",\"outputs\":[{\"text\":\". Tell me a joke about AI. Tell me a joke about AI. Tell me a joke about AI. Tell me\",\"num_completion_tokens\":30}],\"traceback\":null}\n</code></pre></p>"},{"location":"guides/self_hosting/#pointing-llm-engine-client-to-use-self-hosted-infrastructure","title":"Pointing LLM Engine client to use self-hosted infrastructure","text":"<p>The <code>llmengine</code> client makes requests to Scale AI's hosted infrastructure by default. You can have <code>llmengine</code> client make requests to your own self-hosted infrastructure by setting the <code>LLM_ENGINE_BASE_PATH</code> environment variable to the URL of the <code>llm-engine</code> service. </p> <p>The exact URL of <code>llm-engine</code> service depends on your Kubernetes cluster networking setup. The domain is specified at <code>config.values.infra.dns_host_domain</code> in the helm chart values config file. Using <code>charts/llm-engine/values_sample.yaml</code> as an example, you would do: <pre><code>export LLM_ENGINE_BASE_PATH=https://llm-engine.domain.com\n</code></pre></p>"},{"location":"guides/token_streaming/","title":"Token streaming","text":"<p>The Completions APIs support a <code>stream</code> boolean parameter that, when <code>True</code>, will return a streamed response of token-by-token server-sent events (SSEs) rather than waiting to receive the full response when model generation has finished. This decreases latency of when you start getting a response.</p> <p>The response will consist of SSEs of the form <code>{\"token\": dict, \"generated_text\": str | null, \"details\": dict | null}</code>, where the dictionary for each token will contain log probability information in addition to the generated string; the <code>generated_text</code> field will be <code>null</code> for all but the last SSE, for which it will contain the full generated response.</p>"}]}