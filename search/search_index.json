{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#llm-engine","title":"LLM Engine","text":"<p>The open source engine for fine-tuning and serving large language models.</p> <p>LLM Engine is the easiest way to customize and serve LLMs.</p> <p>LLMs can be accessed via Scale's hosted version or by using the Helm charts in this repository to run model inference and fine-tuning in your own infrastructure.</p>"},{"location":"#quick-install","title":"Quick Install","text":"Install the python package <pre><code>pip install scale-llm-engine\n</code></pre>"},{"location":"#about","title":"About","text":"<p>Foundation models are emerging as the building blocks of AI. However, deploying these models to the  cloud and fine-tuning them is an expensive operation that require infrastructure and ML expertise.  It is also difficult to maintain over time as new models are released and new techniques for both inference and fine-tuning are made available.</p> <p>LLM Engine is a Python library and Helm chart that provides everything you need to serve and fine-tune foundation models, whether you use Scale's hosted infrastructure or do it in your own cloud infrastructure using Kubernetes.</p>"},{"location":"#key-features","title":"Key Features","text":"<p>Ready-to-use APIs for your favorite models: Deploy and serve open source foundation models - including Llama-2, MPT, and Falcon. Use Scale-hosted models or deploy to your own infrastructure.</p> <p>Fine-tune the best open-source models: Fine-tune open-source foundation models like Llama-2, MPT, etc. with your own data for optimized performance.</p> <p>Optimized Inference: LLM Engine provides inference APIs for streaming responses and dynamically batching inputs for higher throughput and lower latency.</p> <p>Open-Source Integrations: Deploy any Hugging Face model with a single command.</p> <p>Deploying from any docker image: Turn any Docker image into an auto-scaling deployment with simple APIs.</p>"},{"location":"#features-coming-soon","title":"Features Coming Soon","text":"<p>Kubernetes Installation Enhancements: We are working hard to enhance the  installation and maintenance of inference and fine-tuning functionality on  your infrastructure. For now, our documentation covers experimental libraries to deploy language models on your infrastructure  and libraries to access Scale's hosted infrastructure.</p> <p>Fast Cold-Start Times: To prevent GPUs from idling, LLM Engine automatically scales your model to zero when it's not in use and scales up within seconds, even for large foundation models.</p> <p>Cost Optimization: Deploy AI models cheaper than commercial ones, including cold-start and warm-down times.</p>"},{"location":"contributing/","title":"Contributing to LLM Engine","text":""},{"location":"contributing/#updating-llm-engine-documentation","title":"Updating LLM Engine Documentation","text":"<p>LLM Engine leverages mkdocs to create beautiful, community-oriented documentation.</p>"},{"location":"contributing/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<p>Clone/Fork the LLM Engine Repository. Our documentation lives in the <code>docs</code> folder.</p>"},{"location":"contributing/#step-2-install-the-dependencies","title":"Step 2: Install the Dependencies","text":"<p>Dependencies are located in <code>requirements-docs.txt</code>, go ahead and pip install those with </p> <pre><code>pip install -r requirements-docs.txt\n</code></pre>"},{"location":"contributing/#step-3-install-the-python-client-locally","title":"Step 3: Install the Python client locally","text":"<p>Our Python client API reference is autogenerated from our client. You can install the client in editable mode with</p> <pre><code>pip install -e clients/python\n</code></pre>"},{"location":"contributing/#step-4-run-locally","title":"Step 4: Run Locally","text":"<p>To run the documentation service locally, execute the following command:</p> <pre><code>mkdocs serve\n</code></pre> <p>This should kick off a locally running instance on http://127.0.0.1:8000/.</p> <p>As you edit the content in the <code>docs</code> folder, the site will be automatically reloaded on each file save.</p>"},{"location":"contributing/#step-5-editing-navigation-and-settings","title":"Step 5: Editing Navigation and Settings","text":"<p>If you are less familiar with <code>mkdocs</code>, in addition to the markdown content in the <code>docs</code> folder, there is a top-level  <code>mkdocs.yml</code> file as well that defines the navigation pane and other website settings. If you don't see your page where  you think it should be, double-check the .yml file.</p>"},{"location":"contributing/#step-6-building-and-deploying","title":"Step 6: Building and Deploying","text":"<p>CircleCI (via <code>.circleci/config.yml</code>) handles the building and deployment of our documentation service for us.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"getting_started/","title":"Getting Started","text":"<p>Note: As of October 31st 2024, LLM Engine's public demo service is sunsetted. We have thus removed the documentation  pieces relating to calling the demo service, procuring a Spellbook API key, etc. Please view our Self Hosting Guide instead.  We will however leave behind the Example Code snippets for posterity, and as a reference for self-hosted and Scale internal users.</p> <p>To start, install LLM Engine via pip:</p> pip <pre><code>pip install scale-llm-engine\n</code></pre>"},{"location":"getting_started/#scale-user-id","title":"Scale user ID","text":"<p>Next, you need a Scale user ID. Recall that this is only applicable to Scale internal users for now, and we are just leaving  this note to serve as internal documentation.</p>"},{"location":"getting_started/#set-your-api-key","title":"Set your API Key","text":"<p>LLM Engine uses environment variables to access your API key.</p> <p>Set the <code>SCALE_API_KEY</code> environment variable to your Scale user ID by running the following command in your terminal before you run your python application.</p> <pre><code>export SCALE_API_KEY=\"[Your Scale user ID]\"\n</code></pre> <p>You can also add in the line above to your <code>.zshrc</code> or <code>.bash_profile</code> so it's automatically set for future sessions.</p> <p>Alternatively, you can also set your API key using either of the following patterns: <pre><code>llmengine.api_engine.api_key = \"abc\"\nllmengine.api_engine.set_api_key(\"abc\")\n</code></pre> These patterns are useful for Jupyter Notebook users to set API keys without the need for using <code>os.environ</code>.</p>"},{"location":"getting_started/#example-code","title":"Example Code","text":""},{"location":"getting_started/#sample-completion","title":"Sample Completion","text":"<p>With your API key set, you can now send LLM Engine requests using the Python client:</p> <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"I'm opening a pancake restaurant that specializes in unique pancake shapes, colors, and flavors. List 3 quirky names I could name my restaurant.\",\n    max_new_tokens=100,\n    temperature=0.2,\n)\n\nprint(response.output.text)\n</code></pre>"},{"location":"getting_started/#with-streaming","title":"With Streaming","text":"<pre><code>import sys\n\nfrom llmengine import Completion\n\nstream = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Give me a 200 word summary on the current economic events in the US.\",\n    max_new_tokens=1000,\n    temperature=0.2,\n    stream=True,\n)\n\nfor response in stream:\n    if response.output:\n        print(response.output.text, end=\"\")\n        sys.stdout.flush()\n    else: # an error occurred\n        print(response.error) # print the error message out \n        break\n</code></pre>"},{"location":"integrations/","title":"Integrations","text":""},{"location":"integrations/#weights-biases","title":"Weights &amp; Biases","text":"<p>LLM Engine integrates with Weights &amp; Biases to track metrics during fine tuning. To enable:</p> <pre><code>from llmengine import FineTune\n\nresponse = FineTune.create(\n    model=\"llama-2-7b\",\n    training_file=\"s3://my-bucket/path/to/training-file.csv\",\n    validation_file=\"s3://my-bucket/path/to/validation-file.csv\",\n    hyperparameters={\"report_to\": \"wandb\"},\n    wandb_config={\"api_key\":\"key\", \"project\":\"fine-tune project\"}\n)\n</code></pre> <p>Configs to specify:</p> <ul> <li>(Required) Set <code>hyperparameters.report_to</code> to <code>wandb</code> to enables automatic metrics tracking.</li> <li>(Required) Set <code>wandb_config.api_key</code> to the API key.</li> <li>(Optional) Set <code>wandb_config.base_url</code> to use a custom Weights &amp; Biases server.</li> <li><code>wandb_config</code> also accepts keys from wandb.init().</li> </ul>"},{"location":"model_zoo/","title":"Public Model Zoo","text":"<p>Scale hosts the following models in the LLM Engine Model Zoo:</p> Model Name Inference APIs Available Fine-tuning APIs Available Inference Frameworks Available Inference max total tokens (prompt + response) <code>llama-7b</code> \u2705 \u2705 deepspeed, text-generation-inference 2048 <code>llama-2-7b</code> \u2705 \u2705 text-generation-inference, vllm 4096 <code>llama-2-7b-chat</code> \u2705 text-generation-inference, vllm 4096 <code>llama-2-13b</code> \u2705 text-generation-inference, vllm 4096 <code>llama-2-13b-chat</code> \u2705 text-generation-inference, vllm 4096 <code>llama-2-70b</code> \u2705 \u2705 text-generation-inference, vllm 4096 <code>llama-2-70b-chat</code> \u2705 text-generation-inference, vllm 4096 <code>llama-3-8b</code> \u2705 vllm 8192 <code>llama-3-8b-instruct</code> \u2705 vllm 8192 <code>llama-3-70b</code> \u2705 vllm 8192 <code>llama-3-70b-instruct</code> \u2705 vllm 8192 <code>llama-3-1-8b</code> \u2705 vllm 131072 <code>llama-3-1-8b-instruct</code> \u2705 vllm 131072 <code>llama-3-1-70b</code> \u2705 vllm 131072 <code>llama-3-1-70b-instruct</code> \u2705 vllm 131072 <code>falcon-7b</code> \u2705 text-generation-inference, vllm 2048 <code>falcon-7b-instruct</code> \u2705 text-generation-inference, vllm 2048 <code>falcon-40b</code> \u2705 text-generation-inference, vllm 2048 <code>falcon-40b-instruct</code> \u2705 text-generation-inference, vllm 2048 <code>mpt-7b</code> \u2705 deepspeed, text-generation-inference, vllm 2048 <code>mpt-7b-instruct</code> \u2705 \u2705 deepspeed, text-generation-inference, vllm 2048 <code>flan-t5-xxl</code> \u2705 deepspeed, text-generation-inference 2048 <code>mistral-7b</code> \u2705 \u2705 vllm 8000 <code>mistral-7b-instruct</code> \u2705 \u2705 vllm 8000 <code>mixtral-8x7b</code> \u2705 vllm 32768 <code>mixtral-8x7b-instruct</code> \u2705 vllm 32768 <code>mixtral-8x22b</code> \u2705 vllm 65536 <code>mixtral-8x22b-instruct</code> \u2705 vllm 65536 <code>codellama-7b</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-7b-instruct</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-13b</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-13b-instruct</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-34b</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-34b-instruct</code> \u2705 \u2705 text-generation-inference, vllm 16384 <code>codellama-70b</code> \u2705 vllm 16384 <code>codellama-70b-instruct</code> \u2705 vllm 4096 <code>zephyr-7b-alpha</code> \u2705 text-generation-inference, vllm 32768 <code>zephyr-7b-beta</code> \u2705 text-generation-inference, vllm 32768 <code>gemma-2b</code> \u2705 vllm 8192 <code>gemma-2b-instruct</code> \u2705 vllm 8192 <code>gemma-7b</code> \u2705 vllm 8192 <code>gemma-7b-instruct</code> \u2705 vllm 8192 <code>phi-3-mini-4k-instruct</code> \u2705 vllm 4096 <code>deepseek-coder-v2</code> \u2705 vllm 131072 <code>deepseek-coder-v2-instruct</code> \u2705 vllm 131072 <code>deepseek-coder-v2-lite</code> \u2705 vllm 131072 <code>deepseek-coder-v2-lite-instruct</code> \u2705 vllm 131072 <code>qwen2-72b-instruct</code> \u2705 vllm 32768"},{"location":"model_zoo/#usage","title":"Usage","text":"<p>Each of these models can be used with the Completion API.</p> <p>The specified models can be fine-tuned with the FineTune API.</p> <p>More information about the models can be found using the Model API.</p>"},{"location":"pricing/","title":"Pricing","text":"<p>LLM Engine is an open-source project and free self-hosting will always be an option. As of October 31st 2024,  the free demo service is sunsetted.</p>"},{"location":"pricing/#self-hosted-models","title":"Self-Hosted Models","text":"<p>We are committed to supporting the open-source community. Self-hosting LLM Engine will remain free and open-source.</p> <p>We would love contributions from the community make this even more amazing!</p>"},{"location":"api/data_types/","title":"\ud83d\udc0d Python Client Data Type Reference","text":""},{"location":"api/data_types/#llmengine.CompletionOutput","title":"CompletionOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents the output of a completion request to a model.</p>"},{"location":"api/data_types/#llmengine.CompletionOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text of the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/data_types/#llmengine.CompletionOutput.num_completion_tokens","title":"num_completion_tokens  <code>instance-attribute</code>","text":"<pre><code>num_completion_tokens: int\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput","title":"CompletionStreamOutput","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.text","title":"text  <code>instance-attribute</code>","text":"<pre><code>text: str\n</code></pre> <p>The text of the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.finished","title":"finished  <code>instance-attribute</code>","text":"<pre><code>finished: bool\n</code></pre> <p>Whether the completion is finished.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.num_prompt_tokens","title":"num_prompt_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_prompt_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the prompt.</p>"},{"location":"api/data_types/#llmengine.CompletionStreamOutput.num_completion_tokens","title":"num_completion_tokens  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_completion_tokens: Optional[int] = None\n</code></pre> <p>Number of tokens in the completion.</p>"},{"location":"api/data_types/#llmengine.CompletionSyncResponse","title":"CompletionSyncResponse  <code>module-attribute</code>","text":"<pre><code>CompletionSyncResponse: TypeAlias = CompletionSyncV1Response\n</code></pre>"},{"location":"api/data_types/#llmengine.CompletionStreamResponse","title":"CompletionStreamResponse  <code>module-attribute</code>","text":"<pre><code>CompletionStreamResponse: TypeAlias = (\n    CompletionStreamV1Response\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateFineTuneResponse","title":"CreateFineTuneResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for creating a FineTune.</p>"},{"location":"api/data_types/#llmengine.CreateFineTuneResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(\n    ..., description=\"ID of the created fine-tuning job.\"\n)\n</code></pre> <p>The ID of the FineTune.</p>"},{"location":"api/data_types/#llmengine.GetFineTuneResponse","title":"GetFineTuneResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for retrieving a FineTune.</p>"},{"location":"api/data_types/#llmengine.GetFineTuneResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(..., description=\"ID of the requested job.\")\n</code></pre> <p>The ID of the FineTune.</p>"},{"location":"api/data_types/#llmengine.GetFineTuneResponse.fine_tuned_model","title":"fine_tuned_model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fine_tuned_model: Optional[str] = Field(\n    default=None,\n    description=\"Name of the resulting fine-tuned model. This can be plugged into the Completion API once the fine-tune is complete\",\n)\n</code></pre> <p>The name of the resulting fine-tuned model. This can be plugged into the Completion API once the fine-tune is complete.</p>"},{"location":"api/data_types/#llmengine.ListFineTunesResponse","title":"ListFineTunesResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for listing FineTunes.</p>"},{"location":"api/data_types/#llmengine.ListFineTunesResponse.jobs","title":"jobs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>jobs: List[GetFineTuneResponse] = Field(\n    ...,\n    description=\"List of fine-tuning jobs and their statuses.\",\n)\n</code></pre> <p>A list of FineTunes, represented as <code>GetFineTuneResponse</code>s.</p>"},{"location":"api/data_types/#llmengine.CancelFineTuneResponse","title":"CancelFineTuneResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for cancelling a FineTune.</p>"},{"location":"api/data_types/#llmengine.CancelFineTuneResponse.success","title":"success  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>success: bool = Field(\n    ..., description=\"Whether cancellation was successful.\"\n)\n</code></pre> <p>Whether the cancellation succeeded.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse","title":"GetLLMEndpointResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for retrieving a Model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.name","title":"name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>name: str = Field(\n    description=\"The name of the model. Use this for making inference requests to the model.\"\n)\n</code></pre> <p>The name of the model. Use this for making inference requests to the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.source","title":"source  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source: LLMSource = Field(\n    description=\"The source of the model, e.g. Hugging Face.\"\n)\n</code></pre> <p>The source of the model, e.g. Hugging Face.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.inference_framework","title":"inference_framework  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inference_framework: LLMInferenceFramework = Field(\n    description=\"The inference framework used by the model.\"\n)\n</code></pre> <p>(For self-hosted users) The inference framework used by the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: Optional[str] = Field(\n    default=None,\n    description=\"(For self-hosted users) The autogenerated ID of the model.\",\n)\n</code></pre> <p>(For self-hosted users) The autogenerated ID of the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: Optional[str] = Field(\n    default=None,\n    description=\"(For self-hosted users) For fine-tuned models, the base model. For base models, this will be the same as `name`.\",\n)\n</code></pre> <p>(For self-hosted users) For fine-tuned models, the base model. For base models, this will be the same as <code>name</code>.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.status","title":"status  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>status: ModelEndpointStatus = Field(\n    description=\"The status of the model.\"\n)\n</code></pre> <p>The status of the model (can be one of \"READY\", \"UPDATE_PENDING\", \"UPDATE_IN_PROGRESS\", \"UPDATE_FAILED\", \"DELETE_IN_PROGRESS\").</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.inference_framework_tag","title":"inference_framework_tag  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>inference_framework_tag: Optional[str] = Field(\n    default=None,\n    description=\"(For self-hosted users) The Docker image tag used to run the model.\",\n)\n</code></pre> <p>(For self-hosted users) The Docker image tag used to run the model.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.num_shards","title":"num_shards  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_shards: Optional[int] = Field(\n    default=None,\n    description=\"(For self-hosted users) The number of shards.\",\n)\n</code></pre> <p>(For self-hosted users) The number of shards.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.quantize","title":"quantize  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>quantize: Optional[Quantization] = Field(\n    default=None,\n    description=\"(For self-hosted users) The quantization method.\",\n)\n</code></pre> <p>(For self-hosted users) The quantization method.</p>"},{"location":"api/data_types/#llmengine.GetLLMEndpointResponse.spec","title":"spec  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>spec: Optional[GetModelEndpointResponse] = Field(\n    default=None,\n    description=\"(For self-hosted users) Model endpoint details.\",\n)\n</code></pre> <p>(For self-hosted users) Model endpoint details.</p>"},{"location":"api/data_types/#llmengine.ListLLMEndpointsResponse","title":"ListLLMEndpointsResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for listing Models.</p>"},{"location":"api/data_types/#llmengine.ListLLMEndpointsResponse.model_endpoints","title":"model_endpoints  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_endpoints: List[GetLLMEndpointResponse] = Field(\n    ..., description=\"The list of models.\"\n)\n</code></pre> <p>A list of Models, represented as <code>GetLLMEndpointResponse</code>s.</p>"},{"location":"api/data_types/#llmengine.DeleteLLMEndpointResponse","title":"DeleteLLMEndpointResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for deleting a Model.</p>"},{"location":"api/data_types/#llmengine.DeleteLLMEndpointResponse.deleted","title":"deleted  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deleted: bool = Field(\n    ..., description=\"Whether deletion was successful.\"\n)\n</code></pre> <p>Whether the deletion succeeded.</p>"},{"location":"api/data_types/#llmengine.ModelDownloadRequest","title":"ModelDownloadRequest","text":"<p>               Bases: <code>BaseModel</code></p> <p>Request object for downloading a model.</p>"},{"location":"api/data_types/#llmengine.ModelDownloadRequest.model_name","title":"model_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_name: str = Field(\n    ..., description=\"Name of the model to download.\"\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.ModelDownloadRequest.download_format","title":"download_format  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>download_format: Optional[str] = Field(\n    default=\"hugging_face\",\n    description=\"Desired return format for downloaded model weights (default=hugging_face).\",\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.ModelDownloadResponse","title":"ModelDownloadResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for downloading a model.</p>"},{"location":"api/data_types/#llmengine.ModelDownloadResponse.urls","title":"urls  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>urls: Dict[str, str] = Field(\n    ...,\n    description=\"Dictionary of (file_name, url) pairs to download the model from.\",\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.UploadFileResponse","title":"UploadFileResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for uploading a file.</p>"},{"location":"api/data_types/#llmengine.UploadFileResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(..., description=\"ID of the uploaded file.\")\n</code></pre> <p>ID of the uploaded file.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse","title":"GetFileResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for retrieving a file.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(\n    ..., description=\"ID of the requested file.\"\n)\n</code></pre> <p>ID of the requested file.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse.filename","title":"filename  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filename: str = Field(..., description='File name.')\n</code></pre> <p>File name.</p>"},{"location":"api/data_types/#llmengine.GetFileResponse.size","title":"size  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>size: int = Field(\n    ..., description=\"Length of the file, in characters.\"\n)\n</code></pre> <p>Length of the file, in characters.</p>"},{"location":"api/data_types/#llmengine.GetFileContentResponse","title":"GetFileContentResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for retrieving a file's content.</p>"},{"location":"api/data_types/#llmengine.GetFileContentResponse.id","title":"id  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>id: str = Field(\n    ..., description=\"ID of the requested file.\"\n)\n</code></pre> <p>ID of the requested file.</p>"},{"location":"api/data_types/#llmengine.GetFileContentResponse.content","title":"content  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>content: str = Field(..., description='File content.')\n</code></pre> <p>File content.</p>"},{"location":"api/data_types/#llmengine.ListFilesResponse","title":"ListFilesResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for listing files.</p>"},{"location":"api/data_types/#llmengine.ListFilesResponse.files","title":"files  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>files: List[GetFileResponse] = Field(\n    ..., description=\"List of file IDs, names, and sizes.\"\n)\n</code></pre> <p>List of file IDs, names, and sizes.</p>"},{"location":"api/data_types/#llmengine.DeleteFileResponse","title":"DeleteFileResponse","text":"<p>               Bases: <code>BaseModel</code></p> <p>Response object for deleting a file.</p>"},{"location":"api/data_types/#llmengine.DeleteFileResponse.deleted","title":"deleted  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>deleted: bool = Field(\n    ..., description=\"Whether deletion was successful.\"\n)\n</code></pre> <p>Whether deletion was successful.</p>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequestContent","title":"CreateBatchCompletionsRequestContent  <code>module-attribute</code>","text":"<pre><code>CreateBatchCompletionsRequestContent: TypeAlias = (\n    CreateBatchCompletionsV1RequestContent\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsModelConfig","title":"CreateBatchCompletionsModelConfig  <code>module-attribute</code>","text":"<pre><code>CreateBatchCompletionsModelConfig: TypeAlias = (\n    CreateBatchCompletionsV1ModelConfig\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsRequest","title":"CreateBatchCompletionsRequest  <code>module-attribute</code>","text":"<pre><code>CreateBatchCompletionsRequest: TypeAlias = (\n    CreateBatchCompletionsV1Request\n)\n</code></pre>"},{"location":"api/data_types/#llmengine.CreateBatchCompletionsResponse","title":"CreateBatchCompletionsResponse  <code>module-attribute</code>","text":"<pre><code>CreateBatchCompletionsResponse: TypeAlias = (\n    CreateBatchCompletionsV1Response\n)\n</code></pre>"},{"location":"api/error_handling/","title":"Error handling","text":"<p>LLM Engine uses conventional HTTP response codes to indicate the success or failure of an API request. In general: codes in the <code>2xx</code> range indicate success. Codes in the <code>4xx</code> range indicate indicate an error that failed given the  information provided (e.g. a given Model was not found, or an invalid temperature was specified). Codes in the <code>5xx</code>  range indicate an error with the LLM Engine servers.</p> <p>In the Python client, errors are presented via a set of corresponding Exception classes, which should be caught  and handled by the user accordingly.</p>"},{"location":"api/error_handling/#llmengine.errors.BadRequestError","title":"BadRequestError","text":"<pre><code>BadRequestError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Corresponds to HTTP 400. Indicates that the request had inputs that were invalid. The user should not attempt to retry the request without changing the inputs.</p>"},{"location":"api/error_handling/#llmengine.errors.UnauthorizedError","title":"UnauthorizedError","text":"<pre><code>UnauthorizedError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Corresponds to HTTP 401. This means that no valid API key was provided.</p>"},{"location":"api/error_handling/#llmengine.errors.NotFoundError","title":"NotFoundError","text":"<pre><code>NotFoundError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Corresponds to HTTP 404. This means that the resource (e.g. a Model, FineTune, etc.) could not be found. Note that this can also be returned in some cases where the object might exist, but the user does not have access to the object. This is done to avoid leaking information about the existence or nonexistence of said object that the user does not have access to.</p>"},{"location":"api/error_handling/#llmengine.errors.RateLimitExceededError","title":"RateLimitExceededError","text":"<pre><code>RateLimitExceededError(message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Corresponds to HTTP 429. Too many requests hit the API too quickly. We recommend an exponential backoff for retries.</p>"},{"location":"api/error_handling/#llmengine.errors.ServerError","title":"ServerError","text":"<pre><code>ServerError(status_code: int, message: str)\n</code></pre> <p>               Bases: <code>Exception</code></p> <p>Corresponds to HTTP 5xx errors on the server.</p>"},{"location":"api/langchain/","title":"\ud83e\udd9c Langchain","text":"<p>Coming soon!</p>"},{"location":"api/python_client/","title":"\ud83d\udc0d Python Client API Reference","text":""},{"location":"api/python_client/#llmengine.Completion","title":"Completion","text":"<p>               Bases: <code>APIEngine</code></p> <p>Completion API. This API is used to generate text completions.</p> <p>Language models are trained to understand natural language and predict text outputs as a response to their inputs. The inputs are called prompts and the outputs are referred to as completions. LLMs take the input prompts and chunk them into smaller units called tokens to process and generate language. Tokens may include trailing spaces and even sub-words; this process is language dependent.</p> <p>The Completion API can be run either synchronous or asynchronously (via Python <code>asyncio</code>). For each of these modes, you can also choose whether to stream token responses or not.</p>"},{"location":"api/python_client/#llmengine.Completion.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    model: str,\n    prompt: str,\n    max_new_tokens: int = 20,\n    temperature: float = 0.2,\n    stop_sequences: Optional[List[str]] = None,\n    return_token_log_probs: Optional[bool] = False,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    include_stop_str_in_output: Optional[bool] = None,\n    guided_json: Optional[Dict[str, Any]] = None,\n    guided_regex: Optional[str] = None,\n    guided_choice: Optional[List[str]] = None,\n    guided_grammar: Optional[str] = None,\n    timeout: int = COMPLETION_TIMEOUT,\n    stream: bool = False,\n    request_headers: Optional[Dict[str, str]] = None,\n    **kwargs\n) -&gt; Union[\n    CompletionSyncResponse,\n    Iterator[CompletionStreamResponse],\n]\n</code></pre> <p>Creates a completion for the provided prompt and parameters synchronously.</p> <p>This API can be used to get the LLM to generate a completion synchronously. It takes as parameters the <code>model</code> (see Model Zoo) and the <code>prompt</code>. Optionally it takes <code>max_new_tokens</code>, <code>temperature</code>, <code>timeout</code> and <code>stream</code>. It returns a CompletionSyncResponse if <code>stream=False</code> or an async iterator of CompletionStreamResponse with <code>request_id</code> and <code>outputs</code> fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use. See Model Zoo for a list of Models that are supported.</p> required <code>prompt</code> <code>str</code> <p>The prompt to generate completions for, encoded as a string.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion.</p> <p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See Model Zoo for information on each supported model's context length.</p> <code>20</code> <code>temperature</code> <code>float</code> <p>What sampling temperature to use, in the range <code>[0, 1]</code>. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. When temperature is 0 greedy search is used.</p> <code>0.2</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>One or more sequences where the API will stop generating tokens for the current completion.</p> <code>None</code> <code>return_token_log_probs</code> <code>Optional[bool]</code> <p>Whether to return the log probabilities of generated tokens. When True, the response will include a list of tokens and their log probabilities.</p> <code>False</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Integer that controls the number of top tokens to consider. Range: [1, infinity). -1 means consider all tokens.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Float that controls the cumulative probability of the top tokens to consider. Range: (0.0, 1.0]. 1.0 means consider all tokens.</p> <code>None</code> <code>include_stop_str_in_output</code> <code>Optional[bool]</code> <p>Whether to include the stop sequence in the output. Default to False.</p> <code>None</code> <code>guided_json</code> <code>Optional[Dict[str, Any]]</code> <p>If specified, the output will follow the JSON schema.</p> <code>None</code> <code>guided_regex</code> <code>Optional[str]</code> <p>If specified, the output will follow the regex pattern.</p> <code>None</code> <code>guided_choice</code> <code>Optional[List[str]]</code> <p>If specified, the output will be exactly one of the choices.</p> <code>None</code> <code>guided_grammar</code> <code>Optional[str]</code> <p>If specified, the output will follow the context-free grammar provided.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p> <code>COMPLETION_TIMEOUT</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamResponse]</code>. Otherwise, the return type is a <code>CompletionSyncResponse</code>. When streaming, tokens will be sent as data-only server-sent events.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncResponse, AsyncIterable[CompletionStreamResponse]]</code> <p>The generated response (if <code>stream=False</code>) or iterator of response chunks (if <code>stream=True</code>)</p> Synchronous completion without token streaming in PythonResponse in JSON <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n)\nprint(response.json())\n</code></pre> <pre><code>{\n    \"request_id\": \"8bbd0e83-f94c-465b-a12b-aabad45750a9\",\n    \"output\": {\n        \"text\": \"_______ and I am a _______\",\n        \"num_completion_tokens\": 10\n    }\n}\n</code></pre> <p>Token streaming can be used to reduce perceived latency for applications. Here is how applications can use streaming:</p> Synchronous completion with token streaming in PythonResponse in JSON <pre><code>from llmengine import Completion\n\nstream = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"why is the sky blue?\",\n    max_new_tokens=5,\n    temperature=0.2,\n    stream=True,\n)\n\nfor response in stream:\n    if response.output:\n        print(response.json())\n</code></pre> <pre><code>{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"\\n\", \"finished\": false, \"num_completion_tokens\": 1 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"I\", \"finished\": false, \"num_completion_tokens\": 2 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \" don\", \"finished\": false, \"num_completion_tokens\": 3 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"\u2019\", \"finished\": false, \"num_completion_tokens\": 4 } }\n{\"request_id\": \"ebbde00c-8c31-4c03-8306-24f37cd25fa2\", \"output\": {\"text\": \"t\", \"finished\": true, \"num_completion_tokens\": 5 } }\n</code></pre>"},{"location":"api/python_client/#llmengine.Completion.acreate","title":"acreate  <code>async</code> <code>classmethod</code>","text":"<pre><code>acreate(\n    model: str,\n    prompt: str,\n    max_new_tokens: int = 20,\n    temperature: float = 0.2,\n    stop_sequences: Optional[List[str]] = None,\n    return_token_log_probs: Optional[bool] = False,\n    presence_penalty: Optional[float] = None,\n    frequency_penalty: Optional[float] = None,\n    top_k: Optional[int] = None,\n    top_p: Optional[float] = None,\n    include_stop_str_in_output: Optional[bool] = None,\n    guided_json: Optional[Dict[str, Any]] = None,\n    guided_regex: Optional[str] = None,\n    guided_choice: Optional[List[str]] = None,\n    guided_grammar: Optional[str] = None,\n    timeout: int = COMPLETION_TIMEOUT,\n    stream: bool = False,\n    request_headers: Optional[Dict[str, str]] = None,\n    **kwargs\n) -&gt; Union[\n    CompletionSyncResponse,\n    AsyncIterable[CompletionStreamResponse],\n]\n</code></pre> <p>Creates a completion for the provided prompt and parameters asynchronously (with <code>asyncio</code>).</p> <p>This API can be used to get the LLM to generate a completion asynchronously. It takes as parameters the <code>model</code> (see Model Zoo) and the <code>prompt</code>. Optionally it takes <code>max_new_tokens</code>, <code>temperature</code>, <code>timeout</code> and <code>stream</code>. It returns a CompletionSyncResponse if <code>stream=False</code> or an async iterator of CompletionStreamResponse with <code>request_id</code> and <code>outputs</code> fields.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Name of the model to use. See Model Zoo for a list of Models that are supported.</p> required <code>prompt</code> <code>str</code> <p>The prompt to generate completions for, encoded as a string.</p> required <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to generate in the completion.</p> <p>The token count of your prompt plus <code>max_new_tokens</code> cannot exceed the model's context length. See Model Zoo for information on each supported model's context length.</p> <code>20</code> <code>temperature</code> <code>float</code> <p>What sampling temperature to use, in the range <code>[0, 1]</code>. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. When temperature is 0 greedy search is used.</p> <code>0.2</code> <code>stop_sequences</code> <code>Optional[List[str]]</code> <p>One or more sequences where the API will stop generating tokens for the current completion.</p> <code>None</code> <code>return_token_log_probs</code> <code>Optional[bool]</code> <p>Whether to return the log probabilities of generated tokens. When True, the response will include a list of tokens and their log probabilities.</p> <code>False</code> <code>presence_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>frequency_penalty</code> <code>Optional[float]</code> <p>Only supported in vllm, lightllm Penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. https://platform.openai.com/docs/guides/gpt/parameter-details Range: [0.0, 2.0]. Higher values encourage the model to use new tokens.</p> <code>None</code> <code>top_k</code> <code>Optional[int]</code> <p>Integer that controls the number of top tokens to consider. Range: [1, infinity). -1 means consider all tokens.</p> <code>None</code> <code>top_p</code> <code>Optional[float]</code> <p>Float that controls the cumulative probability of the top tokens to consider. Range: (0.0, 1.0]. 1.0 means consider all tokens.</p> <code>None</code> <code>include_stop_str_in_output</code> <code>Optional[bool]</code> <p>Whether to include the stop sequence in the output. Default to False.</p> <code>None</code> <code>guided_json</code> <code>Optional[Dict[str, Any]]</code> <p>If specified, the output will follow the JSON schema. For examples see https://json-schema.org/learn/miscellaneous-examples.</p> <code>None</code> <code>guided_regex</code> <code>Optional[str]</code> <p>If specified, the output will follow the regex pattern.</p> <code>None</code> <code>guided_choice</code> <code>Optional[List[str]]</code> <p>If specified, the output will be exactly one of the choices.</p> <code>None</code> <code>guided_grammar</code> <code>Optional[str]</code> <p>If specified, the output will follow the context-free grammar provided.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>Timeout in seconds. This is the maximum amount of time you are willing to wait for a response.</p> <code>COMPLETION_TIMEOUT</code> <code>stream</code> <code>bool</code> <p>Whether to stream the response. If true, the return type is an <code>Iterator[CompletionStreamResponse]</code>. Otherwise, the return type is a <code>CompletionSyncResponse</code>. When streaming, tokens will be sent as data-only server-sent events.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>response</code> <code>Union[CompletionSyncResponse, AsyncIterable[CompletionStreamResponse]]</code> <p>The generated response (if <code>stream=False</code>) or iterator of response chunks (if <code>stream=True</code>)</p> Asynchronous completion without token streaming in PythonResponse in JSON <pre><code>import asyncio\nfrom llmengine import Completion\n\nasync def main():\n    response = await Completion.acreate(\n        model=\"llama-2-7b\",\n        prompt=\"Hello, my name is\",\n        max_new_tokens=10,\n        temperature=0.2,\n    )\n    print(response.json())\n\nasyncio.run(main())\n</code></pre> <pre><code>{\n    \"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\",\n    \"output\": {\n        \"text\": \"_______ and I am a _______\",\n        \"num_completion_tokens\": 10\n    }\n}\n</code></pre> <p>Token streaming can be used to reduce perceived latency for applications. Here is how applications can use streaming:</p> Asynchronous completion with token streaming in PythonResponse in JSON <pre><code>import asyncio\nfrom llmengine import Completion\n\nasync def main():\n    stream = await Completion.acreate(\n        model=\"llama-2-7b\",\n        prompt=\"why is the sky blue?\",\n        max_new_tokens=5,\n        temperature=0.2,\n        stream=True,\n    )\n\n    async for response in stream:\n        if response.output:\n            print(response.json())\n\nasyncio.run(main())\n</code></pre> <pre><code>{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \"\\n\", \"finished\": false, \"num_completion_tokens\": 1}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \"I\", \"finished\": false, \"num_completion_tokens\": 2}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \" think\", \"finished\": false, \"num_completion_tokens\": 3}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \" the\", \"finished\": false, \"num_completion_tokens\": 4}}\n{\"request_id\": \"9cfe4d5a-f86f-4094-a935-87f871d90ec0\", \"output\": {\"text\": \" sky\", \"finished\": true, \"num_completion_tokens\": 5}}\n</code></pre>"},{"location":"api/python_client/#llmengine.Completion.batch_create","title":"batch_create  <code>classmethod</code>","text":"<pre><code>batch_create(\n    output_data_path: str,\n    model_config: CreateBatchCompletionsModelConfig,\n    content: Optional[BatchCompletionContent] = None,\n    input_data_path: Optional[str] = None,\n    data_parallelism: int = 1,\n    max_runtime_sec: int = 24 * 3600,\n    labels: Optional[Dict[str, str]] = None,\n    priority: Optional[str] = None,\n    use_v2: bool = False,\n    tool_config: Optional[ToolConfig] = None,\n    cpus: Optional[CpuSpecificationType] = None,\n    gpus: Optional[int] = None,\n    memory: Optional[StorageSpecificationType] = None,\n    gpu_type: Optional[GpuType] = None,\n    storage: Optional[StorageSpecificationType] = None,\n    request_headers: Optional[Dict[str, str]] = None,\n    **kwargs\n) -&gt; Union[\n    CreateBatchCompletionsV1Response,\n    CreateBatchCompletionsV2Response,\n]\n</code></pre> <p>Creates a batch completion for the provided input data. The job runs offline and does not depend on an existing model endpoint.</p> <p>Prompts can be passed in from an input file, or as a part of the request.</p> <p>Parameters:</p> Name Type Description Default <code>output_data_path</code> <code>str</code> <p>The path to the output file. The output file will be a JSON file containing the completions.</p> required <code>model_config</code> <code>CreateBatchCompletionsModelConfig</code> <p>The model configuration to use for the batch completion.</p> required <code>content</code> <code>Optional[CreateBatchCompletionsRequestContent]</code> <p>The content to use for the batch completion. Either one of <code>content</code> or <code>input_data_path</code> must be provided.</p> <code>None</code> <code>input_data_path</code> <code>Optional[str]</code> <p>The path to the input file. The input file should be a JSON file with data of type <code>BatchCompletionsRequestContent</code>. Either one of <code>content</code> or <code>input_data_path</code> must be provided.</p> <code>None</code> <code>data_parallelism</code> <code>int</code> <p>The number of parallel jobs to run. Data will be evenly distributed to the jobs. Defaults to 1.</p> <code>1</code> <code>priority</code> <code>str</code> <p>Priority of the batch inference job. Default to None.</p> <code>None</code> <code>max_runtime_sec</code> <code>int</code> <p>The maximum runtime of the batch completion in seconds. Defaults to 24 hours.</p> <code>24 * 3600</code> <code>use_v2</code> <code>bool</code> <p>Whether to use the v2 batch completion API. Defaults to False.</p> <code>False</code> <code>tool_config</code> <code>Optional[ToolConfig]</code> <p>Configuration for tool use. NOTE: this config is highly experimental and signature will change significantly in future iterations. Currently only Python code evaluator is supported. Python code context starts with \"```python\\n\" and ends with \"\\n&gt;&gt;&gt;\\n\", data before \"\\n```\\n\" and content end will be replaced by the Python execution results. Please format prompts accordingly and provide examples so LLMs could properly generate Python code.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>response</code> <code>CreateBatchCompletionsResponse</code> <p>The response containing the job id.</p> Batch completions with prompts in the requestBatch completions with prompts in a file and with 2 parallel jobsBatch completions with prompts and use toolV2 Batch completions with prompts in the request <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\nresponse = Completion.batch_create(\n    output_data_path=\"s3://my-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    content=CreateBatchCompletionsRequestContent(\n        prompts=[\"What is deep learning\", \"What is a neural network\"],\n        max_new_tokens=10,\n        temperature=0.0\n    )\n)\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\n\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2\n)\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent, ToolConfig\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\n\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2,\n    tool_config=ToolConfig(\n        name=\"code_evaluator\",\n    )\n)\nprint(response.json())\n</code></pre> <p>```python from llmengine import Completion from llmengine.data_types import CreateBatchCompletionsModelConfig, FilteredChatCompletionV2Request</p> <p>model_config = CreateBatchCompletionsModelConfig(     model=\"gemma-2-2b-it\",     checkpoint_path=\"s3://path-to-checkpoint\", )</p> <p>content = {     \"messages\": [         {             \"role\": \"user\",             \"content\": \"What is a good place for travel in the US?\",         },         {\"role\": \"assistant\", \"content\": \"California.\"},         {\"role\": \"user\", \"content\": \"What can I do in California?\"},     ],     \"logprobs\": True, }</p> <p>response = Completion.batch_create(     output_data_path=\"testoutput\",     model_config=model_config,     content=[FilteredChatCompletionV2Request(**content)],     use_v2=True,     labels={\"team\": \"my-team\", \"product\": \"my-product\"}, )</p> <p>print(response.json())</p>"},{"location":"api/python_client/#llmengine.FineTune","title":"FineTune","text":"<p>               Bases: <code>APIEngine</code></p> <p>FineTune API. This API is used to fine-tune models.</p> <p>Fine-tuning is a process where the LLM is further trained on a task-specific dataset, allowing the model to adjust its parameters to better align with the task at hand. Fine-tuning is a supervised training phase, where prompt/response pairs are provided to optimize the performance of the LLM. LLM Engine currently uses LoRA for fine-tuning. Support for additional fine-tuning methods is upcoming.</p> <p>LLM Engine provides APIs to create fine-tunes on a base model with training &amp; validation datasets. APIs are also provided to list, cancel and retrieve fine-tuning jobs.</p> <p>Creating a fine-tune will end with the creation of a Model, which you can view using <code>Model.get(model_name)</code> or delete using <code>Model.delete(model_name)</code>.</p>"},{"location":"api/python_client/#llmengine.FineTune.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    model: str,\n    training_file: str,\n    validation_file: Optional[str] = None,\n    hyperparameters: Optional[\n        Dict[str, Union[str, int, float]]\n    ] = None,\n    wandb_config: Optional[Dict[str, Any]] = None,\n    suffix: Optional[str] = None,\n) -&gt; CreateFineTuneResponse\n</code></pre> <p>Creates a job that fine-tunes a specified model with a given dataset.</p> <p>This API can be used to fine-tune a model. The model is the name of base model (Model Zoo for available models) to fine-tune. The training and validation files should consist of prompt and response pairs. <code>training_file</code> and <code>validation_file</code> must be either publicly accessible HTTP or HTTPS URLs, or file IDs of files uploaded to LLM Engine's Files API (these will have the <code>file-</code> prefix). The referenced files must be CSV files that include two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is currently supported. At least 200 rows of data is recommended to start to see benefits from fine-tuning. For sequences longer than the native <code>max_seq_length</code> of the model, the sequences will be truncated.</p> <p>A fine-tuning job can take roughly 30 minutes for a small dataset (~200 rows) and several hours for larger ones.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`</code> <p>The name of the base model to fine-tune. See Model Zoo for the list of available models to fine-tune.</p> required <code>training_file</code> <code>`str`</code> <p>Publicly accessible URL or file ID referencing a CSV file for training. When no validation_file is provided, one will automatically be created using a 10% split of the training_file data.</p> required <code>validation_file</code> <code>`Optional[str]`</code> <p>Publicly accessible URL or file ID referencing a CSV file for validation. The validation file is used to compute metrics which let LLM Engine pick the best fine-tuned checkpoint, which will be used for inference when fine-tuning is complete.</p> <code>None</code> <code>hyperparameters</code> <code>`Optional[Dict[str, Union[str, int, float, Dict[str, Any]]]]`</code> <p>A dict of hyperparameters to customize fine-tuning behavior.</p> <p>Currently supported hyperparameters:</p> <ul> <li><code>lr</code>: Peak learning rate used during fine-tuning. It decays with a cosine schedule afterward. (Default: 2e-3)</li> <li><code>warmup_ratio</code>: Ratio of training steps used for learning rate warmup. (Default: 0.03)</li> <li><code>epochs</code>: Number of fine-tuning epochs. This should be less than 20. (Default: 5)</li> <li><code>weight_decay</code>: Regularization penalty applied to learned weights. (Default: 0.001)</li> <li><code>peft_config</code>: A dict of parameters for the PEFT algorithm. See LoraConfig for more information.</li> </ul> <code>None</code> <code>wandb_config</code> <code>`Optional[Dict[str, Any]]`</code> <p>A dict of configuration parameters for Weights &amp; Biases. See Weights &amp; Biases for more information. Set <code>hyperparameter[\"report_to\"]</code> to <code>wandb</code> to enable automatic finetune metrics logging. Must include <code>api_key</code> field which is the wandb API key. Also supports setting <code>base_url</code> to use a custom Weights &amp; Biases server.</p> <code>None</code> <code>suffix</code> <code>`Optional[str]`</code> <p>A string that will be added to your fine-tuned model name. If present, the entire fine-tuned model name will be formatted like <code>\"[model].[suffix].[YYMMDD-HHMMSS]\"</code>. If absent, the fine-tuned model name will be formatted <code>\"[model].[YYMMDD-HHMMSS]\"</code>. For example, if <code>suffix</code> is <code>\"my-experiment\"</code>, the fine-tuned model name could be <code>\"llama-2-7b.my-experiment.230717-230150\"</code>. Note: <code>suffix</code> must be between 1 and 28 characters long, and can only contain alphanumeric characters and hyphens.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CreateFineTuneResponse</code> <code>CreateFineTuneResponse</code> <p>an object that contains the ID of the created fine-tuning job</p> <p>Here is an example script to create a 5-row CSV of properly formatted data for fine-tuning an airline question answering bot:</p> Formatting data in Python <pre><code>import csv\n\n# Define data\ndata = [\n  (\"What is your policy on carry-on luggage?\", \"Our policy allows each passenger to bring one piece of carry-on luggage and one personal item such as a purse or briefcase. The maximum size for carry-on luggage is 22 x 14 x 9 inches.\"),\n  (\"How can I change my flight?\", \"You can change your flight through our website or mobile app. Go to 'Manage my booking' section, enter your booking reference and last name, then follow the prompts to change your flight.\"),\n  (\"What meals are available on my flight?\", \"We offer a variety of meals depending on the flight's duration and route. These can range from snacks and light refreshments to full-course meals on long-haul flights. Specific meal options can be viewed during the booking process.\"),\n  (\"How early should I arrive at the airport before my flight?\", \"We recommend arriving at least two hours before domestic flights and three hours before international flights.\"),\n  \"Can I select my seat in advance?\", \"Yes, you can select your seat during the booking process or afterwards via the 'Manage my booking' section on our website or mobile app.\"),\n  ]\n\n# Write data to a CSV file\nwith open('customer_service_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"prompt\", \"response\"])\n    writer.writerows(data)\n</code></pre> <p>Currently, data needs to be uploaded to either a publicly accessible web URL or to LLM Engine's private file server so that it can be read for fine-tuning. Publicly accessible HTTP and HTTPS URLs are currently supported.</p> <p>To privately share data with the LLM Engine API, use LLM Engine's File.upload API. You can upload data in local file to LLM Engine's private file server and then use the returned file ID to reference your data in the FineTune API. The file ID is generally in the form of <code>file-&lt;random_string&gt;</code>, e.g. \"file-7DLVeLdN2Ty4M2m\".</p> <p>Example code for fine-tuning:</p> Fine-tuning in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.create(\n    model=\"llama-2-7b\",\n    training_file=\"file-7DLVeLdN2Ty4M2m\",\n)\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"fine_tune_id\": \"ft-cir3eevt71r003ks6il0\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(fine_tune_id: str) -&gt; GetFineTuneResponse\n</code></pre> <p>Get status of a fine-tuning job.</p> <p>This API can be used to get the status of an already running fine-tuning job. It takes as a single parameter the <code>fine_tune_id</code> and returns a GetFineTuneResponse object with the id and status (<code>PENDING</code>, <code>STARTED</code>, <code>UNDEFINED</code>, <code>FAILURE</code> or <code>SUCCESS</code>).</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>GetFineTuneResponse</code> <code>GetFineTuneResponse</code> <p>an object that contains the ID and status of the requested job</p> Getting status of fine-tuning in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.get(\n    fine_tune_id=\"ft-cir3eevt71r003ks6il0\",\n)\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"fine_tune_id\": \"ft-cir3eevt71r003ks6il0\",\n    \"status\": \"STARTED\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.get_events","title":"get_events  <code>classmethod</code>","text":"<pre><code>get_events(fine_tune_id: str) -&gt; GetFineTuneEventsResponse\n</code></pre> <p>Get events of a fine-tuning job.</p> <p>This API can be used to get the list of detailed events for a fine-tuning job. It takes the <code>fine_tune_id</code> as a parameter and returns a response object which has a list of events that has happened for the fine-tuning job. Two events are logged periodically: an evaluation of the training loss, and an evaluation of the eval loss. This API will return all events for the fine-tuning job.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>GetFineTuneEventsResponse</code> <code>GetFineTuneEventsResponse</code> <p>an object that contains the list of events for the fine-tuning job</p> Getting events for  fine-tuning jobs in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.get_events(fine_tune_id=\"ft-cir3eevt71r003ks6il0\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"events\":\n    [\n        {\n            \"timestamp\": 1689665099.6704428,\n            \"message\": \"{'loss': 2.108, 'learning_rate': 0.002, 'epoch': 0.7}\",\n            \"level\": \"info\"\n        },\n        {\n            \"timestamp\": 1689665100.1966307,\n            \"message\": \"{'eval_loss': 1.67730712890625, 'eval_runtime': 0.2023, 'eval_samples_per_second': 24.717, 'eval_steps_per_second': 4.943, 'epoch': 0.7}\",\n            \"level\": \"info\"\n        },\n        {\n            \"timestamp\": 1689665105.6544185,\n            \"message\": \"{'loss': 1.8961, 'learning_rate': 0.0017071067811865474, 'epoch': 1.39}\",\n            \"level\": \"info\"\n        },\n        {\n            \"timestamp\": 1689665106.159139,\n            \"message\": \"{'eval_loss': 1.513688564300537, 'eval_runtime': 0.2025, 'eval_samples_per_second': 24.696, 'eval_steps_per_second': 4.939, 'epoch': 1.39}\",\n            \"level\": \"info\"\n        }\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListFineTunesResponse\n</code></pre> <p>List fine-tuning jobs.</p> <p>This API can be used to list all the fine-tuning jobs. It returns a list of pairs of <code>fine_tune_id</code> and <code>status</code> for all existing jobs.</p> <p>Returns:</p> Name Type Description <code>ListFineTunesResponse</code> <code>ListFineTunesResponse</code> <p>an object that contains a list of all fine-tuning jobs and their statuses</p> Listing fine-tuning jobs in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.list()\nprint(response.json())\n</code></pre> <pre><code>{\n    \"jobs\": [\n        {\n            \"fine_tune_id\": \"ft-cir3eevt71r003ks6il0\",\n            \"status\": \"STARTED\"\n        },\n        {\n            \"fine_tune_id\": \"ft_def456\",\n            \"status\": \"SUCCESS\"\n        }\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.FineTune.cancel","title":"cancel  <code>classmethod</code>","text":"<pre><code>cancel(fine_tune_id: str) -&gt; CancelFineTuneResponse\n</code></pre> <p>Cancel a fine-tuning job.</p> <p>This API can be used to cancel an existing fine-tuning job if it's no longer required. It takes the <code>fine_tune_id</code> as a parameter and returns a response object which has a <code>success</code> field confirming if the cancellation was successful.</p> <p>Parameters:</p> Name Type Description Default <code>fine_tune_id</code> <code>`str`</code> <p>ID of the fine-tuning job</p> required <p>Returns:</p> Name Type Description <code>CancelFineTuneResponse</code> <code>CancelFineTuneResponse</code> <p>an object that contains whether the cancellation was successful</p> Cancelling fine-tuning job in PythonResponse in JSON <pre><code>from llmengine import FineTune\n\nresponse = FineTune.cancel(fine_tune_id=\"ft-cir3eevt71r003ks6il0\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"success\": true\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model","title":"Model","text":"<p>               Bases: <code>APIEngine</code></p> <p>Model API. This API is used to get, list, and delete models. Models include both base models built into LLM Engine, and fine-tuned models that you create through the FineTune.create() API.</p> <p>See Model Zoo for the list of publicly available base models.</p>"},{"location":"api/python_client/#llmengine.Model.create","title":"create  <code>classmethod</code>","text":"<pre><code>create(\n    name: str,\n    model: str,\n    inference_framework_image_tag: str,\n    source: LLMSource = LLMSource.HUGGING_FACE,\n    inference_framework: LLMInferenceFramework = LLMInferenceFramework.VLLM,\n    num_shards: int = 1,\n    quantize: Optional[Quantization] = None,\n    checkpoint_path: Optional[str] = None,\n    max_model_len: Optional[int] = None,\n    cpus: Optional[int] = None,\n    memory: Optional[str] = None,\n    storage: Optional[str] = None,\n    gpus: Optional[int] = None,\n    nodes_per_worker: int = 1,\n    min_workers: int = 0,\n    max_workers: int = 1,\n    per_worker: int = 2,\n    endpoint_type: ModelEndpointType = ModelEndpointType.STREAMING,\n    gpu_type: Optional[str] = None,\n    high_priority: Optional[bool] = False,\n    post_inference_hooks: Optional[\n        List[PostInferenceHooks]\n    ] = None,\n    default_callback_url: Optional[str] = None,\n    public_inference: Optional[bool] = True,\n    labels: Optional[Dict[str, str]] = None,\n    request_headers: Optional[Dict[str, str]] = None,\n    **extra_kwargs\n) -&gt; CreateLLMEndpointResponse\n</code></pre> <p>Create an LLM model. Note: This API is only available for self-hosted users.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str`</code> <p>Name of the endpoint</p> required <code>model</code> <code>`str`</code> <p>Name of the base model</p> required <code>inference_framework_image_tag</code> <code>`str`</code> <p>Image tag for the inference framework. Use \"latest\" for the most recent image</p> required <code>source</code> <code>`LLMSource`</code> <p>Source of the LLM. Currently only HuggingFace is supported</p> <code>HUGGING_FACE</code> <code>inference_framework</code> <code>`LLMInferenceFramework`</code> <p>Inference framework for the LLM. Current supported frameworks are LLMInferenceFramework.DEEPSPEED, LLMInferenceFramework.TEXT_GENERATION_INFERENCE, LLMInferenceFramework.VLLM and LLMInferenceFramework.LIGHTLLM</p> <code>VLLM</code> <code>num_shards</code> <code>`int`</code> <p>Number of shards for the LLM. When bigger than 1, LLM will be sharded to multiple GPUs. Number of GPUs must be equal or larger than num_shards.</p> <code>1</code> <code>quantize</code> <code>`Optional[Quantization]`</code> <p>Quantization method for the LLM. <code>text_generation_inference</code> supports <code>bitsandbytes</code> and <code>vllm</code> supports <code>awq</code>.</p> <code>None</code> <code>checkpoint_path</code> <code>`Optional[str]`</code> <p>Remote path to the checkpoint for the LLM. LLM engine must have permission to access the given path. Can be either a folder or a tar file. Folder is preferred since we don't need to untar and model loads faster. For model weights, safetensors are preferred but PyTorch checkpoints are also accepted (model loading will be longer).</p> <code>None</code> <code>max_model_len</code> <code>`Optional[int]`</code> <p>Model context length. If unspecified, will be automatically derived from the model config.</p> <code>None</code> <code>cpus</code> <code>`Optional[int]`</code> <p>Number of cpus each node in the worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1. Recommendation is set it to 8 * GPU count. Can be inferred from the model size.</p> <code>None</code> <code>memory</code> <code>`Optional[str]`</code> <p>Amount of memory each node in the worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory. Recommendation is set it to 24Gi * GPU count. Can be inferred from the model size.</p> <code>None</code> <code>storage</code> <code>`Optional[str]`</code> <p>Amount of local ephemeral storage each node in the worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage. Recommendataion is 40Gi for 7B models, 80Gi for 13B models and 200Gi for 70B models. Can be inferred from the model size.</p> <code>None</code> <code>gpus</code> <code>`Optional[int]`</code> <p>Number of gpus each node in the worker should get, e.g. 0, 1, etc. Can be inferred from the model size.</p> <code>None</code> <code>nodes_per_worker</code> <code>`int`</code> <p>Number of nodes per worker. Used to request multinode serving. This must be greater than or equal to 1. Controls how many nodes to dedicate to one instance of the model. Specifically, if <code>nodes_per_worker</code> is set to greater than 1, the model will be sharded across <code>nodes_per_worker</code> nodes (e.g. kubernetes pods). One of these nodes will be a \"leader\" node and receive requests. LLM Engine will set up the inter-node communication. Any compute resource requests (i.e. cpus, memory, storage) apply to each individual node, thus the total resources allocated are multiplied by this number. This is useful for models that require more memory than a single node can provide. Note: autoscaling is not supported for multinode serving. Further note: if your model can fit on GPUs on only one machine, e.g. you have access to an 8xA100 machine and your model fits on 8 A100s, it is recommended to set <code>nodes_per_worker</code> to 1 and the rest of the resources accordingly. <code>nodes_per_worker &gt; 1</code> should only be set if you require more resources than a single machine can provide.</p> <code>1</code> <code>min_workers</code> <code>`int`</code> <p>The minimum number of workers. Must be greater than or equal to 0. This should be determined by computing the minimum throughput of your workload and dividing it by the throughput of a single worker. When this number is 0, max_workers must be 1, and the endpoint will autoscale between 0 and 1 pods. When this number is greater than 0, max_workers can be any number greater or equal to min_workers.</p> <code>0</code> <code>max_workers</code> <code>`int`</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>. This should be determined by computing the maximum throughput of your workload and dividing it by the throughput of a single worker</p> <code>1</code> <code>per_worker</code> <code>`int`</code> <p>The maximum number of concurrent requests that an individual worker can service. LLM engine automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests, subject to the limits defined by <code>min_workers</code> and <code>max_workers</code> - If the average number of concurrent requests per worker is lower than <code>per_worker</code>, then the number of workers will be reduced. - Otherwise, if the average number of concurrent requests per worker is higher than <code>per_worker</code>, then the number of workers will be increased to meet the elevated traffic. Here is our recommendation for computing <code>per_worker</code>: 1. Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum throughput requirements. 2. Determine a value for the maximum number of concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing this ensures that the number of workers will \"climb\" to <code>max_workers</code>.</p> <code>2</code> <code>endpoint_type</code> <code>`ModelEndpointType`</code> <p>Currently only <code>\"streaming\"</code> endpoints are supported.</p> <code>STREAMING</code> <code>gpu_type</code> <code>`Optional[str]`</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Can be inferred from the model size. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> <li><code>nvidia-ampere-a100</code></li> <li><code>nvidia-ampere-a100e</code></li> <li><code>nvidia-hopper-h100</code></li> <li><code>nvidia-hopper-h100-1g20gb</code> # 1 slice of MIG with 1g compute and 20GB memory</li> <li><code>nvidia-hopper-h100-3g40gb</code> # 1 slice of MIG with 3g compute and 40GB memory</li> </ul> <code>None</code> <code>high_priority</code> <code>`Optional[bool]`</code> <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created endpoint to leverage the shared pool of prewarmed nodes for faster spinup time</p> <code>False</code> <code>post_inference_hooks</code> <code>`Optional[List[PostInferenceHooks]]`</code> <p>List of hooks to trigger after inference tasks are served</p> <code>None</code> <code>default_callback_url</code> <code>`Optional[str]`</code> <p>The default callback url to use for sync completion requests. This can be overridden in the task parameters for each individual task. post_inference_hooks must contain \"callback\" for the callback to be triggered</p> <code>None</code> <code>public_inference</code> <code>`Optional[bool]`</code> <p>If <code>True</code>, this endpoint will be available to all user IDs for inference</p> <code>True</code> <code>labels</code> <code>`Optional[Dict[str, str]]`</code> <p>An optional dictionary of key/value pairs to associate with this endpoint</p> <code>None</code> <p>Returns:     CreateLLMEndpointResponse: creation task ID of the created Model. Currently not used.</p> Create Llama 2 70B model with hardware specs inferred in PythonCreate Llama 2 7B model with hardware specs specified in PythonCreate Llama 2 13B model in PythonCreate Llama 2 70B model with 8bit quantization in Python <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-70b-test\"\n    model=\"llama-2-70b\",\n    inference_framework_image_tag=\"0.9.4\",\n    inference_framework=LLMInferenceFramework.TEXT_GENERATION_INFERENCE,\n    num_shards=4,\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-7b-test\"\n    model=\"llama-2-7b\",\n    inference_framework_image_tag=\"0.2.1.post1\",\n    inference_framework=LLMInferenceFramework.VLLM,\n    num_shards=1,\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    cpus=8,\n    memory=\"24Gi\",\n    storage=\"40Gi\",\n    gpus=1,\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    gpu_type=\"nvidia-ampere-a10\",\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-13b-test\"\n    model=\"llama-2-13b\",\n    inference_framework_image_tag=\"0.2.1.post1\",\n    inference_framework=LLMInferenceFramework.VLLM,\n    num_shards=2,\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    cpus=16,\n    memory=\"48Gi\",\n    storage=\"80Gi\",\n    gpus=2,\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    gpu_type=\"nvidia-ampere-a10\",\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre> <pre><code>from llmengine import Model\n\nresponse = Model.create(\n    name=\"llama-2-70b-test\"\n    model=\"llama-2-70b\",\n    inference_framework_image_tag=\"0.9.4\",\n    inference_framework=LLMInferenceFramework.TEXT_GENERATION_INFERENCE,\n    num_shards=4,\n    quantize=\"bitsandbytes\",\n    checkpoint_path=\"s3://path/to/checkpoint\",\n    cpus=40,\n    memory=\"96Gi\",\n    storage=\"200Gi\",\n    gpus=4,\n    min_workers=0,\n    max_workers=1,\n    per_worker=10,\n    endpoint_type=ModelEndpointType.STREAMING,\n    gpu_type=\"nvidia-ampere-a10\",\n    public_inference=False,\n)\n\nprint(response.json())\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(\n    model: str,\n    request_headers: Optional[Dict[str, str]] = None,\n) -&gt; GetLLMEndpointResponse\n</code></pre> <p>Get information about an LLM model.</p> <p>This API can be used to get information about a Model's source and inference framework. For self-hosted users, it returns additional information about number of shards, quantization, infra settings, etc. The function takes as a single parameter the name <code>model</code> and returns a GetLLMEndpointResponse object.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>`str`</code> <p>Name of the model</p> required <p>Returns:</p> Name Type Description <code>GetLLMEndpointResponse</code> <code>GetLLMEndpointResponse</code> <p>object representing the LLM and configurations</p> Accessing model in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.get(\"llama-2-7b.suffix.2023-07-18-12-00-00\")\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": null,\n    \"name\": \"llama-2-7b.suffix.2023-07-18-12-00-00\",\n    \"model_name\": null,\n    \"source\": \"hugging_face\",\n    \"status\": \"READY\",\n    \"inference_framework\": \"text_generation_inference\",\n    \"inference_framework_tag\": null,\n    \"num_shards\": null,\n    \"quantize\": null,\n    \"spec\": null\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.list","title":"list  <code>classmethod</code>","text":"<pre><code>list(\n    request_headers: Optional[Dict[str, str]] = None\n) -&gt; ListLLMEndpointsResponse\n</code></pre> <p>List LLM models available to call inference on.</p> <p>This API can be used to list all available models, including both publicly available models and user-created fine-tuned models. It returns a list of GetLLMEndpointResponse objects for all models. The most important field is the model <code>name</code>.</p> <p>Returns:</p> Name Type Description <code>ListLLMEndpointsResponse</code> <code>ListLLMEndpointsResponse</code> <p>list of models</p> Listing available modes in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.list()\nprint(response.json())\n</code></pre> <pre><code>{\n    \"model_endpoints\": [\n        {\n            \"id\": null,\n            \"name\": \"llama-2-7b.suffix.2023-07-18-12-00-00\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"text_generation_inference\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n        },\n        {\n            \"id\": null,\n            \"name\": \"llama-2-7b\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"text_generation_inference\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n        },\n        {\n            \"id\": null,\n            \"name\": \"llama-13b-deepspeed-sync\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"deepspeed\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n        },\n        {\n            \"id\": null,\n            \"name\": \"falcon-40b\",\n            \"model_name\": null,\n            \"source\": \"hugging_face\",\n            \"inference_framework\": \"text_generation_inference\",\n            \"inference_framework_tag\": null,\n            \"num_shards\": null,\n            \"quantize\": null,\n            \"spec\": null\n        }\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.update","title":"update  <code>classmethod</code>","text":"<pre><code>update(\n    name: str,\n    model: Optional[str] = None,\n    inference_framework_image_tag: Optional[str] = None,\n    source: Optional[LLMSource] = None,\n    num_shards: Optional[int] = None,\n    quantize: Optional[Quantization] = None,\n    checkpoint_path: Optional[str] = None,\n    cpus: Optional[int] = None,\n    memory: Optional[str] = None,\n    storage: Optional[str] = None,\n    gpus: Optional[int] = None,\n    min_workers: Optional[int] = None,\n    max_workers: Optional[int] = None,\n    per_worker: Optional[int] = None,\n    endpoint_type: Optional[ModelEndpointType] = None,\n    gpu_type: Optional[str] = None,\n    high_priority: Optional[bool] = None,\n    post_inference_hooks: Optional[\n        List[PostInferenceHooks]\n    ] = None,\n    default_callback_url: Optional[str] = None,\n    public_inference: Optional[bool] = None,\n    labels: Optional[Dict[str, str]] = None,\n    request_headers: Optional[Dict[str, str]] = None,\n    **extra_kwargs\n) -&gt; UpdateLLMEndpointResponse\n</code></pre> <p>Update an LLM model. Note: This API is only available for self-hosted users.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>`str`</code> <p>Name of the endpoint</p> required <code>model</code> <code>`Optional[str]`</code> <p>Name of the base model</p> <code>None</code> <code>inference_framework_image_tag</code> <code>`Optional[str]`</code> <p>Image tag for the inference framework. Use \"latest\" for the most recent image</p> <code>None</code> <code>source</code> <code>`Optional[LLMSource]`</code> <p>Source of the LLM. Currently only HuggingFace is supported</p> <code>None</code> <code>num_shards</code> <code>`Optional[int]`</code> <p>Number of shards for the LLM. When bigger than 1, LLM will be sharded to multiple GPUs. Number of GPUs must be equal or larger than num_shards.</p> <code>None</code> <code>quantize</code> <code>`Optional[Quantization]`</code> <p>Quantization method for the LLM. <code>text_generation_inference</code> supports <code>bitsandbytes</code> and <code>vllm</code> supports <code>awq</code>.</p> <code>None</code> <code>checkpoint_path</code> <code>`Optional[str]`</code> <p>Remote path to the checkpoint for the LLM. LLM engine must have permission to access the given path. Can be either a folder or a tar file. Folder is preferred since we don't need to untar and model loads faster. For model weights, safetensors are preferred but PyTorch checkpoints are also accepted (model loading will be longer).</p> <code>None</code> <code>cpus</code> <code>`Optional[int]`</code> <p>Number of cpus each node in the worker should get, e.g. 1, 2, etc. This must be greater than or equal to 1. Recommendation is set it to 8 * GPU count.</p> <code>None</code> <code>memory</code> <code>`Optional[str]`</code> <p>Amount of memory each node in the worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of memory. Recommendation is set it to 24Gi * GPU count.</p> <code>None</code> <code>storage</code> <code>`Optional[str]`</code> <p>Amount of local ephemeral storage each node in the worker should get, e.g. \"4Gi\", \"512Mi\", etc. This must be a positive amount of storage. Recommendataion is 40Gi for 7B models, 80Gi for 13B models and 200Gi for 70B models.</p> <code>None</code> <code>gpus</code> <code>`Optional[int]`</code> <p>Number of gpus each node in the worker should get, e.g. 0, 1, etc.</p> <code>None</code> <code>min_workers</code> <code>`Optional[int]`</code> <p>The minimum number of workers. Must be greater than or equal to 0. This should be determined by computing the minimum throughput of your workload and dividing it by the throughput of a single worker. When this number is 0, max_workers must be 1, and the endpoint will autoscale between 0 and 1 pods. When this number is greater than 0, max_workers can be any number greater or equal to min_workers.</p> <code>None</code> <code>max_workers</code> <code>`Optional[int]`</code> <p>The maximum number of workers. Must be greater than or equal to 0, and as well as greater than or equal to <code>min_workers</code>. This should be determined by computing the maximum throughput of your workload and dividing it by the throughput of a single worker</p> <code>None</code> <code>per_worker</code> <code>`Optional[int]`</code> <p>The maximum number of concurrent requests that an individual worker can service. LLM engine automatically scales the number of workers for the endpoint so that each worker is processing <code>per_worker</code> requests, subject to the limits defined by <code>min_workers</code> and <code>max_workers</code> - If the average number of concurrent requests per worker is lower than <code>per_worker</code>, then the number of workers will be reduced. - Otherwise, if the average number of concurrent requests per worker is higher than <code>per_worker</code>, then the number of workers will be increased to meet the elevated traffic. Here is our recommendation for computing <code>per_worker</code>: 1. Compute <code>min_workers</code> and <code>max_workers</code> per your minimum and maximum throughput requirements. 2. Determine a value for the maximum number of concurrent requests in the workload. Divide this number by <code>max_workers</code>. Doing this ensures that the number of workers will \"climb\" to <code>max_workers</code>.</p> <code>None</code> <code>endpoint_type</code> <code>`Optional[ModelEndpointType]`</code> <p>Currently only <code>\"streaming\"</code> endpoints are supported.</p> <code>None</code> <code>gpu_type</code> <code>`Optional[str]`</code> <p>If specifying a non-zero number of gpus, this controls the type of gpu requested. Here are the supported values:</p> <ul> <li><code>nvidia-tesla-t4</code></li> <li><code>nvidia-ampere-a10</code></li> <li><code>nvidia-ampere-a100</code></li> <li><code>nvidia-ampere-a100e</code></li> <li><code>nvidia-hopper-h100</code></li> <li><code>nvidia-hopper-h100-1g20gb</code></li> <li><code>nvidia-hopper-h100-3g40gb</code></li> </ul> <code>None</code> <code>high_priority</code> <code>`Optional[bool]`</code> <p>Either <code>True</code> or <code>False</code>. Enabling this will allow the created endpoint to leverage the shared pool of prewarmed nodes for faster spinup time</p> <code>None</code> <code>post_inference_hooks</code> <code>`Optional[List[PostInferenceHooks]]`</code> <p>List of hooks to trigger after inference tasks are served</p> <code>None</code> <code>default_callback_url</code> <code>`Optional[str]`</code> <p>The default callback url to use for sync completion requests. This can be overridden in the task parameters for each individual task. post_inference_hooks must contain \"callback\" for the callback to be triggered</p> <code>None</code> <code>public_inference</code> <code>`Optional[bool]`</code> <p>If <code>True</code>, this endpoint will be available to all user IDs for inference</p> <code>None</code> <code>labels</code> <code>`Optional[Dict[str, str]]`</code> <p>An optional dictionary of key/value pairs to associate with this endpoint</p> <code>None</code> <p>Returns:     UpdateLLMEndpointResponse: creation task ID of the updated Model. Currently not used.</p>"},{"location":"api/python_client/#llmengine.Model.delete","title":"delete  <code>classmethod</code>","text":"<pre><code>delete(\n    model_endpoint_name: str,\n    request_headers: Optional[Dict[str, str]] = None,\n) -&gt; DeleteLLMEndpointResponse\n</code></pre> <p>Deletes an LLM model.</p> <p>This API can be used to delete a fine-tuned model. It takes as parameter the name of the <code>model</code> and returns a response object which has a <code>deleted</code> field confirming if the deletion was successful. If called on a base model included with LLM Engine, an error will be thrown.</p> <p>Parameters:</p> Name Type Description Default <code>model_endpoint_name</code> <code>`str`</code> <p>Name of the model endpoint to be deleted</p> required <p>Returns:</p> Name Type Description <code>response</code> <code>DeleteLLMEndpointResponse</code> <p>whether the model endpoint was successfully deleted</p> Deleting model in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.delete(\"llama-2-7b.suffix.2023-07-18-12-00-00\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"deleted\": true\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.Model.download","title":"download  <code>classmethod</code>","text":"<pre><code>download(\n    model_name: str, download_format: str = \"hugging_face\"\n) -&gt; ModelDownloadResponse\n</code></pre> <p>Download a fine-tuned model.</p> <p>This API can be used to download the resulting model from a fine-tuning job. It takes the <code>model_name</code> and <code>download_format</code> as parameter and returns a response object which contains a dictonary of filename, url pairs associated with the fine-tuned model. The user can then download these urls to obtain the fine-tuned model. If called on a nonexistent model, an error will be thrown.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>`str`</code> <p>name of the fine-tuned model</p> required <code>download_format</code> <code>`str`</code> <p>download format requested (default=hugging_face)</p> <code>'hugging_face'</code> <p>Returns:     DownloadModelResponse: an object that contains a dictionary of filenames, urls from which to download the model weights.     The urls are presigned urls that grant temporary access and expire after an hour.</p> Downloading model in PythonResponse in JSON <pre><code>from llmengine import Model\n\nresponse = Model.download(\"llama-2-7b.suffix.2023-07-18-12-00-00\", download_format=\"hugging_face\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"urls\": {\"my_model_file\": \"https://url-to-my-model-weights\"}\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File","title":"File","text":"<p>               Bases: <code>APIEngine</code></p> <p>File API. This API is used to upload private files to LLM engine so that fine-tunes can access them for training and validation data.</p> <p>Functions are provided to upload, get, list, and delete files, as well as to get the contents of a file.</p>"},{"location":"api/python_client/#llmengine.File.upload","title":"upload  <code>classmethod</code>","text":"<pre><code>upload(file: BufferedReader) -&gt; UploadFileResponse\n</code></pre> <p>Uploads a file to LLM engine.</p> <p>For use in FineTune creation, this should be a CSV file with two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is currently supported.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>`BufferedReader`</code> <p>A local file opened with <code>open(file_path, \"r\")</code></p> required <p>Returns:</p> Name Type Description <code>UploadFileResponse</code> <code>UploadFileResponse</code> <p>an object that contains the ID of the uploaded file</p> Uploading file in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.upload(open(\"training_dataset.csv\", \"r\"))\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": \"file-abc123\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.get","title":"get  <code>classmethod</code>","text":"<pre><code>get(file_id: str) -&gt; GetFileResponse\n</code></pre> <p>Get file metadata, including filename and size.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>`str`</code> <p>ID of the file</p> required <p>Returns:</p> Name Type Description <code>GetFileResponse</code> <code>GetFileResponse</code> <p>an object that contains the ID, filename, and size of the requested file</p> Getting metadata about file in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.get(\n    file_id=\"file-abc123\",\n)\n\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": \"file-abc123\",\n    \"filename\": \"training_dataset.csv\",\n    \"size\": 100\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.download","title":"download  <code>classmethod</code>","text":"<pre><code>download(file_id: str) -&gt; GetFileContentResponse\n</code></pre> <p>Get contents of a file, as a string. (If the uploaded file is in binary, a string encoding will be returned.)</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>`str`</code> <p>ID of the file</p> required <p>Returns:</p> Name Type Description <code>GetFileContentResponse</code> <code>GetFileContentResponse</code> <p>an object that contains the ID and content of the file</p> Getting file content in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.download(file_id=\"file-abc123\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"id\": \"file-abc123\",\n    \"content\": \"Hello world!\"\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.list","title":"list  <code>classmethod</code>","text":"<pre><code>list() -&gt; ListFilesResponse\n</code></pre> <p>List metadata about all files, e.g. their filenames and sizes.</p> <p>Returns:</p> Name Type Description <code>ListFilesResponse</code> <code>ListFilesResponse</code> <p>an object that contains a list of all files and their filenames and sizes</p> Listing files in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.list()\nprint(response.json())\n</code></pre> <pre><code>{\n    \"files\": [\n        {\n            \"id\": \"file-abc123\",\n            \"filename\": \"training_dataset.csv\",\n            \"size\": 100\n        },\n        {\n            \"id\": \"file-def456\",\n            \"filename\": \"validation_dataset.csv\",\n            \"size\": 50\n        }\n    ]\n}\n</code></pre>"},{"location":"api/python_client/#llmengine.File.delete","title":"delete  <code>classmethod</code>","text":"<pre><code>delete(file_id: str) -&gt; DeleteFileResponse\n</code></pre> <p>Deletes a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_id</code> <code>`str`</code> <p>ID of the file</p> required <p>Returns:</p> Name Type Description <code>DeleteFileResponse</code> <code>DeleteFileResponse</code> <p>an object that contains whether the deletion was successful</p> Deleting file in PythonResponse in JSON <pre><code>from llmengine import File\n\nresponse = File.delete(file_id=\"file-abc123\")\nprint(response.json())\n</code></pre> <pre><code>{\n    \"deleted\": true\n}\n</code></pre>"},{"location":"guides/completions/","title":"Completions","text":"<p>Language Models are trained to predict natural language and provide text outputs as a response to their inputs. The inputs are called prompts and outputs are referred to as completions. LLMs take the input prompts and chunk them into smaller units called tokens to process and generate language. Tokens may include trailing spaces and even sub-words. This process is language dependent.</p> <p>Scale's LLM Engine provides access to open source language models (see Model Zoo) that can be used for producing completions to prompts.</p>"},{"location":"guides/completions/#completion-api-call","title":"Completion API call","text":"<p>An example API call looks as follows:</p> Completion call in Python <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n)\n\nprint(response.json())\n# '{\"request_id\": \"c4bf0732-08e0-48a8-8b44-dfe8d4702fb0\", \"output\": {\"text\": \"________ and I am a ________\", \"num_completion_tokens\": 10}}'\n\nprint(response.output.text)\n# ________ and I am a ________\n</code></pre> <ul> <li>model: The LLM you want to use (see Model Zoo).</li> <li>prompt: The main input for the LLM to respond to.</li> <li>max_new_tokens: The maximum number of tokens to generate in the chat completion.</li> <li>temperature: The sampling temperature to use. Higher values make the output more random,   while lower values will make it more focused and deterministic.   When temperature is 0 greedy search is used.</li> </ul> <p>See the full Completion API reference documentation to learn more.</p>"},{"location":"guides/completions/#completion-api-response","title":"Completion API response","text":"<p>An example Completion API response looks as follows:</p> Response in JSONResponse in Python <pre><code>    &gt;&gt;&gt; print(response.json())\n    {\n      \"request_id\": \"c4bf0732-08e0-48a8-8b44-dfe8d4702fb0\",\n      \"output\": {\n        \"text\": \"_______ and I am a _______\",\n        \"num_completion_tokens\": 10\n      }\n    }\n</code></pre> <pre><code>    &gt;&gt;&gt; print(response.output.text)\n    _______ and I am a _______\n</code></pre>"},{"location":"guides/completions/#token-streaming","title":"Token streaming","text":"<p>The Completions API supports token streaming to reduce perceived latency for certain applications. When streaming, tokens will be sent as data-only server-side events.</p> <p>To enable token streaming, pass <code>stream=True</code> to either Completion.create or Completion.acreate.</p>"},{"location":"guides/completions/#streaming-error-handling","title":"Streaming Error Handling","text":"<p>Note: Error handling semantics are mixed for streaming calls: - Errors that arise before streaming begins are returned back to the user as <code>HTTP</code> errors with the appropriate status code. - Errors that arise after streaming begins within a <code>HTTP 200</code> response are returned back to the user as plain-text messages and currently need to be handled by the client. </p> <p>An example of token streaming using the synchronous Completions API looks as follows:</p> Token streaming with synchronous API in python <pre><code>import sys\n\nfrom llmengine import Completion\n\n# errors occurring before streaming begins will be thrown here\nstream = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Give me a 200 word summary on the current economic events in the US.\",\n    max_new_tokens=1000,\n    temperature=0.2,\n    stream=True,\n)\n\nfor response in stream:\n    if response.output:\n        print(response.output.text, end=\"\")\n        sys.stdout.flush()\n    else: # an error occurred after streaming began\n        print(response.error) # print the error message out \n        break\n</code></pre>"},{"location":"guides/completions/#async-requests","title":"Async requests","text":"<p>The Python client supports <code>asyncio</code> for creating Completions. Use Completion.acreate instead of Completion.create to utilize async processing. The function signatures are otherwise identical.</p> <p>An example of async Completions looks as follows:</p> Completions with asynchronous API in python <pre><code>import asyncio\nfrom llmengine import Completion\n\nasync def main():\n    response = await Completion.acreate(\n        model=\"llama-2-7b\",\n        prompt=\"Hello, my name is\",\n        max_new_tokens=10,\n        temperature=0.2,\n    )\n    print(response.json())\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/completions/#batch-completions","title":"Batch completions","text":"<p>The Python client also supports batch completions. Batch completions supports distributing data to multiple workers to accelerate inference. It also tries to maximize throughput so the completions should finish quite a bit faster than hitting models through HTTP. Use Completion.batch_create to utilize batch completions.</p> <p>Some examples of batch completions:</p> Batch completions with prompts in the request <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\ncontent = CreateBatchCompletionsRequestContent(\n    prompts=[\"What is deep learning\", \"What is a neural network\"],\n    max_new_tokens=10,\n    temperature=0.0\n)\n\nresponse = Completion.batch_create(\n    output_data_path=\"s3://my-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    content=content\n)\nprint(response.job_id)\n</code></pre> Batch completions with prompts in a file and with 2 parallel jobs <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\n\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2\n)\nprint(response.job_id)\n</code></pre> Batch completions with prompts and use tool <p>For how to properly use the tool please see Completion.batch_create tool_config doc. <pre><code>from llmengine import Completion\nfrom llmengine.data_types import CreateBatchCompletionsModelConfig, CreateBatchCompletionsRequestContent, ToolConfig\n\n# Store CreateBatchCompletionsRequestContent data into input file \"s3://my-input-path\"\n\nresponse = Completion.batch_create(\n    input_data_path=\"s3://my-input-path\",\n    output_data_path=\"s3://my-output-path\",\n    model_config=CreateBatchCompletionsModelConfig(\n        model=\"llama-2-7b\",\n        checkpoint_path=\"s3://checkpoint-path\",\n        labels={\"team\":\"my-team\", \"product\":\"my-product\"}\n    ),\n    data_parallelism=2,\n    tool_config=ToolConfig(\n        name=\"code_evaluator\",\n    )\n)\nprint(response.json())\n</code></pre></p>"},{"location":"guides/completions/#guided-decoding","title":"Guided decoding","text":"<p>Guided decoding is supported by vLLM and backed by Outlines. It enforces certain token generation patterns by tinkering with the sampling logits.</p> Guided decoding with regex <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_regex=\"Sean.*\",\n)\n\nprint(response.json())\n# {\"request_id\":\"c19f0fae-317e-4f69-8e06-c04189299b9c\",\"output\":{\"text\":\"Sean. I'm a 2\",\"num_prompt_tokens\":6,\"num_completion_tokens\":10,\"tokens\":null}}\n</code></pre> Guided decoding with choice <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_choice=[\"Sean\", \"Brian\", \"Tim\"],\n)\n\nprint(response.json())\n# {\"request_id\":\"641e2af3-a3e3-4493-98b9-d38115ba0d22\",\"output\":{\"text\":\"Sean\",\"num_prompt_tokens\":6,\"num_completion_tokens\":4,\"tokens\":null}}\n</code></pre> Guided decoding with JSON schema <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_json={\"properties\":{\"myString\":{\"type\":\"string\"}},\"required\":[\"myString\"]},\n)\n\nprint(response.json())\n# {\"request_id\":\"5b184654-96b6-4932-9eb6-382a51fdb3d5\",\"output\":{\"text\":\"{\\\"myString\\\" : \\\"John Doe\",\"num_prompt_tokens\":6,\"num_completion_tokens\":10,\"tokens\":null}}\n</code></pre> Guided decoding with Context-Free Grammar <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b\",\n    prompt=\"Hello, my name is\",\n    max_new_tokens=10,\n    temperature=0.2,\n    guided_grammar=\"start: \\\"John\\\"\"\n)\n\nprint(response.json())\n# {\"request_id\": \"34621b44-c655-402c-a459-f108b3e49b12\", \"output\": {\"text\": \"John\", \"num_prompt_tokens\": 6, \"num_completion_tokens\": 4, \"tokens\": None}}\n</code></pre>"},{"location":"guides/completions/#which-model-should-i-use","title":"Which model should I use?","text":"<p>See the Model Zoo for more information on best practices for which model to use for Completions.</p>"},{"location":"guides/endpoint_creation/","title":"Endpoint creation","text":"<p>When creating a model endpoint, you can periodically poll the model status field to track the status of your model endpoint. In general, you'll need to wait after the  model creation step for the model endpoint to be ready and available for use. An example is provided below: </p> <pre><code>model_name = \"test_deploy\"\nmodel = Model.create(name=model_name, model=\"llama-2-7b\", inference_frame_image_tag=\"0.9.4\")\nresponse = Model.get(model_name)\nwhile response.status.name != \"READY\":\n    print(response.status.name)\n    time.sleep(60)\n    response = Model.get(model_name)\n</code></pre> <p>Once the endpoint status is ready, you can use your newly created model for inference.</p>"},{"location":"guides/fine_tuning/","title":"Fine-tuning","text":"<p>Learn how to customize your models on your data with fine-tuning. Or get started right away with our fine-tuning cookbook.</p>"},{"location":"guides/fine_tuning/#introduction","title":"Introduction","text":"<p>Fine-tuning helps improve model performance by training on specific examples of prompts and desired responses. LLMs are initially trained on data collected from the entire internet. With fine-tuning, LLMs can be optimized to perform better in a specific domain by learning from examples for that domain. Smaller LLMs that have been fine-tuned on a specific use case often outperform larger ones that were trained more generally.</p> <p>Fine-tuning allows for:</p> <ol> <li>Higher quality results than prompt engineering alone</li> <li>Cost savings through shorter prompts</li> <li>The ability to reach equivalent accuracy with a smaller model</li> <li>Lower latency at inference time</li> <li>The chance to show an LLM more examples than can fit in a single context window</li> </ol> <p>LLM Engine's fine-tuning API lets you fine-tune various open source LLMs on your own data and then make inference calls to the resulting LLM. For more specific details, see the fine-tuning API reference.</p>"},{"location":"guides/fine_tuning/#producing-high-quality-data-for-fine-tuning","title":"Producing high quality data for fine-tuning","text":"<p>The training data for fine-tuning should consist of prompt and response pairs.</p> <p>As a rule of thumb, you should expect to see linear improvements in your fine-tuned model's quality with each doubling of the dataset size. Having high-quality data is also essential to improving performance. For every linear increase in the error rate in your training data, you may encounter a roughly quadratic increase in your fine-tuned model's error rate.</p> <p>High quality data is critical to achieve improved model performance, and in several cases will require experts to generate and prepare data - the breadth and diversity of the data is highly critical. Scale's Data Engine can help prepare such high quality, diverse data sets - more information here.</p>"},{"location":"guides/fine_tuning/#preparing-data","title":"Preparing data","text":"<p>Your data must be formatted as a CSV file that includes two columns: <code>prompt</code> and <code>response</code>. A maximum of 100,000 rows of data is currently supported. At least 200 rows of data is recommended to start to see benefits from fine-tuning. LLM Engine supports fine-tuning with a training and validation dataset. If only a training dataset is provided, 10% of the data is randomly split to be used as validation.</p> <p>Here is an example script to create a 50-row CSV of properly formatted data for fine-tuning an airline question answering bot</p> Creating a sample dataset <pre><code>import csv\n\n# Define data\ndata = [\n    (\"What is your policy on carry-on luggage?\", \"Our policy allows each passenger to bring one piece of carry-on luggage and one personal item such as a purse or briefcase. The maximum size for carry-on luggage is 22 x 14 x 9 inches.\"),\n    (\"How can I change my flight?\", \"You can change your flight through our website or mobile app. Go to 'Manage my booking' section, enter your booking reference and last name, then follow the prompts to change your flight.\"),\n    (\"What meals are available on my flight?\", \"We offer a variety of meals depending on the flight's duration and route. These can range from snacks and light refreshments to full-course meals on long-haul flights. Specific meal options can be viewed during the booking process.\"),\n    (\"How early should I arrive at the airport before my flight?\", \"We recommend arriving at least two hours before domestic flights and three hours before international flights.\"),\n    (\"Can I select my seat in advance?\", \"Yes, you can select your seat during the booking process or afterwards via the 'Manage my booking' section on our website or mobile app.\"),\n    (\"What should I do if my luggage is lost?\", \"If your luggage is lost, please report this immediately at our 'Lost and Found' counter at the airport. We will assist you in tracking your luggage.\"),\n    (\"Do you offer special assistance for passengers with disabilities?\", \"Yes, we offer special assistance for passengers with disabilities. Please notify us of your needs at least 48 hours prior to your flight.\"),\n    (\"Can I bring my pet on the flight?\", \"Yes, we allow small pets in the cabin, and larger pets in the cargo hold. Please check our pet policy for more details.\"),\n    (\"What is your policy on flight cancellations?\", \"In case of flight cancellations, we aim to notify passengers as early as possible and offer either a refund or a rebooking on the next available flight.\"),\n    (\"Can I get a refund if I cancel my flight?\", \"Refunds depend on the type of ticket purchased. Please check our cancellation policy for details. Non-refundable tickets, however, are typically not eligible for refunds unless due to extraordinary circumstances.\"),\n    (\"How can I check-in for my flight?\", \"You can check-in for your flight either online, through our mobile app, or at the airport. Online and mobile app check-in opens 24 hours before departure and closes 90 minutes before.\"),\n    (\"Do you offer free meals on your flights?\", \"Yes, we serve free meals on all long-haul flights. For short-haul flights, we offer a complimentary drink and snack. Special meal requests should be made at least 48 hours before departure.\"),\n    (\"Can I use my electronic devices during the flight?\", \"Small electronic devices can be used throughout the flight in flight mode. Larger devices like laptops may be used above 10,000 feet.\"),\n    (\"How much baggage can I check-in?\", \"The checked baggage allowance depends on the class of travel and route. The details would be mentioned on your ticket, or you can check on our website.\"),\n    (\"How can I request for a wheelchair?\", \"To request a wheelchair or any other special assistance, please call our customer service at least 48 hours before your flight.\"),\n    (\"Do I get a discount for group bookings?\", \"Yes, we offer discounts on group bookings of 10 or more passengers. Please contact our group bookings team for more information.\"),\n    (\"Do you offer Wi-fi on your flights?\", \"Yes, we offer complimentary Wi-fi on select flights. You can check the availability during the booking process.\"),\n    (\"What is the minimum connecting time between flights?\", \"The minimum connecting time varies depending on the airport and whether your flight is international or domestic. Generally, it's recommended to allow at least 45-60 minutes for domestic connections and 60-120 minutes for international.\"),\n    (\"Do you offer duty-free shopping on international flights?\", \"Yes, we have a selection of duty-free items that you can pre-order on our website or purchase onboard on international flights.\"),\n    (\"Can I upgrade my ticket to business class?\", \"Yes, you can upgrade your ticket through the 'Manage my booking' section on our website or by contacting our customer service. The availability and costs depend on the specific flight.\"),\n    (\"Can unaccompanied minors travel on your flights?\", \"Yes, we do accommodate unaccompanied minors on our flights, with special services to ensure their safety and comfort. Please contact our customer service for more details.\"),\n    (\"What amenities do you provide in business class?\", \"In business class, you will enjoy additional legroom, reclining seats, premium meals, priority boarding and disembarkation, access to our business lounge, extra baggage allowance, and personalized service.\"),\n    (\"How much does extra baggage cost?\", \"Extra baggage costs vary based on flight route and the weight of the baggage. Please refer to our 'Extra Baggage' section on the website for specific rates.\"),\n    (\"Are there any specific rules for carrying liquids in carry-on?\", \"Yes, liquids carried in your hand luggage must be in containers of 100 ml or less and they should all fit into a single, transparent, resealable plastic bag of 20 cm x 20 cm.\"),\n    (\"What if I have a medical condition that requires special assistance during the flight?\", \"We aim to make the flight comfortable for all passengers. If you have a medical condition that may require special assistance, please contact our \u2018special services\u2019 team 48 hours before your flight.\"),\n    (\"What in-flight entertainment options are available?\", \"We offer a range of in-flight entertainment options including a selection of movies, TV shows, music, and games, available on your personal seat-back screen.\"),\n    (\"What types of payment methods do you accept?\", \"We accept credit/debit cards, PayPal, bank transfers, and various other forms of payment. The available options may vary depending on the country of departure.\"),\n    (\"How can I earn and redeem frequent flyer miles?\", \"You can earn miles for every journey you take with us or our partner airlines. These miles can be redeemed for flight tickets, upgrades, or various other benefits. To earn and redeem miles, you need to join our frequent flyer program.\"),\n    (\"Can I bring a stroller for my baby?\", \"Yes, you can bring a stroller for your baby. It can be checked in for free and will normally be given back to you at the aircraft door upon arrival.\"),\n    (\"What age does my child have to be to qualify as an unaccompanied minor?\", \"Children aged between 5 and 12 years who are traveling alone are considered unaccompanied minors. Our team provides special care for these children from departure to arrival.\"),\n    (\"What documents do I need to travel internationally?\", \"For international travel, you need a valid passport and may also require visas, depending on your destination and your country of residence. It's important to check the specific requirements before you travel.\"),\n    (\"What happens if I miss my flight?\", \"If you miss your flight, please contact our customer service immediately. Depending on the circumstances, you may be able to rebook on a later flight, but additional fees may apply.\"),\n    (\"Can I travel with my musical instrument?\", \"Yes, small musical instruments can be brought on board as your one carry-on item. Larger instruments must be transported in the cargo, or if small enough, a seat may be purchased for them.\"),\n    (\"Do you offer discounts for children or infants?\", \"Yes, children aged 2-11 traveling with an adult usually receive a discount on the fare. Infants under the age of 2 who do not occupy a seat can travel for a reduced fare or sometimes for free.\"),\n    (\"Is smoking allowed on your flights?\", \"No, all our flights are non-smoking for the comfort and safety of all passengers.\"),\n    (\"Do you have family seating?\", \"Yes, we offer the option to seat families together. You can select seats during booking or afterwards through the 'Manage my booking' section on the website.\"),\n    (\"Is there any discount for senior citizens?\", \"Some flights may offer a discount for senior citizens. Please check our website or contact customer service for accurate information.\"),\n    (\"What items are prohibited on your flights?\", \"Prohibited items include, but are not limited to, sharp objects, firearms, explosive materials, and certain chemicals. You can find a comprehensive list on our website under the 'Security Regulations' section.\"),\n    (\"Can I purchase a ticket for someone else?\", \"Yes, you can purchase a ticket for someone else. You'll need their correct name as it appears on their government-issued ID, and their correct travel dates.\"),\n    (\"What is the process for lost and found items on the plane?\", \"If you realize you forgot an item on the plane, report it as soon as possible to our lost and found counter. We will make every effort to locate and return your item.\"),\n    (\"Can I request a special meal?\", \"Yes, we offer a variety of special meals to accommodate dietary restrictions. Please request your preferred meal at least 48 hours prior to your flight.\"),\n    (\"Is there a weight limit for checked baggage?\", \"Yes, luggage weight limits depend on your ticket class and route. You can find the details on your ticket or by visiting our website.\"),\n    (\"Can I bring my sports equipment?\", \"Yes, certain types of sports equipment can be carried either as or in addition to your permitted baggage. Some equipment may require additional fees. It's best to check our policy on our website or contact us directly.\"),\n    (\"Do I need a visa to travel to certain countries?\", \"Yes, visa requirements depend on the country you are visiting and your nationality. We advise checking with the relevant embassy or consulate prior to travel.\"),\n    (\"How can I add extra baggage to my booking?\", \"You can add extra baggage to your booking through the 'Manage my booking' section on our website or by contacting our customer services.\"),\n    (\"Can I check-in at the airport?\", \"Yes, you can choose to check-in at the airport. However, we also offer online and mobile check-in, which may save you time.\"),\n    (\"How do I know if my flight is delayed or cancelled?\", \"In case of any changes to your flight, we will attempt to notify all passengers using the contact information given at the time of booking. You can also check your flight status on our website.\"),\n    (\"What is your policy on pregnant passengers?\", \"Pregnant passengers can travel up to the end of the 36th week for single pregnancies, and the end of the 32nd week for multiple pregnancies. We recommend consulting your doctor before any air travel.\"),\n    (\"Can children travel alone?\", \"Yes, children age 5 to 12 can travel alone as unaccompanied minors. We provide special care for these seats. Please contact our customer service for more information.\"),\n    (\"How can I pay for my booking?\", \"You can pay for your booking using a variety of methods including credit and debit cards, PayPal, or bank transfers. The options may vary depending on the country of departure.\"),\n]\n\n# Write data to a CSV file\nwith open('customer_service_data.csv', 'w', newline='') as file:\n    writer = csv.writer(file)\n    writer.writerow([\"prompt\", \"response\"])\n    writer.writerows(data)\n</code></pre>"},{"location":"guides/fine_tuning/#making-your-data-accessible-to-llm-engine","title":"Making your data accessible to LLM Engine","text":"<p>Currently, data needs to be uploaded to either a publicly accessible web URL or to LLM Engine's private file server so that it can be read for fine-tuning. Publicly accessible HTTP and HTTPS URLs are currently supported.</p> <p>To privately share data with the LLM Engine API, use LLM Engine's File.upload API. You can upload data in local file to LLM Engine's private file server and then use the returned file ID to reference your data in the FineTune API. The file ID is generally in the form of <code>file-&lt;random_string&gt;</code>, e.g. \"file-7DLVeLdN2Ty4M2m\".</p> Upload to LLM Engine's private file server <pre><code>from llmengine import File\n\nresponse = File.upload(open(\"customer_service_data.csv\", \"r\"))\nprint(response.json())\n</code></pre>"},{"location":"guides/fine_tuning/#launching-the-fine-tune","title":"Launching the fine-tune","text":"<p>Once you have uploaded your data, you can use the LLM Engine's FineTune.Create API to launch a fine-tune. You will need to specify which base model to fine-tune, the locations of the training file and optional validation data file, an optional set of hyperparameters to customize the fine-tuning behavior, and an optional suffix to append to the name of the fine-tune. For sequences longer than the native <code>max_seq_length</code> of the model, the sequences will be truncated.</p> <p>If you specify a suffix, the fine-tune will be named <code>model.suffix.&lt;timestamp&gt;</code>. If you do not, the fine-tune will be named <code>model.&lt;timestamp&gt;</code>. The timestamp will be the time the fine-tune was launched. Note: the suffix must only contain alphanumeric characters and hyphens, and be at most 28 characters long.</p> Hyper-parameters for fine-tune  - `lr`: Peak learning rate used during fine-tuning. It decays with a cosine schedule afterward. (Default: 2e-3) - `warmup_ratio`: Ratio of training steps used for learning rate warmup. (Default: 0.03) - `epochs`: Number of fine-tuning epochs. This should be less than 20. (Default: 5) - `weight_decay`: Regularization penalty applied to learned weights. (Default: 0.001)  Create a fine-tune in python <pre><code>from llmengine import FineTune\n\nresponse = FineTune.create(\n    model=\"llama-2-7b\",\n    training_file=\"file-AbCDeLdN2Ty4M2m\",\n    validation_file=\"file-ezSRpgtKQyItI26\",\n)\n\nprint(response.json())\n</code></pre> <p>See the Model Zoo to see which models have fine-tuning support.</p> <p>See Integrations to see how to track fine-tuning metrics.</p>"},{"location":"guides/fine_tuning/#monitoring-the-fine-tune","title":"Monitoring the fine-tune","text":"<p>Once the fine-tune is launched, you can also get the status of your fine-tune.  You can also list events that your fine-tune produces. <pre><code>from llmengine import FineTune\n\nfine_tune_id = \"ft-cabcdefghi1234567890\"\nfine_tune = FineTune.get(fine_tune_id)\nprint(fine_tune.status)  # BatchJobStatus.RUNNING\nprint(fine_tune.fine_tuned_model)  # \"llama-2-7b.700101-000000\n\nfine_tune_events = FineTune.get_events(fine_tune_id)\nfor event in fine_tune_events.events:\n    print(event)\n# Prints something like:\n# timestamp=1697590000.0 message=\"{'loss': 12.345, 'learning_rate': 0.0, 'epoch': 0.97}\" level='info'\n# timestamp=1697590000.0 message=\"{'eval_loss': 23.456, 'eval_runtime': 19.876, 'eval_samples_per_second': 4.9, 'eval_steps_per_second': 4.9, 'epoch': 0.97}\" level='info'\n# timestamp=1697590020.0 message=\"{'train_runtime': 421.234, 'train_samples_per_second': 2.042, 'train_steps_per_second': 0.042, 'total_flos': 123.45, 'train_loss': 34.567, 'epoch': 0.97}\" level='info'\n</code></pre></p> <p>The status of your fine-tune will give a high-level overview of the fine-tune's progress. The events of your fine-tune will give more detail, such as the training loss and validation loss at each epoch,  as well as any errors that may have occurred. If you encounter any errors with your fine-tune,  the events are a good place to start debugging. For example, if you see <code>Unable to read training or validation dataset</code>, you may need to make your files accessible to LLM Engine. If you see <code>Invalid value received for lora parameter 'lora_alpha'!</code>, you should check that your hyperparameters are valid.</p>"},{"location":"guides/fine_tuning/#making-inference-calls-to-your-fine-tune","title":"Making inference calls to your fine-tune","text":"<p>Once your fine-tune is finished, you will be able to start making inference requests to the model. You can use the <code>fine_tuned_model</code> returned from your FineTune.get API call to reference your fine-tuned model in the Completions API. Alternatively, you can list available LLMs with <code>Model.list</code> in order to find the name of your fine-tuned model. See the Completion API for more details. You can then use that name to direct your completion requests. You must wait until your fine-tune is complete before you can plug it into the Completions API. You can check the status of your fine-tune with FineTune.get.</p> Inference with a fine-tuned model in python <pre><code>from llmengine import Completion\n\nresponse = Completion.create(\n    model=\"llama-2-7b.airlines.2023-07-17-08-30-45\",\n    prompt=\"Do you offer in-flight Wi-fi?\",\n    max_new_tokens=100,\n    temperature=0.2,\n)\nprint(response.json())\n</code></pre>"},{"location":"guides/rate_limits/","title":"Overview","text":""},{"location":"guides/rate_limits/#what-are-rate-limits","title":"What are rate limits?","text":"<p>A rate limit is a restriction that an API imposes on the number of times a user or client can access the server within a specified period of time.</p>"},{"location":"guides/rate_limits/#how-do-i-know-if-i-am-rate-limited","title":"How do I know if I am rate limited?","text":"<p>Per standard HTTP practices, your request will receive a response with HTTP status code of <code>429</code>, <code>Too Many Requests</code>.</p>"},{"location":"guides/rate_limits/#what-are-the-rate-limits-for-our-api","title":"What are the rate limits for our API?","text":"<p>The LLM Engine API is currently in a preview mode, and therefore we currently do not have any advertised rate limits. As the API moves towards a production release, we will update this section with specific rate limits. For now, the API will return HTTP 429 on an as-needed basis.</p>"},{"location":"guides/rate_limits/#error-mitigation","title":"Error mitigation","text":""},{"location":"guides/rate_limits/#retrying-with-exponential-backoff","title":"Retrying with exponential backoff","text":"<p>One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached. This approach has many benefits:</p> <ul> <li>Automatic retries means you can recover from rate limit errors without crashes or missing data</li> <li>Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail</li> <li>Adding random jitter to the delay helps retries from all hitting at the same time.</li> </ul> <p>Below are a few example solutions for Python that use exponential backoff.</p>"},{"location":"guides/rate_limits/#example-1-using-the-tenacity-library","title":"Example #1: Using the <code>tenacity</code> library","text":"<p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything. To add exponential backoff to your requests, you can use the tenacity.retry decorator. The below example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.</p> Exponential backoff in python <pre><code>import llmengine\nfrom tenacity import (\n    retry,\n    stop_after_attempt,\n    wait_random_exponential,\n)  # for exponential backoff\n\n@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\ndef completion_with_backoff(**kwargs):\n    return llmengine.Completion.create(**kwargs)\n\ncompletion_with_backoff(model=\"llama-2-7b\", prompt=\"Why is the sky blue?\")\n</code></pre>"},{"location":"guides/rate_limits/#example-2-using-the-backoff-library","title":"Example #2: Using the <code>backoff</code> library","text":"<p>Backoff is another python library that provides function decorators which can be used to wrap a function such that it will be retried until some condition is met.</p> Decorators for backoff and retry in python <pre><code>import llmengine\nimport backoff\n\n@backoff.on_exception(backoff.expo, llmengine.errors.RateLimitExceededError)\ndef completion_with_backoff(**kwargs):\n    return llmengine.Completion.create(**kwargs)\n\ncompletions_with_backoff(model=\"llama-2-7b\", prompt=\"Why is the sky blue?\")\n</code></pre>"},{"location":"guides/self_hosting/","title":"Self Hosting [Experimental]","text":"<p>This guide is currently highly experimental. Instructions are subject to change as we improve support for self-hosting.</p> <p>We provide a Helm chart that deploys LLM Engine to an Elastic Kubernetes Cluster (EKS) in AWS. This Helm chart should be configured to connect to dependencies (such as a PostgreSQL database) that you may already have available in your environment.</p> <p>The only portions of the Helm chart that are production ready are the parts that configure and manage LLM Server itself (not PostgreSQL, IAM, etc.)</p> <p>We first go over required AWS dependencies that are required to exist before we can run <code>helm install</code> in your EKS cluster.</p>"},{"location":"guides/self_hosting/#aws-dependencies","title":"AWS Dependencies","text":"<p>This section describes assumptions about existing AWS resources required run to the LLM Engine Server</p>"},{"location":"guides/self_hosting/#eks","title":"EKS","text":"<p>The LLM Engine server must be deployed in an EKS cluster environment. Currently only versions <code>1.23+</code> are supported. Below are the assumed requirements for the EKS cluster: </p> <p>You will need to provision EKS node groups with GPUs to schedule model pods. These node groups must have the <code>node-lifecycle: normal</code> label on them. Additionally, they must have the <code>k8s.amazonaws.com/accelerator</code> label set appropriately depending on the instance type:</p> Instance family <code>k8s.amazonaws.com/accelerator</code> label g4dn nvidia-tesla-t4 g5 nvidia-tesla-a10 p4d nvidia-ampere-a100 p4de nvidia-ampere-a100e p5 nvidia-hopper-h100 <p>We also recommend setting the following taint on your GPU nodes to prevent pods requiring GPU resources from being scheduled on them: - { key = \"nvidia.com/gpu\", value = \"true\", effect = \"NO_SCHEDULE\" }</p>"},{"location":"guides/self_hosting/#postgresql","title":"PostgreSQL","text":"<p>The LLM Engine server requires a PostgreSQL database to back data. LLM Engine currently supports PostgreSQL version 14. Create a PostgreSQL database (e.g. AWS RDS PostgreSQL) if you do not have an existing one you wish to connect LLM Engine to.</p> <p>To enable LLM Engine to connect to the PostgreSQL engine, we create a Kubernetes secret with the PostgreSQL url. An example YAML is provided below: <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: llm-engine-database-credentials  # this name will be an input to our Helm Chart\ndata:\n    database_url = \"postgresql://[user[:password]@][netloc][:port][/dbname][?param1=value1&amp;...]\"\n</code></pre></p>"},{"location":"guides/self_hosting/#redis","title":"Redis","text":"<p>The LLM Engine server requires Redis for various caching/queue functionality. LLM Engine currently supports Redis version 6. Create a Redis cluster (e.g. AWS Elasticache for Redis) if you do not have an existing one you wish to connect LLM Engine to.</p> <p>To enable LLM Engine to connect redis, fill out the Helm chart values with the redis host and url.</p>"},{"location":"guides/self_hosting/#amazon-s3","title":"Amazon S3","text":"<p>You will need to have an S3 bucket for LLM Engine to store various assets (e.g model weigts, prediction restuls). The ARN of this bucket should be provided in the Helm chart values.</p>"},{"location":"guides/self_hosting/#amazon-ecr","title":"Amazon ECR","text":"<p>You will need to provide an ECR repository for the deployment to store model containers. The ARN of this repository should be provided in the Helm chart values.</p>"},{"location":"guides/self_hosting/#amazon-sqs","title":"Amazon SQS","text":"<p>LLM Engine utilizes Amazon SQS to keep track of jobs. LLM Engine will create and use SQS queues as needed.</p>"},{"location":"guides/self_hosting/#identity-and-access-management-iam","title":"Identity and Access Management (IAM)","text":"<p>The LLM Engine server will an IAM role to perform various AWS operations. This role will be assumed by the serviceaccount <code>llm-engine</code> in the <code>launch</code> namespace in the EKS cluster. The ARN of this role needs to be provided to the Helm chart, and the role needs to be provided the following permissions:</p> Action Resource <code>s3:Get*</code>, <code>s3:Put*</code> <code>${s3_bucket_arn}/*</code> <code>s3:List*</code> <code>${s3_bucket_arn}</code> <code>sqs:*</code> <code>arn:aws:sqs:${region}:${account_id}:llm-engine-endpoint-id-*</code> <code>sqs:ListQueues</code> <code>*</code> <code>ecr:BatchGetImage</code>, <code>ecr:DescribeImages</code>, <code>ecr:GetDownloadUrlForLayer</code>, <code>ecr:ListImages</code> <code>${ecr_repository_arn}</code>"},{"location":"guides/self_hosting/#helm-chart","title":"Helm Chart","text":"<p>Now that all dependencies have been installed and configured, we can run the provided Helm chart. The values in the Helm chart will need to correspond with the resources described in the Dependencies section. </p> <p>Ensure that Helm V3 is installed instructions and can connect to the EKS cluster. Users should be able to install the chart with <code>helm install llm-engine llm-engine -f llm-engine/values_sample.yaml -n &lt;DESIRED_NAMESPACE&gt;</code>. Below are the configurations to specify in the <code>values_sample.yaml</code> file. </p> Parameter Description Required tag The LLM Engine docker image tag Yes context A user-specified deployment tag No image.gatewayRepository The docker repository to pull the LLM Engine gateway image from Yes image.builderRepository The docker repository to pull the LLM Engine endpoint builder image from Yes image.cacherRepository The docker repository to pull the LLM Engine cacher image from Yes image.forwarderRepository The docker repository to pull the LLM Engine forwarder image from Yes image.pullPolicy The docker image pull policy No secrets.kubernetesDatabaseSecretName The name of the secret that contains the database credentials Yes serviceAccount.annotations.eks.amazonaws.com/role-arn The ARN of the IAM role that the service account will assume Yes service.type The service configuration for the main LLM Engine server No service.port The service configuration for the main LLM Engine server No replicaCount The amount of replica pods for each deployment No autoscaling The autoscaling configuration for LLM Engine server deployments No resources.requests.cpu The k8s resources for LLM Engine server deployments No nodeSelector The node selector for LLM Engine server deployments No tolerations The tolerations for LLM Engine server deployments No affinity The affinity for LLM Engine server deployments No aws.configMap.name The AWS configurations (by configMap) for LLM Engine server deployments No aws.configMap.create The AWS configurations (by configMap) for LLM Engine server deployments No aws.profileName The AWS configurations (by configMap) for LLM Engine server deployments No serviceTemplate.securityContext.capabilities.drop Additional flags for model endpoints No serviceTemplate.mountInfraConfig Additional flags for model endpoints No config.values.infra.k8s_cluster_name The name of the k8s cluster Yes config.values.infra.dns_host_domain The domain name of the k8s cluster Yes config.values.infra.default_region The default AWS region for various resources Yes config.values.infra.ml_account_id The AWS account ID for various resources Yes config.values.infra.docker_repo_prefix The prefix for AWS ECR repositories Yes config.values.infra.redis_host The hostname of the redis cluster you wish to connect Yes config.values.infra.s3_bucket The S3 bucket you wish to connect Yes config.values.llm_engine.endpoint_namespace K8s namespace the endpoints will be created in Yes config.values.llm_engine.cache_redis_aws_url The full url for the redis cluster you wish to connect No config.values.llm_engine.cache_redis_azure_host The redis cluster host when using cloud_provider azure No config.values.llm_engine.s3_file_llm_fine_tuning_job_repository The S3 URI for the S3 bucket/key that you wish to save fine-tuned assets Yes config.values.dd_trace_enabled Whether to enable datadog tracing, datadog must be installed in the cluster No"},{"location":"guides/self_hosting/#play-with-it","title":"Play With It","text":"<p>Once <code>helm install</code> succeeds, you can forward port <code>5000</code> from a <code>llm-engine</code> pod and test sending requests to it.</p> <p>First, see a list of pods in the namespace that you performed <code>helm install</code> in: <pre><code>$ kubectl get pods -n &lt;NAMESPACE_WHERE_LLM_ENGINE_IS_INSTALLED&gt;\nNAME                                           READY   STATUS             RESTARTS      AGE\nllm-engine-668679554-9q4wj                     1/1     Running            0             18m\nllm-engine-668679554-xfhxx                     1/1     Running            0             18m\nllm-engine-cacher-5f8b794585-fq7dj             1/1     Running            0             18m\nllm-engine-endpoint-builder-5cd6bf5bbc-sm254   1/1     Running            0             18m\nllm-engine-image-cache-a10-sw4pg               1/1     Running            0             18m \n</code></pre> Note the pod names you see may be different.</p> <p>Forward a port from a <code>llm-engine</code> pod: <pre><code>$ kubectl port-forward pod/llm-engine-&lt;REST_OF_POD_NAME&gt; 5000:5000 -n &lt;NAMESPACE_WHERE_LLM_ENGINE_IS_INSTALLED&gt;\n</code></pre></p> <p>Then, try sending a request to get LLM model endpoints for <code>test-user-id</code>: <pre><code>$ curl -X GET -H \"Content-Type: application/json\" -u \"test-user-id:\" \"http://localhost:5000/v1/llm/model-endpoints\"\n</code></pre></p> <p>You should get the following response: <pre><code>{\"model_endpoints\":[]}\n</code></pre></p> <p>Next, let's create a LLM endpoint using llama-7b: <pre><code>$ curl -X POST 'http://localhost:5000/v1/llm/model-endpoints' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"name\": \"llama-7b\",\n        \"model_name\": \"llama-7b\",\n        \"source\": \"hugging_face\",\n        \"inference_framework\": \"text_generation_inference\",\n        \"inference_framework_image_tag\": \"0.9.3\",\n        \"num_shards\": 4,\n        \"endpoint_type\": \"streaming\",\n        \"cpus\": 32,\n        \"gpus\": 4,\n        \"memory\": \"40Gi\",\n        \"storage\": \"40Gi\",\n        \"gpu_type\": \"nvidia-ampere-a10\",\n        \"min_workers\": 1,\n        \"max_workers\": 12,\n        \"per_worker\": 1,\n        \"labels\": {},\n        \"metadata\": {}\n    }' \\\n    -u test_user_id:\n</code></pre></p> <p>It should output something like: <pre><code>{\"endpoint_creation_task_id\":\"8d323344-b1b5-497d-a851-6d6284d2f8e4\"}\n</code></pre></p> <p>Wait a few minutes for the endpoint to be ready. You can tell that it's ready by listing pods and checking that all containers in the llm endpoint pod are ready: <pre><code>$ kubectl get pods -n &lt;endpoint_namespace specified in values_sample.yaml&gt;\nNAME                                                              READY   STATUS    RESTARTS        AGE\nllm-engine-endpoint-id-end-cismpd08agn003rr2kc0-7f86ff64f9qj9xp   2/2     Running   1 (4m41s ago)   7m26s\n</code></pre> Note the endpoint name could be different.</p> <p>Then, you can send an inference request to the endppoint: <pre><code>$ curl -X POST 'http://localhost:5000/v1/llm/completions-sync?model_endpoint_name=llama-7b' \\\n    -H 'Content-Type: application/json' \\\n    -d '{\n        \"prompts\": [\"Tell me a joke about AI\"],\n        \"max_new_tokens\": 30,\n        \"temperature\": 0.1\n    }' \\\n    -u test-user-id:\n</code></pre></p> <p>You should get a response similar to: <pre><code>{\"status\":\"SUCCESS\",\"outputs\":[{\"text\":\". Tell me a joke about AI. Tell me a joke about AI. Tell me a joke about AI. Tell me\",\"num_completion_tokens\":30}],\"traceback\":null}\n</code></pre></p>"},{"location":"guides/self_hosting/#pointing-llm-engine-client-to-use-self-hosted-infrastructure","title":"Pointing LLM Engine client to use self-hosted infrastructure","text":"<p>The <code>llmengine</code> client makes requests to Scale AI's hosted infrastructure by default. You can have <code>llmengine</code> client make requests to your own self-hosted infrastructure by setting the <code>LLM_ENGINE_BASE_PATH</code> environment variable to the URL of the <code>llm-engine</code> service. </p> <p>The exact URL of <code>llm-engine</code> service depends on your Kubernetes cluster networking setup. The domain is specified at <code>config.values.infra.dns_host_domain</code> in the helm chart values config file. Using <code>charts/llm-engine/values_sample.yaml</code> as an example, you would do: <pre><code>export LLM_ENGINE_BASE_PATH=https://llm-engine.domain.com\n</code></pre></p>"},{"location":"guides/token_streaming/","title":"Token streaming","text":"<p>The Completions APIs support a <code>stream</code> boolean parameter that, when <code>True</code>, will return a streamed response of token-by-token server-sent events (SSEs) rather than waiting to receive the full response when model generation has finished. This decreases latency of when you start getting a response.</p> <p>The response will consist of SSEs of the form <code>{\"token\": dict, \"generated_text\": str | null, \"details\": dict | null}</code>, where the dictionary for each token will contain log probability information in addition to the generated string; the <code>generated_text</code> field will be <code>null</code> for all but the last SSE, for which it will contain the full generated response.</p>"},{"location":"internal/","title":"Model Engine \u2014 Operators Guide","text":"<p>Internal documentation for service owners and deployment engineers installing, operating, and debugging model engine in customer environments.</p> <p>Not the end-user docs? The user-facing Python client documentation is at the root of this site.</p>"},{"location":"internal/#contents","title":"Contents","text":"Document What it covers When to use it Architecture System overview, component deep-dives, lifecycle flows, autoscaling Before your first deployment; when debugging unfamiliar failures Helm Values Reference Every configurable value, organized by concern, with high-risk callouts During installation and upgrades Smoke Tests Post-deploy validation checklist (Tier A: CPU, Tier B: GPU+LLM) After every <code>helm install</code> or <code>helm upgrade</code> Cloud Support Matrix Per-cloud config reference, behavior differences, image mirroring When deploying to a specific cloud"},{"location":"internal/#quick-links","title":"Quick Links","text":"<ul> <li>Installing for the first time? \u2192 Start with Architecture for the mental model, then Helm Values for configuration.</li> <li>Validating a deployment? \u2192 Go straight to Smoke Tests.</li> <li>Deploying to Azure / GCP / on-prem? \u2192 See Cloud Support Matrix for what to configure differently.</li> <li>Something broken? \u2192 Troubleshooting Guide (Confluence).</li> </ul>"},{"location":"internal/#contributing","title":"Contributing","text":"<p>These docs live at <code>docs/internal/</code> in the repo. Update them when you change behavior or configuration \u2014 the PR template includes a reminder.</p> <p>Rule: If a PR author would need to update this doc when changing the code \u2192 it belongs here. Operational notes from specific deployments belong in Confluence.</p>"},{"location":"internal/architecture/","title":"Model Engine Architecture","text":"<p>Audience: Service owners and deployment engineers installing, operating, or debugging model engine in a customer environment.</p> <p>Scope: This document covers system structure, lifecycle flows, cross-cutting concerns, and component deep-dives. Configuration reference is in <code>helm-values.md</code>. Per-cloud behavior differences are in <code>cloud-matrix.md</code>.</p>"},{"location":"internal/architecture/#1-system-structure","title":"1. System Structure","text":""},{"location":"internal/architecture/#11-architecture-overview","title":"1.1 Architecture Overview","text":"<p>Model engine consists of five core pods and a set of external dependencies. The control plane (Gateway, Service Builder, K8s Cacher) runs in the model engine namespace. Inference pods run in a separate endpoint namespace, typically <code>llm-engine</code>.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Control Plane Namespace (e.g. model-engine)                                     \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   REST    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502  \u2502   Gateway   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Service Builder \u2502                                  \u2502\n\u2502  \u2502  (FastAPI)  \u2502           \u2502  (Celery worker) \u2502                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n\u2502         \u2502  read                     \u2502 write K8s                                  \u2502\n\u2502         \u2502  endpoint                 \u2502 resources                                  \u2502\n\u2502         \u25bc  status                   \u25bc                                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                  \u2502\n\u2502  \u2502  K8s Cacher \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502     Redis       \u2502                                  \u2502\n\u2502  \u2502 (Deployment)\u2502  write     \u2502  (cache store)  \u2502                                  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  TTL 60s  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                            \u2502\n\u2502  \u2502 Celery Autoscaler\u2502  (scales async endpoint workers by queue depth)            \u2502\n\u2502  \u2502  (StatefulSet)   \u2502                                                            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                            \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                                                \u2502\n\u2502  \u2502 Balloon Pods \u2502  (low-priority GPU placeholder pods, one Deployment per GPU)  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nExternal Dependencies\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   PostgreSQL   \u2502  \u2502    Redis     \u2502  \u2502   Message Broker     \u2502  \u2502 Object Storage \u2502\n\u2502  (endpoint DB) \u2502  \u2502 (K8s cache)  \u2502  \u2502 SQS / ASB / Redis    \u2502  \u2502 S3 / GCS / ABS \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Endpoint Namespace (e.g. llm-engine)                                            \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502  \u2502  Sync/Streaming Endpoint (Deployment)               \u2502                        \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502                        \u2502\n\u2502  \u2502  \u2502 HTTP Forwarder\u2502  \u2502  vLLM / inference process   \u2502 \u2502                        \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502  \u2502  Async Endpoint (Deployment)                        \u2502                        \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502                        \u2502\n\u2502  \u2502  \u2502  Celery worker (reads from SQS/ASB/Redis)    \u2502   \u2502                        \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2502                                                                                  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502  \u2502  Multi-node Endpoint (LeaderWorkerSet / LWS)        \u2502                        \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502                        \u2502\n\u2502  \u2502  \u2502  Leader pod  \u2502  \u2502  Worker pods \u2502  (no HPA/KEDA)  \u2502                        \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2502                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Data flow summary:</p> <ul> <li>Endpoint creation: Client \u2192 Gateway REST \u2192 broker queue \u2192 Service Builder Celery worker \u2192 K8s API</li> <li>Sync inference: Client \u2192 Gateway \u2192 HTTP forward to inference pod \u2192 response</li> <li>Async inference: Client \u2192 Gateway \u2192 broker queue \u2192 Celery worker in inference pod \u2192 result stored \u2192 Client polls</li> <li>Streaming inference: Client \u2192 Gateway \u2192 SSE stream from inference pod</li> <li>Status reads: Gateway \u2192 Redis (written by K8s Cacher, not K8s API directly)</li> </ul>"},{"location":"internal/architecture/#12-kubernetes-resource-inventory","title":"1.2 Kubernetes Resource Inventory","text":"<p>Resources created and managed by the helm chart (control plane):</p> Resource Kind Notes <code>model-engine-gateway</code> Deployment FastAPI server; replicas configured via <code>replicaCount.gateway</code> <code>model-engine-builder</code> Deployment Celery worker for endpoint creation; replicas via <code>replicaCount.builder</code> <code>model-engine-cacher</code> Deployment K8s cache loop; typically 1 replica (<code>replicaCount.cacher</code>) <code>model-engine-celery-autoscaler</code> StatefulSet Scales async endpoint workers; shards via <code>celery_autoscaler.num_shards</code> <code>model-engine-gateway</code> HPA Autoscales gateway replicas based on concurrency <code>model-engine-config</code> ConfigMap Runtime config mounted into all control plane pods <code>model-engine</code> ServiceAccount Used by control plane pods <code>model-engine</code> ClusterRole + ClusterRoleBinding K8s API access for Service Builder and Cacher Balloon Deployments Deployment (one per GPU type) Low-priority placeholder pods; see <code>balloons</code> in values <p>Resources created per inference endpoint (in endpoint namespace):</p> Resource Kind Condition Inference Deployment Deployment All non-LWS endpoints LeaderWorkerSet LeaderWorkerSet (CRD) Multi-node endpoints only K8s Service Service Sync and streaming endpoints HPA HorizontalPodAutoscaler Sync/streaming, <code>min_workers &gt; 0</code> KEDA ScaledObject ScaledObject (CRD) Sync/streaming, <code>min_workers == 0</code> PodDisruptionBudget PodDisruptionBudget All endpoints (configurable) Istio VirtualService VirtualService Sync/streaming, <code>istio_enabled: true</code> Istio DestinationRule DestinationRule Sync/streaming, <code>istio_enabled: true</code> Istio ServiceEntry ServiceEntry Multi-node + <code>istio_enabled: true</code> SQS Queue / ASB Topic Cloud resource Async endpoints and all endpoints on async clouds <p>HPA and KEDA are mutually exclusive</p> <p>The Service Builder enforces this: when creating or updating an endpoint, it deletes the KEDA ScaledObject before creating an HPA (if <code>min_workers &gt; 0</code>), or deletes the HPA before creating a KEDA ScaledObject (if <code>min_workers == 0</code>). Both never coexist on the same endpoint.</p>"},{"location":"internal/architecture/#13-external-dependencies-and-prerequisites","title":"1.3 External Dependencies and Prerequisites","text":"<p>The following must exist and be reachable from the cluster before <code>helm install</code>:</p> Dependency Required For Notes PostgreSQL All operations Endpoint metadata, bundle records, batch job records Redis Gateway routing, cacher, async metrics Two logical roles: K8s cache and inference autoscaling metrics Message broker (SQS / ASB / Redis) Async endpoints; endpoint creation queue Cloud-dependent; see \u00a73.3 Object storage (S3 / GCS / ABS) LLM artifacts, fine-tune repos, batch job progress Cloud-dependent Image registry (ECR / ACR / GAR) All image pulls Must be mirrored from <code>public.ecr.aws/b2z8n5q1/</code> in customer envs Prometheus KEDA scale-to-zero Required if any sync endpoint uses <code>min_workers == 0</code>; see \u00a73.1 KEDA Scale-to-zero Must be installed in cluster if any endpoint uses <code>min_workers == 0</code> Istio VirtualService routing, mTLS Optional but strongly recommended; set <code>istio_enabled: true/false</code> to match actual state NVIDIA GPU Operator GPU inference Required for GPU workloads; nodes must be labeled and driver-ready <p>Image registry mirroring</p> <p>In customer environments, all model engine images must be mirrored from the public ECR source (<code>public.ecr.aws/b2z8n5q1/</code>) to the customer registry before installation. The <code>vllm_repository</code> value defaults to a relative path that resolves to Scale's internal ECR in many deployment configurations and must be overridden. Failing to mirror is the most common silent deployment failure: endpoint creation returns HTTP 200 but the endpoint stays <code>INITIALIZING</code> indefinitely.</p>"},{"location":"internal/architecture/#2-lifecycle-flows","title":"2. Lifecycle Flows","text":""},{"location":"internal/architecture/#21-generic-endpoint-creation-flow","title":"2.1 Generic Endpoint Creation Flow","text":"<p>The endpoint creation path is identical for all endpoint types (sync, async, streaming, multi-node). The LLM API layer (\u00a72.3) is a higher-level wrapper that feeds into the same flow.</p> <pre><code>Client\n  \u2502\n  \u2502  POST /v1/model-endpoints\n  \u25bc\nGateway (FastAPI)\n  \u2502  Validates request, writes endpoint record to PostgreSQL (status: PENDING)\n  \u2502  Enqueues Celery task to endpoint creation queue (SQS / ASB / Redis)\n  \u2502\n  \u25bc\nMessage Broker\n  \u2502  Task sits in queue (SQS queue / ASB topic / Redis queue)\n  \u2502\n  \u25bc\nService Builder (Celery worker)\n  \u2502  Dequeues task\n  \u2502  Calls K8s API to create/update:\n  \u2502    - Deployment or LeaderWorkerSet\n  \u2502    - HPA or KEDA ScaledObject (sync/streaming, non-LWS only)\n  \u2502    - K8s Service (sync/streaming only)\n  \u2502    - Istio VirtualService + DestinationRule (if istio_enabled, non-LWS)\n  \u2502    - Istio ServiceEntry (if istio_enabled, LWS only)\n  \u2502    - PodDisruptionBudget\n  \u2502  Updates endpoint record in PostgreSQL (status: INITIALIZING \u2192 READY)\n  \u2502\n  \u25bc\nK8s Cacher (background loop, every 15s)\n  \u2502  Reads endpoint state from K8s API\n  \u2502  Writes to Redis with 60s TTL\n  \u2502\n  \u25bc\nGateway\n  \u2502  Reads endpoint status from Redis (not K8s API directly)\n  \u2502  Returns status to client via GET /v1/model-endpoints/{id}\n</code></pre> <p>Timing constraints:</p> <ul> <li>The Celery task has a 30-minute hard timeout. Endpoint creation that exceeds this ceiling (e.g., very large image pulls on cold nodes) will fail with no retry, and the endpoint will be stuck <code>INITIALIZING</code>.</li> <li>The K8s Cacher runs on a 15-second poll cycle. After the Service Builder marks an endpoint <code>READY</code> in PostgreSQL, there is a brief window (up to 15s) before the Gateway's Redis cache reflects the new state. During this window, status reads may lag.</li> </ul> <p>Celery task timeout is a hard ceiling</p> <p>The 30-minute Celery task timeout applies to the entire endpoint creation operation, including image pull time. For large model images on cold nodes, image pull alone can approach this limit. Plan capacity accordingly and ensure balloon pods keep GPU nodes warm so image pulls start quickly.</p>"},{"location":"internal/architecture/#22-inference-flows","title":"2.2 Inference Flows","text":""},{"location":"internal/architecture/#synchronous-inference","title":"Synchronous Inference","text":"<pre><code>Client\n  \u2502  POST /v1/model-endpoints/{id}/predict\n  \u25bc\nGateway\n  \u2502  Looks up endpoint URL from Redis cache\n  \u2502  HTTP POST directly to inference pod's HTTP forwarder\n  \u25bc\nInference Pod (HTTP Forwarder + vLLM / model process)\n  \u2502  Processes request, returns response\n  \u25bc\nGateway \u2192 Client  (response forwarded synchronously)\n</code></pre> <p>The Gateway does not queue synchronous requests. The inference pod must be reachable at the time of the request. If the pod is not yet ready or has been evicted, the client receives an error immediately.</p>"},{"location":"internal/architecture/#asynchronous-inference","title":"Asynchronous Inference","text":"<pre><code>Client\n  \u2502  POST /v1/model-endpoints/{id}/predict  (async endpoint)\n  \u25bc\nGateway\n  \u2502  Enqueues Celery task to inference queue\n  \u2502  (per-endpoint SQS queue / ASB topic / Redis queue)\n  \u2502  Returns task_id immediately (HTTP 200)\n  \u25bc\nMessage Broker (per-endpoint queue)\n  \u2502\n  \u25bc\nCelery Worker (inside inference pod)\n  \u2502  Dequeues task\n  \u2502  Runs inference\n  \u2502  Stores result in Celery result backend (Redis / SQS)\n  \u25bc\nClient polls GET /v1/tasks/{task_id}\n  \u2502\n  \u25bc\nGateway\n  \u2502  Reads task result from Celery result backend\n  \u2502  Returns status: PENDING / SUCCESS / FAILURE\n</code></pre> <p>Each async endpoint has its own dedicated queue: one SQS queue per endpoint on AWS, one ASB topic per endpoint on Azure. The Celery Autoscaler monitors queue depth and scales the Deployment's replica count accordingly (see \u00a73.1).</p>"},{"location":"internal/architecture/#streaming-inference","title":"Streaming Inference","text":"<p>Streaming follows the same routing path as synchronous inference. The Gateway establishes a Server-Sent Events (SSE) connection to the inference pod and streams response chunks back to the client as they arrive. The inference pod must support streaming \u2014 vLLM does natively via its <code>/v1/chat/completions</code> and <code>/v1/completions</code> endpoints with <code>stream=true</code>.</p>"},{"location":"internal/architecture/#23-llm-api-layer","title":"2.3 LLM API Layer","text":"<p>Model engine exposes two API surfaces for LLM inference:</p> API Surface Routes Description Generic endpoint API <code>GET/POST /v1/model-endpoints</code>, <code>/v1/model-endpoints/{id}/predict</code> Low-level; caller specifies image, resources, and all parameters explicitly LLM endpoint API v1 <code>/v1/llms/...</code> Higher-level; opinionated defaults, auto-selects vLLM image and hardware LLM endpoint API v2 <code>/v2/...</code> OpenAI-compatible; same infrastructure as v1 LLM API <p>v1 vs v2:</p> <ul> <li>v1 (<code>/v1/llms/...</code>): Model engine's native LLM API. Returns model engine response format.</li> <li>v2 (<code>/v2/...</code>): OpenAI-compatible API. Accepts and returns the same request/response format as OpenAI's API, including <code>stream=true</code> for SSE streaming. Pydantic models are generated from OpenAI's official OpenAPI spec. Endpoints: <code>POST /v2/chat/completions</code>, <code>POST /v2/completions</code>.</li> </ul> <p>How LLM endpoints use Service Builder:</p> <p>The LLM endpoint API (<code>LiveLLMModelEndpointService</code>) is a thin wrapper over the generic <code>LiveModelEndpointService</code>. When a client calls <code>POST /v1/llms</code> to create an LLM endpoint, the service translates a <code>CreateLLMModelEndpointV1Request</code> into a <code>CreateModelEndpointV1Request</code> with opinionated defaults \u2014 vLLM image from <code>vllm_repository</code>, resource sizing from <code>recommendedHardware</code>, GPU type selection \u2014 and then delegates to the same Service Builder queue path described in \u00a72.1. There is no separate infrastructure for LLM endpoints. They are regular model endpoints with a curated configuration. All failure modes from \u00a72.1 apply equally.</p> <p><code>recommendedHardware</code> auto-selection:</p> <p>The <code>recommendedHardware</code> helm value contains a lookup table keyed by GPU memory requirement (<code>byGpuMemoryGb</code>) and by model name (<code>byModelName</code>). When an LLM endpoint is created without explicit resource specifications, the service queries this table to select GPU type, GPU count, CPU, memory, storage, and <code>nodes_per_worker</code>. When <code>nodes_per_worker &gt; 1</code>, the service creates a multi-node (LWS) endpoint instead of a regular Deployment. See \u00a73.4 for details.</p>"},{"location":"internal/architecture/#3-cross-cutting-concerns","title":"3. Cross-cutting Concerns","text":""},{"location":"internal/architecture/#31-autoscaling","title":"3.1 Autoscaling","text":"<p>Model engine uses three distinct autoscaling mechanisms depending on endpoint type and configuration. They are not interchangeable, and only one mechanism applies to any given endpoint at a time.</p>"},{"location":"internal/architecture/#sync-and-streaming-endpoints-hpa-min_workers-0","title":"Sync and Streaming Endpoints: HPA (<code>min_workers &gt; 0</code>)","text":"<p>When <code>min_workers &gt; 0</code>, the Service Builder creates a <code>HorizontalPodAutoscaler</code> targeting the endpoint's Deployment. The HPA scales based on CPU and memory metrics. The autoscaling API version is selected based on cluster version: <code>autoscaling/v2</code> for Kubernetes &gt;= 1.26, <code>autoscaling/v2beta2</code> for Kubernetes 1.23\u20131.25.</p> <pre><code>min_workers &gt; 0  \u2192  KEDA ScaledObject deleted (if exists)  \u2192  HPA created\n</code></pre>"},{"location":"internal/architecture/#sync-and-streaming-endpoints-keda-min_workers-0","title":"Sync and Streaming Endpoints: KEDA (<code>min_workers == 0</code>)","text":"<p>When <code>min_workers == 0</code>, the Service Builder creates a KEDA <code>ScaledObject</code> instead of an HPA. KEDA uses request concurrency metrics sourced from Prometheus to decide when to scale the endpoint from 0 replicas to 1 replica.</p> <pre><code>min_workers == 0  \u2192  HPA deleted (if exists)  \u2192  KEDA ScaledObject created\n</code></pre> <p>KEDA requires <code>prometheus_server_address</code></p> <p>KEDA-based scale-to-zero requires <code>config.values.infra.prometheus_server_address</code> to be set in helm values. Without it, the <code>can_scale_http_endpoint_from_zero_flag</code> is <code>False</code> and scale-to-zero will silently not work. This is enforced in <code>dependencies.py</code>:</p> <pre><code>can_scale_http_endpoint_from_zero_flag=infra_config().prometheus_server_address is not None\n</code></pre> <p>This is one of the most non-obvious configuration dependencies in the system. The endpoint creation will succeed and the KEDA ScaledObject will be created, but scaling will not function.</p> <p>Known limitation: KEDA only scales 0\u21921, not 1\u2192N</p> <p>As of the current codebase, KEDA ScaledObjects only support scaling a sync endpoint from 0 replicas to 1 replica. Scaling from 1 to N is not implemented. This is a documented TODO in <code>k8s_endpoint_resource_delegate.py</code>:</p> <pre><code># Right now, keda only will support scaling from 0 to 1\n# TODO support keda scaling from 1 to N as well\nif request.build_endpoint_request.min_workers &gt; 0:\n    # ... create HPA\nelse:  # min workers == 0, use keda\n    # ... create KEDA ScaledObject\n</code></pre> <p>For endpoints that need to scale beyond 1 replica, use <code>min_workers &gt;= 1</code> (which triggers HPA instead of KEDA).</p>"},{"location":"internal/architecture/#async-endpoints-celery-autoscaler","title":"Async Endpoints: Celery Autoscaler","text":"<p>Async endpoints are scaled by the Celery Autoscaler StatefulSet, not by HPA or KEDA. The Celery Autoscaler monitors the depth of each endpoint's message queue (SQS queue on AWS, ASB topic on Azure, Redis queue on GCP/on-prem) and adjusts the Deployment's replica count by patching the K8s API directly.</p> <p>The number of autoscaler shards is configured via <code>celery_autoscaler.num_shards</code>. Multiple shards distribute the monitoring load across many concurrent endpoints. The Celery Autoscaler is enabled via <code>celery_autoscaler.enabled: true</code>.</p>"},{"location":"internal/architecture/#multi-node-lws-endpoints-no-autoscaling","title":"Multi-node (LWS) Endpoints: No Autoscaling","text":"<p>LeaderWorkerSet endpoints do not support autoscaling. <code>min_workers</code> must equal <code>max_workers</code>. No HPA or KEDA ScaledObject is created. Capacity changes require deleting and recreating the endpoint.</p>"},{"location":"internal/architecture/#autoscaling-summary","title":"Autoscaling Summary","text":"Endpoint Type <code>min_workers</code> Scaler Metric Source Sync / Streaming <code>&gt; 0</code> HPA CPU / memory Sync / Streaming <code>== 0</code> KEDA ScaledObject Prometheus (request concurrency) Async any Celery Autoscaler StatefulSet Queue depth (SQS / ASB / Redis) Multi-node (LWS) must equal <code>max_workers</code> None \u2014"},{"location":"internal/architecture/#32-observability","title":"3.2 Observability","text":"<p>Structured logging: All control plane components emit structured JSON logs. Log verbosity is controlled via <code>debug_mode</code> in helm values.</p> <p>Datadog APM (optional): Enabled by setting <code>dd_trace_enabled: true</code> in <code>config.values.launch</code> and installing the Datadog agent in the cluster. When enabled, the <code>DatadogMonitoringMetricsGateway</code> is used instead of <code>FakeMonitoringMetricsGateway</code>. This gates distributed tracing and APM metrics. The top-level <code>datadog.enabled</code> helm value controls Datadog agent sidecar injection.</p> <pre><code># from dependencies.py\nif hmi_config.dd_trace_enabled:\n    monitoring_metrics_gateway = DatadogMonitoringMetricsGateway()\nelse:\n    monitoring_metrics_gateway = FakeMonitoringMetricsGateway()\n</code></pre> <p>Prometheus metrics: Request concurrency metrics are exposed and consumed by KEDA for scale-to-zero. The Prometheus server must be reachable at the address configured in <code>prometheus_server_address</code>. See \u00a73.1 for the dependency.</p> <p>OpenTelemetry tracing: An OTel-based telemetry design is in progress and not yet in production. Current tracing is provided via the <code>TracingGateway</code> abstraction, with Datadog as the primary production implementation.</p> <p>K8s Cacher readiness probe: The K8s Cacher writes a readiness file (<code>READYZ_FPATH</code>) after its first successful loop iteration. This gates the cacher pod's <code>readinessProbe</code>, ensuring the Redis cache has at least one warm cycle before the pod is considered ready.</p>"},{"location":"internal/architecture/#33-cloud-backend-abstraction","title":"3.3 Cloud Backend Abstraction","text":"<p>The <code>config.values.infra.cloud_provider</code> value is the single switch that drives selection of broker, storage, registry, and auth implementations at runtime. This selection happens in <code>dependencies.py</code> and <code>k8s_cache.py</code> on startup. Changing this value without corresponding infrastructure changes will cause runtime failures.</p>"},{"location":"internal/architecture/#broker-message-queue-selection","title":"Broker (message queue) selection","text":"<code>cloud_provider</code> Endpoint creation queue Async inference queue Queue delegate <code>aws</code> (default) SQS SQS <code>SQSQueueEndpointResourceDelegate</code> <code>azure</code> Azure Service Bus Azure Service Bus <code>ASBQueueEndpointResourceDelegate</code> <code>gcp</code> Redis (Memorystore) Redis (Memorystore) <code>RedisQueueEndpointResourceDelegate</code> <code>onprem</code> Redis Redis <code>OnPremQueueEndpointResourceDelegate</code> <p>Redis broker is the legacy path</p> <p>Redis was the original broker for all clouds. SQS (AWS) and Azure Service Bus (Azure) replaced it due to reliability and scale limitations. GCP and on-prem still use Redis as the broker. Redis-as-broker has known reliability limitations compared to SQS and ASB.</p> <p>Azure Service Bus idle connection drops</p> <p>Azure Service Bus drops idle AMQP connections after approximately 300 seconds. This manifests as random 503 errors on async inference with no obvious configuration cause. The fix is <code>broker_pool_limit=0</code> (disables connection pooling, forcing reconnection on each use). This was resolved in a recent commit \u2014 verify your deployment includes the fix before deploying to Azure.</p>"},{"location":"internal/architecture/#storage-selection","title":"Storage selection","text":"<code>cloud_provider</code> Filesystem gateway LLM artifact gateway File storage gateway <code>aws</code> / <code>onprem</code> <code>S3FilesystemGateway</code> <code>S3LLMArtifactGateway</code> <code>S3FileStorageGateway</code> <code>azure</code> <code>ABSFilesystemGateway</code> <code>ABSLLMArtifactGateway</code> <code>ABSFileStorageGateway</code> <code>gcp</code> <code>GCSFilesystemGateway</code> <code>GCSLLMArtifactGateway</code> <code>GCSFileStorageGateway</code> <p>On-prem uses S3-compatible storage (MinIO or equivalent) via the same S3 gateways as AWS.</p>"},{"location":"internal/architecture/#registry-selection","title":"Registry selection","text":"<code>cloud_provider</code> Docker repository class <code>aws</code> (default) <code>ECRDockerRepository</code> <code>azure</code> <code>ACRDockerRepository</code> <code>gcp</code> <code>GARDockerRepository</code> <code>onprem</code> <code>OnPremDockerRepository</code>"},{"location":"internal/architecture/#inference-autoscaling-metrics-gateway-selection","title":"Inference autoscaling metrics gateway selection","text":"<code>cloud_provider</code> Autoscaling metrics gateway <code>azure</code> <code>ASBInferenceAutoscalingMetricsGateway</code> all others <code>RedisInferenceAutoscalingMetricsGateway</code>"},{"location":"internal/architecture/#fine-tune-repository-selection","title":"Fine-tune repository selection","text":"<code>cloud_provider</code> Fine-tune repository Fine-tune events repository <code>aws</code> / <code>onprem</code> <code>S3FileLLMFineTuneRepository</code> <code>S3FileLLMFineTuneEventsRepository</code> <code>azure</code> <code>ABSFileLLMFineTuneRepository</code> <code>ABSFileLLMFineTuneEventsRepository</code> <code>gcp</code> <code>GCSFileLLMFineTuneRepository</code> <code>GCSFileLLMFineTuneEventsRepository</code>"},{"location":"internal/architecture/#34-gpu-and-hardware-configuration","title":"3.4 GPU and Hardware Configuration","text":""},{"location":"internal/architecture/#node-selectors-and-gpu-labels","title":"Node selectors and GPU labels","text":"<p>Inference pods are scheduled to GPU nodes using the <code>k8s.amazonaws.com/accelerator</code> node label. This label must be present on GPU nodes before endpoints can be created. The GPU types referenced across model engine configuration:</p> Label value GPU <code>nvidia-ampere-a10</code> NVIDIA A10 <code>nvidia-ampere-a100</code> NVIDIA A100 <code>nvidia-tesla-t4</code> NVIDIA T4 <code>nvidia-hopper-h100</code> NVIDIA H100 (full) <code>nvidia-hopper-h100-1g20gb</code> NVIDIA H100 (MIG 1g.20gb) <code>nvidia-hopper-h100-3g40gb</code> NVIDIA H100 (MIG 3g.40gb) <p>GPU nodes must have the <code>nvidia.com/gpu: NoSchedule</code> taint that GPU inference pods tolerate. The NVIDIA GPU Operator must be installed and the driver must be functional on every GPU node (<code>nvidia-smi</code> must succeed).</p>"},{"location":"internal/architecture/#balloon-pods-and-gpu-node-warming","title":"Balloon pods and GPU node warming","text":"<p>The <code>balloons</code> helm value creates one low-priority Deployment per accelerator type. Each balloon Deployment occupies a configurable number of replicas (<code>replicaCount</code>) on the corresponding node type, requesting GPU resources to prevent the cluster autoscaler from scaling down GPU nodes between inference workloads.</p> <p>The <code>balloonConfig.reserveHighPriority: true</code> flag restricts eviction to only high-priority pods. When a real inference pod is scheduled, it evicts balloon pods to claim GPU resources. Setting <code>replicaCount: 0</code> for a GPU type disables warming for that node type.</p> <pre><code># Example: keep 2 H100 nodes and 1 A10 node warm\nballoonConfig:\n  reserveHighPriority: true\n\nballoons:\n  - acceleratorName: nvidia-hopper-h100\n    replicaCount: 2\n    gpuCount: 4\n  - acceleratorName: nvidia-ampere-a10\n    replicaCount: 1\n  - acceleratorName: cpu\n    replicaCount: 0   # disabled\n</code></pre>"},{"location":"internal/architecture/#recommendedhardware-auto-selection","title":"<code>recommendedHardware</code> auto-selection","text":"<p>The <code>recommendedHardware</code> helm value provides two lookup tables used by the LLM endpoint service:</p> <ul> <li><code>byGpuMemoryGb</code>: Matches on <code>gpu_memory_le</code> (less-than-or-equal GB of model GPU memory). Selects GPU type, GPU count, CPU, memory, storage, and <code>nodes_per_worker</code>.</li> <li><code>byModelName</code>: Named overrides that take precedence over the <code>byGpuMemoryGb</code> table for specific models.</li> </ul> <pre><code>recommendedHardware:\n  byGpuMemoryGb:\n    - gpu_memory_le: 24\n      cpus: 10\n      gpus: 1\n      memory: 24Gi\n      storage: 80Gi\n      gpu_type: nvidia-ampere-a10\n      nodes_per_worker: 1\n    - gpu_memory_le: 180\n      cpus: 20\n      gpus: 2\n      memory: 160Gi\n      storage: 160Gi\n      gpu_type: nvidia-hopper-h100\n      nodes_per_worker: 1\n    - gpu_memory_le: 640\n      cpus: 80\n      gpus: 8\n      memory: 800Gi\n      storage: 640Gi\n      gpu_type: nvidia-hopper-h100\n      nodes_per_worker: 2       # triggers LWS creation\n  byModelName:\n    - name: deepseek-coder-v2\n      cpus: 160\n      gpus: 8\n      memory: 800Gi\n      storage: 640Gi\n      gpu_type: nvidia-hopper-h100\n      nodes_per_worker: 1\n</code></pre> <p>When <code>nodes_per_worker &gt; 1</code>, the LLM endpoint service creates a multi-node (LWS) endpoint instead of a regular Deployment. This is the mechanism by which large models are automatically placed on multi-node configurations without requiring the caller to specify resource details.</p>"},{"location":"internal/architecture/#imagecache","title":"<code>imageCache</code>","text":"<p>The <code>imageCache</code> helm value defines per-node-type image pre-pulling configuration. Each entry specifies a <code>nodeSelector</code> and optional tolerations matching a GPU node pool. Pre-pulling model images onto nodes reduces inference pod startup time. This is distinct from balloon pods: balloon pods keep nodes allocated; <code>imageCache</code> keeps images warm on those nodes.</p>"},{"location":"internal/architecture/#4-component-reference","title":"4. Component Reference","text":""},{"location":"internal/architecture/#41-k8s-cacher","title":"4.1 K8s Cacher","text":"<p>What it does: The K8s Cacher is a standalone Deployment (typically 1 replica) that runs a continuous polling loop. Every <code>sleep_interval_seconds</code> (default: 15 seconds), it:</p> <ol> <li>Reads the current state of all model endpoint Deployments and LeaderWorkerSets from the K8s API</li> <li>Writes endpoint status records to Redis with a TTL of <code>ttl_seconds</code> (default: 60 seconds)</li> <li>Updates the image cache state (for the <code>imageCache</code> feature)</li> </ol> <p>Why it exists: Direct K8s API calls from Gateway pods were unreliable at scale \u2014 requests would time out under load. The Cacher decouples Gateway reads from K8s API polling, with Redis as the intermediary. The Gateway reads exclusively from Redis for endpoint status; it never calls the K8s API for status lookups at request time.</p> <p>Code path: <pre><code>k8s_cache.py (main loop, --sleep-interval-seconds)\n  \u2514\u2500 ModelEndpointCacheWriteService.execute()\n       \u251c\u2500 LiveEndpointResourceGateway  \u2192  K8s API (reads Deployments / LWS)\n       \u2514\u2500 RedisModelEndpointCacheRepository.write(ttl=60s)\n</code></pre></p> <p>Startup behavior: The cacher calls <code>load_incluster_config()</code> first (for in-cluster operation), falling back to <code>load_kube_config()</code> for local development. It writes a readiness file after the first successful loop iteration to gate its <code>readinessProbe</code> \u2014 the pod is not considered ready until at least one cache cycle has completed successfully.</p> <p>Failure mode: Redis auth broken \u2192 endpoint status <code>unknown</code></p> <p>If the cacher cannot write to Redis \u2014 due to misconfigured Redis auth, network partition, or expired credentials \u2014 it fails silently from the Gateway's perspective. The Gateway reads stale or absent Redis entries and returns endpoint status as <code>\"unknown\"</code>, not an error and not <code>INITIALIZING</code>.</p> <p>This is the most deceptive failure mode in model engine. An endpoint may be fully <code>READY</code> and serving traffic, but the status API returns <code>\"unknown\"</code> indefinitely because the cacher-to-Redis path is broken.</p> <p>How to diagnose: check cacher pod logs for Redis connection errors. Verify Redis auth credentials and network reachability from the cacher pod. In smoke tests, the signature is: Service Builder logs show the endpoint reached <code>READY</code>, but <code>GET /v1/model-endpoints/{id}</code> returns <code>\"unknown\"</code> without ever transitioning.</p> <p>Parameters (configurable via CLI args, set in helm Deployment spec):</p> Parameter Default Description <code>--ttl-seconds</code> <code>60</code> Redis TTL for cache entries <code>--sleep-interval-seconds</code> <code>15</code> Poll interval between K8s API reads <code>--redis-url-override</code> None Override the Redis URL from <code>hmi_config.cache_redis_url</code> <p>TTL must be greater than sleep interval</p> <p>If <code>ttl_seconds &lt; sleep_interval_seconds</code>, cache entries expire between writes, causing cache misses on every Gateway status request. The cacher logs a warning if this condition is detected, but does not fail or exit. The default values (60s TTL, 15s interval) satisfy this requirement with a 4x margin.</p>"},{"location":"internal/architecture/#42-balloon-pods","title":"4.2 Balloon Pods","text":"<p>What they do: Balloon pods are low-priority Deployments that run an <code>ubuntu</code> container with an infinite sleep command. One Deployment exists per GPU type, configured via the <code>balloons</code> helm value. They request GPU resources, causing the cluster autoscaler to provision GPU nodes and keep them allocated even when no inference pods are running.</p> <p>Why they exist: GPU nodes are expensive to run continuously but slow to provision (5\u201315 minutes for a new node to join and be ready). Without balloon pods, the cluster autoscaler scales GPU nodes down during idle periods. When a new endpoint is created, the cluster must provision a fresh GPU node, and the 30-minute Celery task timeout (\u00a72.1) starts counting during this wait. Balloon pods eliminate this cold-start delay.</p> <p>How eviction works: Balloon pods are created with a low PriorityClass. When a real inference pod needs to be scheduled on a node occupied by a balloon pod, Kubernetes evicts the balloon pod (preemption). The <code>balloonConfig.reserveHighPriority: true</code> setting restricts preemption to only high-priority pods, preventing lower-priority workloads from accidentally evicting balloons and defeating the warming strategy.</p> <p>Configuration: <pre><code>balloonConfig:\n  reserveHighPriority: true\n\nballoons:\n  - acceleratorName: nvidia-ampere-a10\n    replicaCount: 1\n  - acceleratorName: nvidia-ampere-a100\n    replicaCount: 0       # disabled \u2014 no A100 node warming\n  - acceleratorName: nvidia-hopper-h100\n    replicaCount: 2\n    gpuCount: 4           # request 4 GPUs per balloon pod\n  - acceleratorName: cpu\n    replicaCount: 0\n</code></pre></p> <p><code>replicaCount: 0</code> disables a balloon type</p> <p>Setting <code>replicaCount: 0</code> for a GPU type disables node warming for that type. Cold-start delays will occur on the first endpoint creation after a period of inactivity on that GPU type. This is the default for all GPU types in <code>values_sample.yaml</code> \u2014 production deployments should set non-zero counts for GPU types in active use.</p>"},{"location":"internal/architecture/#43-multi-node-endpoints-lws","title":"4.3 Multi-node Endpoints (LWS)","text":"<p>What they are: Multi-node endpoints use <code>LeaderWorkerSet</code> (LWS), a Kubernetes CRD designed for distributed inference workloads that span multiple nodes. LWS is required for models too large to fit on a single node's GPU memory (e.g., 70B+ parameter models requiring more than 8 GPUs).</p> <p>How they differ from regular Deployments:</p> Aspect Regular Deployment LeaderWorkerSet K8s resource kind <code>Deployment</code> <code>LeaderWorkerSet</code> (CRD) Autoscaling HPA or KEDA None <code>min_workers</code> vs <code>max_workers</code> Can differ Must be equal Istio resources created VirtualService + DestinationRule ServiceEntry only K8s Service template <code>service.yaml</code> <code>lws-service.yaml</code> Scale-to-zero Supported (via KEDA) Not supported Capacity change Update <code>min_workers</code>/<code>max_workers</code> Delete and recreate <p>When LWS is used: The LLM endpoint service selects LWS automatically when <code>nodes_per_worker &gt; 1</code> in the matched <code>recommendedHardware</code> entry. It can also be specified explicitly in a <code>CreateModelEndpointV1Request</code> by setting <code>nodes_per_worker &gt; 1</code>.</p> <p>Resource creation differences in Service Builder: For LWS endpoints, the Service Builder takes a different code branch:</p> <ul> <li>Creates a <code>LeaderWorkerSet</code> resource instead of a <code>Deployment</code></li> <li>Creates the K8s Service from <code>lws-service.yaml</code> (not the standard <code>service.yaml</code>)</li> <li>If <code>istio_enabled: true</code>, creates a <code>ServiceEntry</code> (not a <code>VirtualService</code> or <code>DestinationRule</code>) \u2014 required because LWS routing uses direct IP address resolution rather than Istio's standard hostname-based VirtualService routing</li> <li>Does not create an HPA or KEDA ScaledObject</li> </ul> <p>Istio and LWS routing: LWS endpoints require a workaround for Istio. The Gateway manually resolves the K8s Service cluster IP and sends requests directly to that IP, bypassing Istio's standard VirtualService routing. A <code>ServiceEntry</code> is created to allow this direct IP traffic to pass through Istio's policy enforcement. See <code>live_sync_model_endpoint_inference_gateway.py</code> and <code>live_streaming_model_endpoint_inference_gateway.py</code> for the implementation details.</p> <p>No autoscaling for LWS endpoints</p> <p>LeaderWorkerSet endpoints cannot be autoscaled. <code>min_workers</code> must equal <code>max_workers</code> at creation time. If you need different capacity, delete the endpoint and recreate it with the desired worker count. This is a known limitation with no current workaround.</p>"},{"location":"internal/architecture/#appendix-key-configuration-values-quick-reference","title":"Appendix: Key Configuration Values Quick Reference","text":"<p>The values below have the highest operational impact. Full reference is in <code>helm-values.md</code>.</p> Value Default Risk Impact if wrong <code>db.runDbMigrationScript</code> <code>false</code> HIGH Schema errors on first deploy; no clear error surface <code>config.values.infra.prometheus_server_address</code> unset HIGH KEDA scale-to-zero silently broken <code>config.values.launch.vllm_repository</code> <code>vllm</code> (relative) HIGH Resolves to Scale's internal ECR in many envs; image pull fails silently <code>celeryBrokerType</code> <code>sqs</code> HIGH Wrong broker for cloud \u2192 async endpoints broken <code>config.values.infra.cloud_provider</code> <code>aws</code> HIGH Wrong storage, broker, and auth clients loaded for cloud <code>balloons[*].replicaCount</code> <code>0</code> MEDIUM No GPU node warming \u2192 cold-start delays; risks hitting 30-min Celery timeout <code>celery_autoscaler.enabled</code> <code>true</code> MEDIUM Async endpoints never scale if disabled <code>config.values.launch.istio_enabled</code> <code>true</code> MEDIUM Must match actual cluster Istio installation state exactly <p><code>db.runDbMigrationScript</code> defaults to <code>false</code></p> <p>On first install, the database schema must be initialized. The default <code>false</code> means the migration job does not run, resulting in schema errors at runtime that have no clear error surface. Set <code>db.runDbMigrationScript: true</code> on every first install into a new environment. There is an open TODO to change this default to <code>true</code>.</p>"},{"location":"internal/cloud-matrix/","title":"Cloud Support Matrix","text":"<p>Last updated: 2026-02-26 Owner: Platform / ML Infra</p> <p>This is the canonical reference for deploying model-engine into different cloud environments. It answers the question \"what do I set for cloud X?\" without requiring you to cross-reference other docs. For the installation runbook and step-by-step setup, see Confluence: Installing Model Engine.</p>"},{"location":"internal/cloud-matrix/#section-1-support-status","title":"Section 1: Support Status","text":"Feature AWS Azure GCP On-prem Message broker SQS \u2705 Azure Service Bus \u2705 Redis / Memorystore \u26a0\ufe0f Redis \u26a0\ufe0f Object storage S3 \u2705 Azure Blob Storage \u2705 GCS \u26a0\ufe0f S3-compatible (MinIO) \u26a0\ufe0f Image registry ECR \u2705 ACR \u2705 GAR \u26a0\ufe0f Custom \u26a0\ufe0f Redis auth URL or Secrets Manager \u2705 RBAC token via <code>DefaultAzureCredential</code> \u2705 Standard URL \u26a0\ufe0f Standard URL \u26a0\ufe0f Service account auth IRSA \u2705 Azure Workload Identity \u2705 GCP Workload Identity \u26a0\ufe0f \u2014 Tested end-to-end \u2705 \u2705 (partial) \u26a0\ufe0f \u26a0\ufe0f <p>Legend: - \u2705 Tested in production - \u26a0\ufe0f Implemented in code, not yet tested end-to-end in a real deployment - \u274c Not implemented</p>"},{"location":"internal/cloud-matrix/#section-2-per-cloud-configuration-reference","title":"Section 2: Per-Cloud Configuration Reference","text":"<p>All configuration enters the service through two mechanisms:</p> <ol> <li>Helm values \u2014 rendered into the service ConfigMap and environment variables by the chart templates.</li> <li>Service config YAML (<code>service_config.yaml</code>) \u2014 mounted into pods at runtime; its path is set via the <code>DEPLOY_SERVICE_CONFIG_PATH</code> environment variable.</li> </ol> <p>The <code>config.values</code> block in <code>values.yaml</code> is rendered directly into the service ConfigMap and maps to the fields of <code>HostedModelInferenceServiceConfig</code> (<code>model_engine_server/common/config.py</code>). The <code>infra</code> sub-block maps to <code>InfraConfig</code> (<code>model_engine_server/core/config.py</code>).</p>"},{"location":"internal/cloud-matrix/#aws-reference-configuration","title":"AWS (Reference Configuration)","text":"<p>AWS is the primary supported environment. The configuration below is the complete reference; all other clouds show only what differs from this.</p>"},{"location":"internal/cloud-matrix/#broker","title":"Broker","text":"<pre><code>celeryBrokerType: sqs\n\nconfig:\n  values:\n    launch:\n      sqs_profile: default\n      sqs_queue_policy_template: &gt;\n        {\n          \"Version\": \"2012-10-17\",\n          \"Id\": \"__default_policy_ID\",\n          \"Statement\": [\n            {\n              \"Sid\": \"__owner_statement\",\n              \"Effect\": \"Allow\",\n              \"Principal\": {\"AWS\": \"arn:aws:iam::000000000000:root\"},\n              \"Action\": \"sqs:*\",\n              \"Resource\": \"arn:aws:sqs:us-east-1:000000000000:${queue_name}\"\n            },\n            {\n              \"Effect\": \"Allow\",\n              \"Principal\": {\"AWS\": \"arn:aws:iam::000000000000:role/k8s-main-llm-engine\"},\n              \"Action\": \"sqs:*\",\n              \"Resource\": \"arn:aws:sqs:us-east-1:000000000000:${queue_name}\"\n            }\n          ]\n        }\n      sqs_queue_tag_template: &gt;\n        {\n          \"Spellbook-Serve-Endpoint-Id\": \"${endpoint_id}\",\n          \"Spellbook-Serve-Endpoint-Name\": \"${endpoint_name}\",\n          \"Spellbook-Serve-Endpoint-Created-By\": \"${endpoint_created_by}\"\n        }\n</code></pre> <p>SQS queues are created automatically per endpoint by <code>SQSQueueEndpointResourceDelegate</code>. No pre-provisioning required.</p>"},{"location":"internal/cloud-matrix/#object-storage","title":"Object Storage","text":"<pre><code>config:\n  values:\n    infra:\n      s3_bucket: llm-engine\n      default_region: us-east-1\n    launch:\n      s3_file_llm_fine_tuning_job_repository: \"s3://llm-engine/llm-ft-job-repository\"\n      hf_user_fine_tuned_weights_prefix: \"s3://llm-engine/fine_tuned_weights\"\n      batch_inference_vllm_repository: \"llm-engine/batch-infer-vllm\"\n</code></pre> <p>Auth is handled by IRSA \u2014 the IAM role attached to the service account is used transparently by boto3.</p>"},{"location":"internal/cloud-matrix/#redis","title":"Redis","text":"<p>Exactly one of the following must be set:</p> <pre><code>config:\n  values:\n    launch:\n      # Option A \u2014 direct URL\n      cache_redis_aws_url: redis://llm-engine-prod-cache.use1.cache.amazonaws.com:6379/15\n\n      # Option B \u2014 Secrets Manager (recommended for production)\n      # Secret must contain a key \"cache-url\" with the full Redis URL including db number\n      cache_redis_aws_secret_name: sample-prod/redis-credentials\n</code></pre> <p>Also set the matching <code>redis_host</code> under <code>infra</code> for the Celery autoscaler:</p> <pre><code>config:\n  values:\n    infra:\n      redis_host: llm-engine-prod-cache.use1.cache.amazonaws.com\n</code></pre>"},{"location":"internal/cloud-matrix/#service-account-irsa","title":"Service Account (IRSA)","text":"<pre><code>serviceAccount:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/k8s-main-llm-engine\n\nimageBuilderServiceAccount:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/k8s-main-llm-engine-image-builder\n\nserviceTemplate:\n  createServiceAccount: true\n  serviceAccountName: model-engine\n  serviceAccountAnnotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::000000000000:role/llm-engine\n</code></pre>"},{"location":"internal/cloud-matrix/#image-registry-ecr","title":"Image Registry (ECR)","text":"<pre><code>image:\n  gatewayRepository: 000000000000.dkr.ecr.us-east-1.amazonaws.com/model-engine\n  builderRepository: 000000000000.dkr.ecr.us-east-1.amazonaws.com/model-engine\n  cacherRepository:  000000000000.dkr.ecr.us-east-1.amazonaws.com/model-engine\n  forwarderRepository: 000000000000.dkr.ecr.us-east-1.amazonaws.com/model-engine\n\nconfig:\n  values:\n    infra:\n      docker_repo_prefix: \"000000000000.dkr.ecr.us-east-1.amazonaws.com\"\n      ml_account_id: \"000000000000\"\n    launch:\n      vllm_repository: \"000000000000.dkr.ecr.us-east-1.amazonaws.com/vllm\"\n      tensorrt_llm_repository: \"000000000000.dkr.ecr.us-east-1.amazonaws.com/tensorrt-llm\"\n</code></pre>"},{"location":"internal/cloud-matrix/#cloud-provider-flag","title":"Cloud Provider Flag","text":"<pre><code>config:\n  values:\n    infra:\n      cloud_provider: aws\n</code></pre> <p>This is the master switch. It controls which storage client, broker, and image registry class are instantiated at runtime (see <code>model_engine_server/api/dependencies.py</code>).</p>"},{"location":"internal/cloud-matrix/#azure","title":"Azure","text":"<p>The following values differ from the AWS reference configuration. All other values (autoscaling, balloons, networking, etc.) remain the same.</p>"},{"location":"internal/cloud-matrix/#broker_1","title":"Broker","text":"<pre><code>celeryBrokerType: servicebus\n\nazure:\n  servicebus_namespace: my-servicebus-namespace  # the part before .servicebus.windows.net\n</code></pre> <p>The broker URL is constructed as:</p> <pre><code>azureservicebus://DefaultAzureCredential@{servicebus_namespace}.servicebus.windows.net\n</code></pre> <p>Auth uses <code>DefaultAzureCredential</code>, which picks up the Workload Identity token automatically. No connection string or SAS token is needed.</p> <p>Azure Service Bus drops idle AMQP connections after 300 seconds</p> <p>Azure Service Bus force-closes idle AMQP connections with <code>amqp:connection:forced</code> after 300 seconds of inactivity. This manifests as a 503 error on the first inference request following an idle period \u2014 it looks like a random backend failure, not a configuration problem, which makes it very hard to diagnose without knowing about this behavior.</p> <p>Fix (applied in commits #765 and #767): The Celery app now sends AMQP keepalive heartbeats every 30 seconds via <code>uamqp_keep_alive_interval</code>, uses Celery's default connection pool (limit=10) to keep connections alive between requests, and enables <code>broker_connection_retry</code> so producers reconnect transparently on stale connections.</p> <p>The keepalive interval is configurable via the <code>SERVICEBUS_KEEP_ALIVE_INTERVAL</code> environment variable (default: <code>30</code>).</p> <p>Minimum required version: <code>azure-servicebus &gt;= 7.14.3</code>. Version 7.11.4 has a known regression (azure-sdk-for-python#34212) where idle AMQP connections are not properly managed.</p> <p>If you are seeing intermittent 503s on Azure after idle periods, verify you are running a version of model-engine that includes commit <code>9deb59f1</code> or later.</p>"},{"location":"internal/cloud-matrix/#object-storage-azure-blob-storage","title":"Object Storage (Azure Blob Storage)","text":"<pre><code>azure:\n  abs_account_name: mystorageaccount\n  abs_container_name: llm-engine\n\nconfig:\n  values:\n    launch:\n      s3_file_llm_fine_tuning_job_repository: \"az://llm-engine/llm-ft-job-repository\"\n      hf_user_fine_tuned_weights_prefix: \"az://llm-engine/fine_tuned_weights\"\n</code></pre> <p><code>ABS_ACCOUNT_NAME</code> and <code>ABS_CONTAINER_NAME</code> are injected as environment variables by the chart templates (<code>_helpers.tpl</code>). Storage auth uses <code>DefaultAzureCredential</code> (Workload Identity).</p> <p>From <code>model_engine_server/common/io.py</code>, the <code>open_wrapper</code> function switches to <code>BlobServiceClient</code> when <code>cloud_provider == \"azure\"</code>:</p> <pre><code>client = BlobServiceClient(\n    f\"https://{os.getenv('ABS_ACCOUNT_NAME')}.blob.core.windows.net\",\n    DefaultAzureCredential(),\n)\n</code></pre>"},{"location":"internal/cloud-matrix/#redis_1","title":"Redis","text":"<pre><code>config:\n  values:\n    launch:\n      cache_redis_azure_host: my-redis-cache.redis.cache.windows.net:6380\n</code></pre> <p>Do not set <code>cache_redis_aws_url</code> or <code>cache_redis_aws_secret_name</code> for Azure. The <code>cache_redis_url</code> property in <code>HostedModelInferenceServiceConfig</code> detects <code>cache_redis_azure_host</code> and builds a <code>rediss://</code> URL using an RBAC token fetched at runtime via <code>DefaultAzureCredential</code>:</p> <pre><code>username = os.getenv(\"AZURE_OBJECT_ID\")\ntoken = DefaultAzureCredential().get_token(\"https://redis.azure.com/.default\")\npassword = token.token\nreturn f\"rediss://{username}:{password}@{self.cache_redis_azure_host}\"\n</code></pre> <p>The token expiry timestamp is tracked globally and the aioredis connection pool is recreated automatically when the token expires (<code>get_or_create_aioredis_pool</code> in <code>dependencies.py</code>).</p> <pre><code>azure:\n  object_id: 00000000-0000-0000-0000-000000000000   # injected as AZURE_OBJECT_ID\n  client_id: 00000000-0000-0000-0000-000000000000   # used for Workload Identity\n</code></pre>"},{"location":"internal/cloud-matrix/#service-account-azure-workload-identity","title":"Service Account (Azure Workload Identity)","text":"<pre><code>azure:\n  identity_name: my-managed-identity\n  client_id: 00000000-0000-0000-0000-000000000000\n\nserviceAccount:\n  annotations:\n    azure.workload.identity/client-id: 00000000-0000-0000-0000-000000000000\n</code></pre> <p>The chart injects <code>AZURE_CLIENT_ID</code> and <code>AZURE_OBJECT_ID</code> as environment variables into all pods. KEDA uses a <code>TriggerAuthentication</code> resource with <code>provider: azure-workload</code> and the same <code>client_id</code>.</p>"},{"location":"internal/cloud-matrix/#secret-management-azure-key-vault","title":"Secret Management (Azure Key Vault)","text":"<pre><code>keyvaultName: my-llm-engine-keyvault\n\nazure:\n  keyvault_name: my-llm-engine-keyvault  # injected as KEYVAULT_NAME env var\n</code></pre> <p>Key Vault is required for Azure deployments. Database credentials can be pulled from Key Vault via <code>cloudDatabaseSecretName</code>, or supplied as a Kubernetes secret:</p> <pre><code>secrets:\n  cloudDatabaseSecretName: my-db-secret          # Key Vault secret name\n  # OR\n  kubernetesDatabaseSecretName: llm-engine-postgres-credentials  # K8s secret (simpler)\n</code></pre>"},{"location":"internal/cloud-matrix/#image-registry-acr","title":"Image Registry (ACR)","text":"<pre><code>image:\n  gatewayRepository: myregistry.azurecr.io/model-engine\n  builderRepository: myregistry.azurecr.io/model-engine\n  cacherRepository:  myregistry.azurecr.io/model-engine\n  forwarderRepository: myregistry.azurecr.io/model-engine\n\nconfig:\n  values:\n    infra:\n      docker_repo_prefix: \"myregistry.azurecr.io\"\n    launch:\n      vllm_repository: \"myregistry.azurecr.io/vllm\"\n      tensorrt_llm_repository: \"myregistry.azurecr.io/tensorrt-llm\"\n</code></pre>"},{"location":"internal/cloud-matrix/#cloud-provider-flag_1","title":"Cloud Provider Flag","text":"<pre><code>config:\n  values:\n    infra:\n      cloud_provider: azure\n</code></pre>"},{"location":"internal/cloud-matrix/#gcp","title":"GCP","text":"<p>GCP is not tested end-to-end</p> <p>GCP support is implemented in code (added in commit #750, <code>f436d25e</code>) but has not been validated in a real deployment. These values are derived from the source code. Treat this section as a starting point; expect to debug and iterate. When you complete a GCP deployment, update this section and write a Confluence runbook.</p> <p>The following values differ from the AWS reference configuration.</p>"},{"location":"internal/cloud-matrix/#broker_2","title":"Broker","text":"<p>GCP uses Redis (Google Memorystore) as the Celery broker. This is the legacy broker path \u2014 Redis was replaced by SQS (AWS) and Azure Service Bus (Azure) due to reliability limitations at scale. See Section 3 for known limitations.</p> <pre><code>celery_broker_type_redis: true  # or celeryBrokerType: elasticache\n\nredisHost: my-memorystore-instance.redis.cache.googleapis.com\nredisPort: \"6379\"\n</code></pre> <p>The chart injects <code>REDIS_HOST</code> and <code>REDIS_PORT</code> as environment variables. <code>RedisQueueEndpointResourceDelegate</code> is selected for GCP in <code>dependencies.py</code>:</p> <pre><code>elif infra_config().cloud_provider == \"gcp\":\n    queue_delegate = RedisQueueEndpointResourceDelegate(redis_client=redis_client)\n</code></pre>"},{"location":"internal/cloud-matrix/#object-storage-gcs","title":"Object Storage (GCS)","text":"<pre><code>config:\n  values:\n    launch:\n      s3_file_llm_fine_tuning_job_repository: \"gs://my-bucket/llm-ft-job-repository\"\n      hf_user_fine_tuned_weights_prefix: \"gs://my-bucket/fine_tuned_weights\"\n</code></pre> <p>Storage auth uses GCP Workload Identity via Application Default Credentials (ADC). <code>GCSFilesystemGateway</code> uses <code>gcloud-aio-storage</code> and <code>google-cloud-storage</code> \u2014 no additional credential configuration is needed if Workload Identity is properly configured on the node pool.</p>"},{"location":"internal/cloud-matrix/#redis-gcp-memorystore","title":"Redis (GCP Memorystore)","text":"<pre><code>config:\n  values:\n    launch:\n      cache_redis_onprem_url: redis://my-memorystore-instance.redis.cache.googleapis.com:6379/0\n</code></pre> <p>Alternatively, <code>cache_redis_aws_url</code> is also accepted (the on-prem fallback path in <code>config.py</code> accepts it with a log warning). The same Redis instance serves as both the caching layer and the Celery broker.</p>"},{"location":"internal/cloud-matrix/#service-account-gcp-workload-identity","title":"Service Account (GCP Workload Identity)","text":"<pre><code>serviceAccount:\n  annotations:\n    iam.gke.io/gcp-service-account: model-engine@my-project.iam.gserviceaccount.com\n</code></pre> <p>The specific annotation format depends on your GKE setup (standard Workload Identity vs. Workload Identity Federation). ADC picks up the bound service account automatically.</p>"},{"location":"internal/cloud-matrix/#image-registry-gar","title":"Image Registry (GAR)","text":"<pre><code>image:\n  gatewayRepository: us-docker.pkg.dev/my-project/my-repo/model-engine\n  builderRepository: us-docker.pkg.dev/my-project/my-repo/model-engine\n  cacherRepository:  us-docker.pkg.dev/my-project/my-repo/model-engine\n  forwarderRepository: us-docker.pkg.dev/my-project/my-repo/model-engine\n\nconfig:\n  values:\n    infra:\n      # Must be parseable as {location}-docker.pkg.dev/{project}/{repository}\n      docker_repo_prefix: \"us-docker.pkg.dev/my-project/my-repo\"\n    launch:\n      vllm_repository: \"us-docker.pkg.dev/my-project/my-repo/vllm\"\n      tensorrt_llm_repository: \"us-docker.pkg.dev/my-project/my-repo/tensorrt-llm\"\n</code></pre> <p><code>GARDockerRepository</code> parses <code>docker_repo_prefix</code> by splitting on <code>/</code> to extract <code>(location, project, repository)</code> and calls the Artifact Registry API using ADC. Image builds are not supported via GAR \u2014 <code>build_image</code> raises <code>NotImplementedError</code>.</p>"},{"location":"internal/cloud-matrix/#cloud-provider-flag_2","title":"Cloud Provider Flag","text":"<pre><code>config:\n  values:\n    infra:\n      cloud_provider: gcp\n</code></pre>"},{"location":"internal/cloud-matrix/#on-prem","title":"On-prem","text":"<p>On-prem is not tested end-to-end</p> <p>On-prem support is implemented in code (added in commit #744, <code>ecc8ff42</code>) but has not been validated in a full production deployment. Known limitations: Redis-only broker (no managed queue service), no managed secret storage, NVIDIA driver setup is not covered here. Treat this section as a starting point.</p> <p>On-prem uses Redis as both the Celery broker and the caching layer, and S3-compatible object storage (typically MinIO).</p>"},{"location":"internal/cloud-matrix/#broker_3","title":"Broker","text":"<p>On-prem uses <code>OnPremQueueEndpointResourceDelegate</code>, selected in <code>dependencies.py</code> when <code>cloud_provider == \"onprem\"</code>. This is the legacy broker path \u2014 see Section 3 for known limitations.</p> <pre><code>celery_broker_type_redis: true  # or celeryBrokerType: elasticache\n\nredisHost: redis.my-namespace.svc.cluster.local\nredisPort: \"6379\"\n</code></pre>"},{"location":"internal/cloud-matrix/#object-storage-s3-compatible-minio","title":"Object Storage (S3-compatible / MinIO)","text":"<pre><code>s3EndpointUrl: http://minio.my-namespace.svc.cluster.local:9000\n\nconfig:\n  values:\n    infra:\n      s3_bucket: llm-engine\n    launch:\n      s3_file_llm_fine_tuning_job_repository: \"s3://llm-engine/llm-ft-job-repository\"\n      hf_user_fine_tuned_weights_prefix: \"s3://llm-engine/fine_tuned_weights\"\n</code></pre> <p><code>S3_ENDPOINT_URL</code> is injected as an environment variable when <code>s3EndpointUrl</code> is set in helm values. boto3 and smart_open pick this up automatically, redirecting all S3 calls to the MinIO endpoint.</p> <p>The code in <code>dependencies.py</code> falls through to <code>S3FilesystemGateway</code> and <code>S3LLMArtifactGateway</code> for any <code>cloud_provider</code> value that is not <code>azure</code> or <code>gcp</code> \u2014 this covers both <code>aws</code> and <code>onprem</code>:</p> <pre><code>else:\n    # AWS uses S3, on-prem uses MinIO (S3-compatible)\n    filesystem_gateway = S3FilesystemGateway()\n    llm_artifact_gateway = S3LLMArtifactGateway()\n</code></pre>"},{"location":"internal/cloud-matrix/#redis_2","title":"Redis","text":"<pre><code>config:\n  values:\n    launch:\n      cache_redis_onprem_url: redis://redis.my-namespace.svc.cluster.local:6379/0\n</code></pre> <p>If <code>cache_redis_onprem_url</code> is not set, the code falls back to the <code>REDIS_HOST</code> / <code>REDIS_PORT</code> environment variables, then defaults to <code>redis://redis:6379/0</code>. The same Redis instance serves as both the caching layer and the Celery broker.</p>"},{"location":"internal/cloud-matrix/#service-account","title":"Service Account","text":"<p>On-prem has no managed IAM system. No service account annotations are required if your pods access storage via static MinIO credentials or in-cluster service accounts with appropriate RBAC. Configure MinIO credentials via environment variables or a Kubernetes secret as appropriate for your setup.</p>"},{"location":"internal/cloud-matrix/#secret-management","title":"Secret Management","text":"<p>On-prem has no managed secret store. Use Kubernetes secrets directly:</p> <pre><code>secrets:\n  kubernetesDatabaseSecretName: llm-engine-postgres-credentials\n</code></pre>"},{"location":"internal/cloud-matrix/#image-registry-custom","title":"Image Registry (Custom)","text":"<pre><code>image:\n  gatewayRepository: my-registry.internal/model-engine\n  builderRepository: my-registry.internal/model-engine\n  cacherRepository:  my-registry.internal/model-engine\n  forwarderRepository: my-registry.internal/model-engine\n\nconfig:\n  values:\n    infra:\n      docker_repo_prefix: \"my-registry.internal\"\n    launch:\n      vllm_repository: \"my-registry.internal/vllm\"\n      tensorrt_llm_repository: \"my-registry.internal/tensorrt-llm\"\n</code></pre> <p><code>OnPremDockerRepository</code> is selected when <code>cloud_provider == \"onprem\"</code>. Image pulls use the standard Kubernetes image pull mechanism with whatever credentials are configured in your registry secret.</p>"},{"location":"internal/cloud-matrix/#cloud-provider-flag_3","title":"Cloud Provider Flag","text":"<pre><code>config:\n  values:\n    infra:\n      cloud_provider: onprem\n</code></pre>"},{"location":"internal/cloud-matrix/#section-3-key-behavior-differences","title":"Section 3: Key Behavior Differences","text":"Behavior AWS Azure GCP On-prem Async broker SQS \u2014 queue per endpoint, auto-created Azure Service Bus \u2014 topic per endpoint Redis / Memorystore Redis Queue delegate class <code>SQSQueueEndpointResourceDelegate</code> <code>ASBQueueEndpointResourceDelegate</code> <code>RedisQueueEndpointResourceDelegate</code> <code>OnPremQueueEndpointResourceDelegate</code> Inference task queue <code>CeleryTaskQueueGateway(BrokerType.SQS)</code> <code>CeleryTaskQueueGateway(BrokerType.SERVICEBUS)</code> <code>CeleryTaskQueueGateway(BrokerType.REDIS)</code> <code>CeleryTaskQueueGateway(BrokerType.REDIS_24H)</code> KEDA autoscaling trigger Redis list (via <code>redis_host</code>) Azure Service Bus queue Redis list Redis list Object storage client <code>S3FilesystemGateway</code> / <code>S3LLMArtifactGateway</code> <code>ABSFilesystemGateway</code> / <code>ABSLLMArtifactGateway</code> <code>GCSFilesystemGateway</code> / <code>GCSLLMArtifactGateway</code> <code>S3FilesystemGateway</code> (via MinIO) Storage auth IAM / IRSA (boto3 transparent) <code>DefaultAzureCredential</code> (Workload Identity) ADC (GCP Workload Identity) Static credentials / MinIO config Redis auth Plain URL or Secrets Manager RBAC token via <code>DefaultAzureCredential</code>, refreshed on expiry Standard URL Standard URL Secret management AWS Secrets Manager Azure Key Vault (<code>keyvaultName</code> required) \u2014 (ADC only) K8s secrets only Image registry class <code>ECRDockerRepository</code> <code>ACRDockerRepository</code> <code>GARDockerRepository</code> <code>OnPremDockerRepository</code> Inference autoscaling metrics <code>RedisInferenceAutoscalingMetricsGateway</code> <code>ASBInferenceAutoscalingMetricsGateway</code> <code>RedisInferenceAutoscalingMetricsGateway</code> <code>RedisInferenceAutoscalingMetricsGateway</code>"},{"location":"internal/cloud-matrix/#azure-service-bus-idle-connection-drop-critical","title":"Azure Service Bus \u2014 Idle Connection Drop (Critical)","text":"<p>Azure Service Bus idle AMQP connections \u2014 root cause of intermittent 503s</p> <p>Azure Service Bus force-closes idle AMQP connections after 300 seconds with an <code>amqp:connection:forced</code> error. The symptom is a 503 on the first inference request after a quiet period. It looks like a flaky backend failure, not a configuration problem \u2014 which makes it very hard to diagnose without knowing about this behavior.</p> <p>Timeline of the fix:</p> <ul> <li>Commit #765 (<code>1fefec11</code>): Added <code>uamqp_keep_alive_interval=30</code> to send AMQP heartbeats, added retry policy (<code>retry_total=3</code>, <code>retry_backoff_factor=0.8</code>, <code>retry_backoff_max=120</code>), enabled <code>broker_connection_retry</code> and <code>broker_connection_retry_on_startup</code>, and set <code>broker_pool_limit=0</code> to avoid reusing connections Azure had already closed.</li> <li>Commit #767 (<code>9deb59f1</code>): Reverted <code>broker_pool_limit=0</code>. Setting pool limit to 0 destroys pool connections after each request, orphaning kombu's cached <code>ServiceBusClient</code> and sender objects \u2014 meaning keepalive heartbeats can never flow. The correct fix is the default Celery connection pool (limit=10) combined with <code>uamqp_keep_alive_interval=30</code>. Also bumped <code>azure-servicebus</code> from <code>7.11.4</code> to <code>7.14.3</code> to fix a known SDK regression.</li> </ul> <p>Current behavior (post #767): Keepalive heartbeats flow every 30 seconds over pooled connections. Azure never reaches the 300s idle timeout.</p> <p>Configuration: The keepalive interval is overridable via the <code>SERVICEBUS_KEEP_ALIVE_INTERVAL</code> environment variable (default: <code>30</code>).</p> <p>Minimum required dependency: <code>azure-servicebus &gt;= 7.14.3</code>. Version 7.11.4 has a known regression (azure-sdk-for-python#34212) that prevents idle connection management from working correctly even with the code changes above.</p> <p>Diagnosis: If you see intermittent 503s on Azure after idle periods, check gateway and builder pod logs for <code>amqp:connection:forced</code>. Confirm you are running a model-engine image that includes commit <code>9deb59f1</code>.</p>"},{"location":"internal/cloud-matrix/#gcp-and-on-prem-redis-as-broker-legacy-path","title":"GCP and On-prem \u2014 Redis as Broker (Legacy Path)","text":"<p>Redis broker is the legacy path \u2014 known reliability limitations at scale</p> <p>Both GCP and on-prem use Redis as the Celery broker. Redis was the original broker for all clouds, but was replaced by SQS on AWS and Azure Service Bus on Azure due to reliability and stability issues at scale.</p> <p>Known limitations of Redis-as-broker:</p> <ul> <li>No dead-letter queue \u2014 failed tasks are lost unless the worker explicitly retries.</li> <li>No per-message visibility timeout \u2014 if a worker dies mid-task, the message stays invisible until the Redis key expires or is manually cleared.</li> <li>Queue lifecycle management is handled in-process by the queue delegate (not by a managed service), which is less reliable under high load or during pod restarts.</li> <li>KEDA autoscaling for async endpoints uses a Redis list trigger rather than a managed queue trigger. This code path is less battle-tested.</li> </ul> <p>If you are deploying at scale on GCP, evaluate whether a managed broker (Cloud Pub/Sub, or a managed Redis with stronger delivery guarantees) is feasible before committing to the Redis broker path.</p>"},{"location":"internal/cloud-matrix/#section-4-image-mirroring-requirements","title":"Section 4: Image Mirroring Requirements","text":"<p>Image mirroring is the #1 silent deployment failure</p> <p>Model-engine defaults to pulling images from Scale's internal ECR (<code>public.ecr.aws/b2z8n5q1/</code>). In any customer or non-Scale environment, these pulls will either fail outright or pull images that do not match your pinned version.</p> <p>Failure mode if mirroring is skipped: Endpoint creation returns HTTP 200. The endpoint stays in <code>INITIALIZING</code> forever. There is no clear error message surfaced by the API \u2014 the image pull failure is buried in the pod events of the inference pod, which is created by Service Builder in the endpoint namespace.</p> <p>Always mirror images before running <code>helm install</code>.</p>"},{"location":"internal/cloud-matrix/#why-mirroring-is-required","title":"Why Mirroring Is Required","text":"<p>The helm chart and service config have several hardcoded or defaulted image references pointing to Scale's ECR:</p> Config key Default value What it affects <code>image.gatewayRepository</code> <code>public.ecr.aws/b2z8n5q1/model-engine</code> Gateway pod image pull <code>image.builderRepository</code> <code>public.ecr.aws/b2z8n5q1/model-engine</code> Builder pod image pull <code>image.cacherRepository</code> <code>public.ecr.aws/b2z8n5q1/model-engine</code> Cacher pod image pull <code>image.forwarderRepository</code> <code>public.ecr.aws/b2z8n5q1/model-engine</code> Forwarder container in inference pods <code>config.values.launch.vllm_repository</code> <code>vllm</code> (relative, resolves against <code>docker_repo_prefix</code>) vLLM image for LLM endpoints <code>config.values.launch.tensorrt_llm_repository</code> <code>tensorrt-llm</code> TensorRT-LLM image <code>config.values.launch.batch_inference_vllm_repository</code> <code>llm-engine/batch-infer-vllm</code> Batch inference image"},{"location":"internal/cloud-matrix/#images-to-mirror","title":"Images to Mirror","text":"Image name Source path Notes <code>model-engine</code> <code>public.ecr.aws/b2z8n5q1/model-engine</code> Used for gateway, builder, cacher, and forwarder <code>vllm</code> <code>public.ecr.aws/b2z8n5q1/vllm</code> Used for LLM inference endpoints <code>tensorrt-llm</code> <code>public.ecr.aws/b2z8n5q1/tensorrt-llm</code> Used for TensorRT-LLM inference endpoints <code>batch-infer-vllm</code> <code>public.ecr.aws/b2z8n5q1/llm-engine/batch-infer-vllm</code> Used for batch inference jobs"},{"location":"internal/cloud-matrix/#step-by-step-mirroring-process","title":"Step-by-Step Mirroring Process","text":""},{"location":"internal/cloud-matrix/#step-1-authenticate-to-the-source-scales-public-ecr","title":"Step 1: Authenticate to the source (Scale's public ECR)","text":"<pre><code>aws ecr-public get-login-password --region us-east-1 \\\n  | docker login --username AWS --password-stdin public.ecr.aws\n</code></pre>"},{"location":"internal/cloud-matrix/#step-2-pull-the-images","title":"Step 2: Pull the images","text":"<pre><code>export ME_TAG=&lt;model-engine image tag, e.g. 60ac144c55aad971cdd7f152f4f7816ce2fb7d2f&gt;\nexport VLLM_TAG=&lt;vllm image tag&gt;\nexport TRT_TAG=&lt;tensorrt-llm image tag&gt;\nexport BATCH_TAG=&lt;batch-infer-vllm image tag&gt;\n\ndocker pull public.ecr.aws/b2z8n5q1/model-engine:${ME_TAG}\ndocker pull public.ecr.aws/b2z8n5q1/vllm:${VLLM_TAG}\ndocker pull public.ecr.aws/b2z8n5q1/tensorrt-llm:${TRT_TAG}\ndocker pull public.ecr.aws/b2z8n5q1/llm-engine/batch-infer-vllm:${BATCH_TAG}\n</code></pre>"},{"location":"internal/cloud-matrix/#step-3-tag-and-push-to-your-registry","title":"Step 3: Tag and push to your registry","text":"<p>AWS ECR</p> <pre><code># Authenticate\naws ecr get-login-password --region us-east-1 \\\n  | docker login --username AWS --password-stdin 000000000000.dkr.ecr.us-east-1.amazonaws.com\n\n# Create repositories if they don't exist\naws ecr create-repository --repository-name model-engine --region us-east-1\naws ecr create-repository --repository-name vllm --region us-east-1\naws ecr create-repository --repository-name tensorrt-llm --region us-east-1\naws ecr create-repository --repository-name llm-engine/batch-infer-vllm --region us-east-1\n\nexport REGISTRY=000000000000.dkr.ecr.us-east-1.amazonaws.com\n\ndocker tag public.ecr.aws/b2z8n5q1/model-engine:${ME_TAG} ${REGISTRY}/model-engine:${ME_TAG}\ndocker push ${REGISTRY}/model-engine:${ME_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/vllm:${VLLM_TAG} ${REGISTRY}/vllm:${VLLM_TAG}\ndocker push ${REGISTRY}/vllm:${VLLM_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/tensorrt-llm:${TRT_TAG} ${REGISTRY}/tensorrt-llm:${TRT_TAG}\ndocker push ${REGISTRY}/tensorrt-llm:${TRT_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/llm-engine/batch-infer-vllm:${BATCH_TAG} \\\n  ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\ndocker push ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\n</code></pre> <p>Azure ACR</p> <pre><code># Authenticate\naz acr login --name myregistry\n\nexport REGISTRY=myregistry.azurecr.io\n\ndocker tag public.ecr.aws/b2z8n5q1/model-engine:${ME_TAG} ${REGISTRY}/model-engine:${ME_TAG}\ndocker push ${REGISTRY}/model-engine:${ME_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/vllm:${VLLM_TAG} ${REGISTRY}/vllm:${VLLM_TAG}\ndocker push ${REGISTRY}/vllm:${VLLM_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/tensorrt-llm:${TRT_TAG} ${REGISTRY}/tensorrt-llm:${TRT_TAG}\ndocker push ${REGISTRY}/tensorrt-llm:${TRT_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/llm-engine/batch-infer-vllm:${BATCH_TAG} \\\n  ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\ndocker push ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\n</code></pre> <p>GCP GAR</p> <pre><code># Authenticate\ngcloud auth configure-docker us-docker.pkg.dev\n\nexport REGISTRY=us-docker.pkg.dev/my-project/my-repo\n\ndocker tag public.ecr.aws/b2z8n5q1/model-engine:${ME_TAG} ${REGISTRY}/model-engine:${ME_TAG}\ndocker push ${REGISTRY}/model-engine:${ME_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/vllm:${VLLM_TAG} ${REGISTRY}/vllm:${VLLM_TAG}\ndocker push ${REGISTRY}/vllm:${VLLM_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/tensorrt-llm:${TRT_TAG} ${REGISTRY}/tensorrt-llm:${TRT_TAG}\ndocker push ${REGISTRY}/tensorrt-llm:${TRT_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/llm-engine/batch-infer-vllm:${BATCH_TAG} \\\n  ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\ndocker push ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\n</code></pre> <p>On-prem / Custom registry</p> <pre><code>export REGISTRY=my-registry.internal\n\n# Authenticate using your registry's mechanism (e.g. docker login my-registry.internal)\n\ndocker tag public.ecr.aws/b2z8n5q1/model-engine:${ME_TAG} ${REGISTRY}/model-engine:${ME_TAG}\ndocker push ${REGISTRY}/model-engine:${ME_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/vllm:${VLLM_TAG} ${REGISTRY}/vllm:${VLLM_TAG}\ndocker push ${REGISTRY}/vllm:${VLLM_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/tensorrt-llm:${TRT_TAG} ${REGISTRY}/tensorrt-llm:${TRT_TAG}\ndocker push ${REGISTRY}/tensorrt-llm:${TRT_TAG}\n\ndocker tag public.ecr.aws/b2z8n5q1/llm-engine/batch-infer-vllm:${BATCH_TAG} \\\n  ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\ndocker push ${REGISTRY}/llm-engine/batch-infer-vllm:${BATCH_TAG}\n</code></pre>"},{"location":"internal/cloud-matrix/#step-4-update-helm-values","title":"Step 4: Update helm values","text":"<p>After mirroring, set the following in your <code>values.yaml</code> before running <code>helm install</code>:</p> <pre><code>tag: &lt;model-engine-tag&gt;\n\nimage:\n  gatewayRepository: &lt;your-registry&gt;/model-engine\n  builderRepository: &lt;your-registry&gt;/model-engine\n  cacherRepository:  &lt;your-registry&gt;/model-engine\n  forwarderRepository: &lt;your-registry&gt;/model-engine\n\nconfig:\n  values:\n    infra:\n      docker_repo_prefix: \"&lt;your-registry&gt;\"\n    launch:\n      vllm_repository: \"&lt;your-registry&gt;/vllm\"\n      tensorrt_llm_repository: \"&lt;your-registry&gt;/tensorrt-llm\"\n      batch_inference_vllm_repository: \"&lt;your-registry&gt;/llm-engine/batch-infer-vllm\"\n</code></pre>"},{"location":"internal/cloud-matrix/#verifying-image-accessibility-before-install","title":"Verifying Image Accessibility Before Install","text":"<p>Confirm that images are pullable from within the cluster before running <code>helm install</code>:</p> <pre><code># Run a one-shot pod in the endpoint namespace to verify the pull\nkubectl run image-check \\\n  --image=&lt;your-registry&gt;/model-engine:&lt;tag&gt; \\\n  --restart=Never \\\n  --namespace=llm-engine \\\n  -- echo \"image pulled successfully\"\n\nkubectl logs image-check --namespace=llm-engine\nkubectl delete pod image-check --namespace=llm-engine\n</code></pre>"},{"location":"internal/cloud-matrix/#diagnosing-image-pull-failures","title":"Diagnosing Image Pull Failures","text":"<p>If an endpoint stays <code>INITIALIZING</code> and you suspect a missing image:</p> <pre><code># List pods in the endpoint namespace\nkubectl get pods -n llm-engine\n\n# Check for image pull errors\nkubectl describe pod &lt;pod-name&gt; -n llm-engine | grep -A 10 Events\n</code></pre> <p>Look for <code>ErrImagePull</code> or <code>ImagePullBackOff</code> in the events. If present, either the image was not mirrored or the registry credentials are not available to the inference pod's service account.</p> <p>[TODO] Automate the mirroring process \u2014 a script or CI job that pulls from <code>public.ecr.aws/b2z8n5q1/</code> and pushes to the customer registry, parameterized by registry URL and image tags. Tracking in the Artifacts &amp; Versioning Confluence page.</p>"},{"location":"internal/cloud-matrix/#section-5-known-gaps","title":"Section 5: Known Gaps","text":""},{"location":"internal/cloud-matrix/#gcp_1","title":"GCP","text":"<ul> <li>Not tested end-to-end. GCP code paths were added in commit #750 (<code>f436d25e</code>) but have not been validated in a real deployment with real infrastructure.</li> <li><code>GARDockerRepository.build_image</code> raises <code>NotImplementedError</code> \u2014 custom image builds (user-defined model bundles) are not supported on GCP.</li> <li>There is no GCP-specific block in the helm chart analogous to the <code>azure:</code> block. GCP currently reuses the <code>redisHost</code> / <code>redisPort</code> / <code>s3EndpointUrl</code> fields that were added for on-prem. A dedicated <code>gcp:</code> values block would improve clarity.</li> <li>No GCP-specific Confluence runbook exists. When the first GCP deployment happens, write one and link it here.</li> </ul>"},{"location":"internal/cloud-matrix/#on-prem_1","title":"On-prem","text":"<ul> <li>Not tested end-to-end. On-prem support was added in commit #744 (<code>ecc8ff42</code>).</li> <li>Redis broker only. There is no managed message broker option for on-prem. See Section 3 for known reliability limitations of Redis-as-broker at scale.</li> <li>No managed secret store. All secrets must be provided as Kubernetes secrets. There is no Key Vault or Secrets Manager integration path.</li> <li>NVIDIA driver setup is not covered here. GPU nodes must have a working NVIDIA driver and GPU operator installed and configured before <code>helm install</code>. Verify with <code>nvidia-smi</code> on a GPU node before attempting GPU endpoint creation. If driver setup is untested in your environment, do a CPU-only smoke test first.</li> <li><code>OnPremDockerRepository</code> does not support image builds \u2014 custom image bundles are not supported on-prem.</li> </ul>"},{"location":"internal/cloud-matrix/#azure_1","title":"Azure","text":"<ul> <li>Service Bus 300s idle connection drop \u2014 fixed in commits #765 (<code>1fefec11</code>) and #767 (<code>9deb59f1</code>). Requires <code>azure-servicebus &gt;= 7.14.3</code> and a model-engine image that includes <code>9deb59f1</code>. Deployments on older images will see intermittent 503s after idle periods. See Section 3 for the full diagnosis and fix details.</li> <li>Tested in production at partial scale. Edge cases involving very long idle periods or concurrent token refresh races may still surface.</li> </ul>"},{"location":"internal/helm-values/","title":"Helm Values Reference","text":"<p>Audience: Deployment engineers installing model engine into a customer environment. Purpose: Reference for every configurable value, organized by deployment concern. Full chart source: <code>charts/model-engine/values_sample.yaml</code></p>"},{"location":"internal/helm-values/#high-risk-values","title":"High-Risk Values","text":"<p>Read this before starting your installation</p> <p>The following values have non-obvious defaults or silent failure modes. Getting them wrong causes hard-to-diagnose issues.</p> Value Default Risk Impact if wrong <code>db.runDbMigrationScript</code> <code>false</code> HIGH Schema not initialized on first install \u2014 model creation fails with cryptic DB errors <code>config.values.infra.prometheus_server_address</code> unset HIGH KEDA scale-to-zero silently broken for sync endpoints with <code>min_workers=0</code> <code>config.values.launch.vllm_repository</code> <code>vllm</code> (resolves to Scale's ECR) HIGH Endpoint creation succeeds but pods stay INITIALIZING \u2014 image pull fails silently <code>celeryBrokerType</code> <code>sqs</code> HIGH Wrong broker for non-AWS clouds \u2014 all async endpoints broken <code>config.values.infra.cloud_provider</code> <code>aws</code> HIGH Wrong storage/auth/registry clients loaded for non-AWS environments <p>TODO</p> <p>Change <code>db.runDbMigrationScript</code> default to <code>true</code> in <code>values.yaml</code>.</p>"},{"location":"internal/helm-values/#1-minimum-viable-config","title":"1. Minimum Viable Config","text":"<p>These are the values you must set for the service to start. All other values have safe defaults. Getting any of these wrong will prevent the control plane from coming up or prevent any endpoint from being created successfully.</p> Value Type Default Required Description <code>tag</code> string \u2014 Yes LLM Engine Docker image tag to deploy <code>image.gatewayRepository</code> string \u2014 Yes Docker repository for the gateway image <code>image.builderRepository</code> string \u2014 Yes Docker repository for the endpoint builder image <code>image.cacherRepository</code> string \u2014 Yes Docker repository for the cacher image <code>image.forwarderRepository</code> string \u2014 Yes Docker repository for the forwarder image <code>secrets.kubernetesDatabaseSecretName</code> string <code>llm-engine-postgres-credentials</code> Yes (one of two) Kubernetes Secret name containing <code>DATABASE_URL</code>. Mutually exclusive with <code>secrets.cloudDatabaseSecretName</code> <code>secrets.cloudDatabaseSecretName</code> string \u2014 Yes (one of two) Cloud-provider secret name (e.g., AWS Secrets Manager) containing database credentials <code>serviceAccount.annotations</code> map \u2014 Yes Annotations to apply to the control-plane service account. On EKS, set <code>eks.amazonaws.com/role-arn</code> <code>config.values.infra.cloud_provider</code> string <code>aws</code> Yes Cloud provider: <code>aws</code>, <code>azure</code>, or <code>onprem</code> <code>config.values.infra.k8s_cluster_name</code> string <code>main_cluster</code> Yes Kubernetes cluster name used for resource tagging and lookups <code>config.values.infra.dns_host_domain</code> string <code>llm-engine.domain.com</code> Yes Base domain for endpoint hostnames <code>config.values.infra.default_region</code> string <code>us-east-1</code> Yes Default cloud region for all resource operations <code>config.values.infra.ml_account_id</code> string <code>\"000000000000\"</code> Yes Cloud account/subscription ID <code>config.values.infra.docker_repo_prefix</code> string <code>000000000000.dkr.ecr.us-east-1.amazonaws.com</code> Yes Prefix prepended to all inference image repositories <code>config.values.infra.redis_host</code> string \u2014 Yes (if not using secret) Hostname of the Redis cluster used by the inference control plane <code>config.values.infra.s3_bucket</code> string <code>llm-engine</code> Yes S3 bucket (or equivalent) for storing fine-tuning artifacts and other assets <code>config.values.launch.endpoint_namespace</code> string <code>llm-engine</code> Yes Kubernetes namespace where inference endpoint pods are created <code>config.values.launch.cache_redis_aws_url</code> string \u2014 Yes (one of three) Full Redis URL used by the cacher. Exactly one of <code>cache_redis_aws_url</code>, <code>cache_redis_azure_host</code>, or <code>cache_redis_aws_secret_name</code> must be set"},{"location":"internal/helm-values/#minimal-working-yaml","title":"Minimal Working YAML","text":"<pre><code>tag: \"abc123def456\"\n\nimage:\n  gatewayRepository: public.ecr.aws/b2z8n5q1/model-engine\n  builderRepository: public.ecr.aws/b2z8n5q1/model-engine\n  cacherRepository: public.ecr.aws/b2z8n5q1/model-engine\n  forwarderRepository: public.ecr.aws/b2z8n5q1/model-engine\n  pullPolicy: Always\n\nsecrets:\n  kubernetesDatabaseSecretName: llm-engine-postgres-credentials\n\nserviceAccount:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/k8s-main-llm-engine\n\ndb:\n  runDbMigrationScript: true  # required on first install\n  runDbInitScript: false\n\nconfig:\n  values:\n    infra:\n      cloud_provider: aws\n      k8s_cluster_name: my-cluster\n      dns_host_domain: llm-engine.example.com\n      default_region: us-east-1\n      ml_account_id: \"111122223333\"\n      docker_repo_prefix: \"111122223333.dkr.ecr.us-east-1.amazonaws.com\"\n      redis_host: my-redis.use1.cache.amazonaws.com\n      s3_bucket: my-llm-engine-bucket\n    launch:\n      endpoint_namespace: llm-engine\n      cache_redis_aws_url: redis://my-redis.use1.cache.amazonaws.com:6379/15\n      s3_file_llm_fine_tuning_job_repository: \"s3://my-llm-engine-bucket/llm-ft-job-repository\"\n      hf_user_fine_tuned_weights_prefix: \"s3://my-llm-engine-bucket/fine_tuned_weights\"\n      vllm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/vllm\"\n      tensorrt_llm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/tensorrt-llm\"\n      batch_inference_vllm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/llm-engine/batch-infer-vllm\"\n</code></pre>"},{"location":"internal/helm-values/#2-cloud-specific-config","title":"2. Cloud-Specific Config","text":""},{"location":"internal/helm-values/#aws-reference-configuration","title":"AWS (Reference Configuration)","text":"<p>AWS is the default. The values below represent the full set of AWS-specific fields.</p> Value Type Default Required Description <code>config.values.infra.cloud_provider</code> string <code>aws</code> Yes Set to <code>aws</code> <code>config.values.infra.default_region</code> string <code>us-east-1</code> Yes AWS region for ECR, SQS, and other resources <code>config.values.infra.ml_account_id</code> string \u2014 Yes AWS account ID (12 digits, quoted as string) <code>config.values.infra.docker_repo_prefix</code> string \u2014 Yes ECR registry prefix: <code>&lt;account&gt;.dkr.ecr.&lt;region&gt;.amazonaws.com</code> <code>config.values.infra.redis_host</code> string \u2014 Yes (or use secret) ElastiCache hostname <code>config.values.infra.redis_aws_secret_name</code> string \u2014 No AWS Secrets Manager secret name containing Redis connection info. Fields: <code>scheme</code>, <code>host</code>, <code>port</code>, <code>auth_token</code> (optional), <code>query_params</code> (optional) <code>config.values.infra.s3_bucket</code> string <code>llm-engine</code> Yes S3 bucket name for artifacts <code>config.values.launch.cache_redis_aws_url</code> string \u2014 Yes (one of three) Full Redis URL: <code>redis://&lt;host&gt;:&lt;port&gt;/&lt;db&gt;</code> <code>config.values.launch.cache_redis_aws_secret_name</code> string \u2014 Yes (one of three) AWS Secrets Manager secret with field <code>cache-url</code> containing full Redis URL <code>config.values.launch.sqs_profile</code> string <code>default</code> No AWS profile for SQS operations <code>config.values.launch.sqs_queue_policy_template</code> string \u2014 Yes (for async) IAM policy template for per-endpoint SQS queues. Must grant <code>sqs:*</code> to the LLM Engine role <code>config.values.launch.sqs_queue_tag_template</code> string \u2014 No JSON template for SQS queue tags <code>celeryBrokerType</code> string <code>sqs</code> Yes Use <code>sqs</code> for AWS async endpoints <code>serviceAccount.annotations.\"eks.amazonaws.com/role-arn\"</code> string \u2014 Yes IRSA role ARN for the control-plane service account <pre><code># AWS reference config diff\nconfig:\n  values:\n    infra:\n      cloud_provider: aws\n      default_region: us-east-1\n      ml_account_id: \"111122223333\"\n      docker_repo_prefix: \"111122223333.dkr.ecr.us-east-1.amazonaws.com\"\n      redis_host: my-redis.use1.cache.amazonaws.com\n      s3_bucket: my-llm-engine-bucket\n    launch:\n      cache_redis_aws_url: redis://my-redis.use1.cache.amazonaws.com:6379/15\n      sqs_profile: default\n      sqs_queue_policy_template: &gt;\n        {\n          \"Version\": \"2012-10-17\",\n          \"Statement\": [{\n            \"Effect\": \"Allow\",\n            \"Principal\": {\"AWS\": \"arn:aws:iam::111122223333:role/k8s-main-llm-engine\"},\n            \"Action\": \"sqs:*\",\n            \"Resource\": \"arn:aws:sqs:us-east-1:111122223333:${queue_name}\"\n          }]\n        }\n\nceleryBrokerType: sqs\n\nserviceAccount:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/k8s-main-llm-engine\n</code></pre>"},{"location":"internal/helm-values/#azure-diff-from-aws","title":"Azure (Diff from AWS)","text":"Value Type Default Required Description <code>config.values.infra.cloud_provider</code> string \u2014 Yes Set to <code>azure</code> <code>config.values.infra.default_region</code> string \u2014 Yes Azure region (e.g., <code>eastus</code>) <code>config.values.launch.cache_redis_azure_host</code> string \u2014 Yes Azure Cache for Redis hostname: <code>&lt;name&gt;.redis.cache.windows.net:6380</code> <code>keyvaultName</code> string <code>llm-engine-keyvault</code> Yes Azure Key Vault name for secret retrieval <code>celeryBrokerType</code> string \u2014 Yes Set to <code>elasticache</code> for Azure Service Bus-backed broker <p>Azure Service Bus: broker_pool_limit</p> <p>When using Azure Service Bus as the Celery broker, do not set <code>broker_pool_limit=0</code>. This was previously thought to help with connection management but actually causes idle AMQP connections to drop, resulting in 503 errors on async endpoints. The fix (removing <code>broker_pool_limit=0</code>) is tracked in commit <code>9deb59f1</code>. Leave this at the library default.</p> <pre><code># Azure diff\nconfig:\n  values:\n    infra:\n      cloud_provider: azure\n      default_region: eastus\n      ml_account_id: \"your-subscription-id\"\n      docker_repo_prefix: \"myregistry.azurecr.io\"\n    launch:\n      cache_redis_azure_host: my-llm-engine-cache.redis.cache.windows.net:6380\n      # Do NOT set cache_redis_aws_url for Azure\n\nkeyvaultName: my-llm-engine-keyvault\nceleryBrokerType: elasticache  # Azure Service Bus-backed\n</code></pre>"},{"location":"internal/helm-values/#gcp-on-premises-diff-from-aws","title":"GCP / On-Premises (Diff from AWS)","text":"Value Type Default Required Description <code>config.values.infra.cloud_provider</code> string \u2014 Yes Set to <code>onprem</code> <code>config.values.launch.cache_redis_onprem_url</code> string \u2014 Yes (one of three) Explicit Redis URL for on-prem: <code>redis://redis:6379/0</code>. Highest priority \u2014 takes precedence over all other Redis URL fields <code>celeryBrokerType</code> string \u2014 Yes Set to <code>elasticache</code> to use Redis as the Celery broker instead of SQS <code>celery_broker_type_redis</code> bool <code>null</code> No Alternative override flag to force Redis broker regardless of <code>celeryBrokerType</code> <pre><code># On-prem / GCP diff\nconfig:\n  values:\n    infra:\n      cloud_provider: onprem\n      default_region: us-central1\n      ml_account_id: \"my-gcp-project\"\n      docker_repo_prefix: \"gcr.io/my-gcp-project\"\n    launch:\n      cache_redis_onprem_url: redis://redis.llm-engine.svc.cluster.local:6379/0\n\nceleryBrokerType: elasticache\ncelery_broker_type_redis: true\n</code></pre> <p>Cloud matrix</p> <p>For a full per-cloud capability and limitation matrix, see cloud-matrix.md.</p>"},{"location":"internal/helm-values/#3-gpu-hardware-config","title":"3. GPU / Hardware Config","text":""},{"location":"internal/helm-values/#balloon-pods","title":"Balloon Pods","text":"<p>Balloon pods are low-priority placeholder deployments that keep GPU nodes warm. When real inference pods need to be scheduled, they preempt the balloon pods, eliminating cold-start node provisioning time.</p> Value Type Default Required Description <code>balloons[].acceleratorName</code> string \u2014 Yes GPU type identifier. Must match node labels. Supported: <code>nvidia-ampere-a10</code>, <code>nvidia-ampere-a100</code>, <code>nvidia-tesla-t4</code>, <code>nvidia-hopper-h100</code>, <code>cpu</code> <code>balloons[].replicaCount</code> integer <code>0</code> Yes Number of balloon pods to maintain for this GPU type. Set to <code>0</code> to disable warming for that type <code>balloons[].gpuCount</code> integer <code>1</code> No Number of GPUs each balloon pod requests. Relevant for multi-GPU balloon pods (e.g., <code>gpuCount: 4</code> for H100 nodes) <code>balloonConfig.reserveHighPriority</code> bool <code>true</code> No If <code>true</code>, only high-priority pods can preempt balloon pods. If <code>false</code>, any pod can preempt balloons, which may cause unintended disruption <code>balloonNodeSelector</code> map <code>{node-lifecycle: normal}</code> No Node selector applied to all balloon pod deployments. Restricts balloons to on-demand (non-spot) nodes by default <pre><code>balloonConfig:\n  reserveHighPriority: true\n\nballoonNodeSelector:\n  node-lifecycle: normal\n\nballoons:\n  - acceleratorName: nvidia-ampere-a10\n    replicaCount: 2\n  - acceleratorName: nvidia-ampere-a100\n    replicaCount: 1\n  - acceleratorName: nvidia-hopper-h100\n    replicaCount: 1\n    gpuCount: 4\n  - acceleratorName: nvidia-tesla-t4\n    replicaCount: 0\n  - acceleratorName: cpu\n    replicaCount: 0\n</code></pre>"},{"location":"internal/helm-values/#image-cache","title":"Image Cache","text":"<p>Image caching pre-pulls large inference images onto GPU nodes so that endpoint scale-up does not spend time pulling multi-GB images. Each device entry specifies a node selector (and optional tolerations) to target a specific GPU node pool.</p> Value Type Default Required Description <code>imageCache.devices[].name</code> string \u2014 Yes Logical name for this device pool (e.g., <code>a10</code>, <code>h100</code>) <code>imageCache.devices[].nodeSelector</code> map \u2014 Yes Label selector targeting nodes in this device pool <code>imageCache.devices[].tolerations</code> list <code>[]</code> No Tolerations for GPU taint. Required for GPU node pools with <code>nvidia.com/gpu:NoSchedule</code> taint <pre><code>imageCache:\n  devices:\n    - name: cpu\n      nodeSelector:\n        cpu-only: \"true\"\n    - name: a10\n      nodeSelector:\n        k8s.amazonaws.com/accelerator: nvidia-ampere-a10\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n    - name: a100\n      nodeSelector:\n        k8s.amazonaws.com/accelerator: nvidia-ampere-a100\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n    - name: t4\n      nodeSelector:\n        k8s.amazonaws.com/accelerator: nvidia-tesla-t4\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n    - name: h100\n      nodeSelector:\n        k8s.amazonaws.com/accelerator: nvidia-hopper-h100\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n    - name: h100-1g20gb\n      nodeSelector:\n        k8s.amazonaws.com/accelerator: nvidia-hopper-h100-1g20gb\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n    - name: h100-3g40gb\n      nodeSelector:\n        k8s.amazonaws.com/accelerator: nvidia-hopper-h100-3g40gb\n      tolerations:\n        - key: \"nvidia.com/gpu\"\n          operator: \"Exists\"\n          effect: \"NoSchedule\"\n</code></pre>"},{"location":"internal/helm-values/#recommended-hardware-tables","title":"Recommended Hardware Tables","text":"<p>These tables are used by the LLM Engine to suggest appropriate hardware configurations when a user creates an endpoint without specifying hardware. The engine selects the first matching tier from <code>byModelName</code> (exact match by model name slug), then falls back to <code>byGpuMemoryGb</code> (based on model weight size in GPU memory).</p>"},{"location":"internal/helm-values/#bygpumemorygb","title":"byGpuMemoryGb","text":"<p>Tiers are evaluated in ascending <code>gpu_memory_le</code> order. The first tier where the model's estimated GPU memory requirement is less than or equal to <code>gpu_memory_le</code> is selected.</p> <code>gpu_memory_le</code> (GB) CPUs GPUs Memory Storage GPU Type <code>nodes_per_worker</code> 24 10 1 24Gi 80Gi nvidia-ampere-a10 1 48 20 2 48Gi 80Gi nvidia-ampere-a10 1 96 40 4 96Gi 96Gi nvidia-ampere-a10 1 180 20 2 160Gi 160Gi nvidia-hopper-h100 1 320 40 4 320Gi 320Gi nvidia-hopper-h100 1 640 80 8 800Gi 640Gi nvidia-hopper-h100 1 640 80 8 800Gi 640Gi nvidia-hopper-h100 2"},{"location":"internal/helm-values/#bymodelname","title":"byModelName","text":"<p>Exact overrides by model name slug. Takes precedence over <code>byGpuMemoryGb</code>.</p> Model Name CPUs GPUs Memory Storage GPU Type <code>nodes_per_worker</code> llama-3-8b-instruct-262k 20 2 40Gi 40Gi nvidia-hopper-h100 1 deepseek-coder-v2 160 8 800Gi 640Gi nvidia-hopper-h100 1 deepseek-coder-v2-instruct 160 8 800Gi 640Gi nvidia-hopper-h100 1 <pre><code>recommendedHardware:\n  byGpuMemoryGb:\n    - gpu_memory_le: 24\n      cpus: 10\n      gpus: 1\n      memory: 24Gi\n      storage: 80Gi\n      gpu_type: nvidia-ampere-a10\n      nodes_per_worker: 1\n    - gpu_memory_le: 48\n      cpus: 20\n      gpus: 2\n      memory: 48Gi\n      storage: 80Gi\n      gpu_type: nvidia-ampere-a10\n      nodes_per_worker: 1\n    # ... additional tiers\n  byModelName:\n    - name: deepseek-coder-v2-instruct\n      cpus: 160\n      gpus: 8\n      memory: 800Gi\n      storage: 640Gi\n      gpu_type: nvidia-hopper-h100\n      nodes_per_worker: 1\n</code></pre>"},{"location":"internal/helm-values/#control-plane-node-selector-tolerations-and-affinity","title":"Control-Plane Node Selector, Tolerations, and Affinity","text":"<p>These apply to the LLM Engine control-plane deployments (gateway, cacher, builder) \u2014 not to inference endpoint pods.</p> Value Type Default Required Description <code>nodeSelector</code> map <code>{node-lifecycle: normal}</code> No Node selector for control-plane pods. Default pins to on-demand nodes <code>tolerations</code> list <code>[]</code> No Tolerations for control-plane pods <code>affinity</code> map <code>{}</code> No Affinity rules for control-plane pods <pre><code>nodeSelector:\n  node-lifecycle: normal\n  kubernetes.io/arch: amd64\n\ntolerations:\n  - key: \"dedicated\"\n    operator: \"Equal\"\n    value: \"llm-engine\"\n    effect: \"NoSchedule\"\n\naffinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchLabels:\n              app: llm-engine-gateway\n          topologyKey: kubernetes.io/hostname\n</code></pre>"},{"location":"internal/helm-values/#4-autoscaling","title":"4. Autoscaling","text":""},{"location":"internal/helm-values/#gateway-horizontal-pod-autoscaler","title":"Gateway Horizontal Pod Autoscaler","text":"<p>The HPA governs the number of gateway replicas based on concurrent request load. This applies only to the control-plane gateway deployment, not to inference endpoint pods.</p> Value Type Default Required Description <code>autoscaling.horizontal.enabled</code> bool <code>true</code> No Enable HPA for the gateway deployment <code>autoscaling.horizontal.minReplicas</code> integer <code>2</code> No Minimum number of gateway replicas <code>autoscaling.horizontal.maxReplicas</code> integer <code>10</code> No Maximum number of gateway replicas <code>autoscaling.horizontal.targetConcurrency</code> integer <code>50</code> No Target average concurrent requests per replica before scaling out <code>autoscaling.vertical.enabled</code> bool <code>false</code> No Enable Vertical Pod Autoscaler (VPA) for control-plane deployments. Requires VPA operator installed in cluster <code>autoscaling.prewarming.enabled</code> bool <code>false</code> No Enable endpoint pre-warming (reserved for future use)"},{"location":"internal/helm-values/#celery-autoscaler","title":"Celery Autoscaler","text":"<p>For async (queue-backed) endpoints, a separate Celery autoscaler process monitors queue depth and scales inference pods accordingly.</p> Value Type Default Required Description <code>celery_autoscaler.enabled</code> bool <code>true</code> No Enable the Celery autoscaler for async endpoint scaling <code>celery_autoscaler.num_shards</code> integer <code>3</code> No Number of autoscaler shard instances. More shards reduces per-shard queue-watching load at high endpoint counts"},{"location":"internal/helm-values/#keda-scale-to-zero-for-sync-endpoints","title":"KEDA (Scale-to-Zero for Sync Endpoints)","text":"<p>KEDA enables sync/streaming endpoints to scale from zero replicas to one when the first request arrives. This is distinct from the HPA (which cannot scale below <code>minReplicas</code>).</p> Value Type Default Required Description <code>keda.cooldownPeriod</code> integer <code>300</code> No Seconds KEDA waits after the last request before scaling a sync endpoint down to zero <code>config.values.infra.prometheus_server_address</code> string unset Yes (for KEDA) Address of the Prometheus server that KEDA queries for endpoint request metrics <p>KEDA requires Prometheus</p> <p><code>config.values.infra.prometheus_server_address</code> must be set for KEDA scale-to-zero to function. If it is unset, sync endpoints with <code>min_workers=0</code> will silently fail to scale up from zero \u2014 the endpoint will appear healthy but all requests will hang until manually scaled.</p> <p>KEDA vs HPA: mutual exclusivity</p> <p>KEDA and HPA are mutually exclusive per endpoint. When an endpoint has <code>min_workers=0</code>, KEDA manages scaling from 0 to 1. Once at 1+ replicas, the HPA (if configured) takes over scaling above 1. Do not configure both KEDA and HPA to manage the same endpoint's replica range. Additionally, KEDA can only scale from 0 to 1 \u2014 it does not replace the HPA for scaling beyond 1 replica.</p> <pre><code>autoscaling:\n  horizontal:\n    enabled: true\n    minReplicas: 2\n    maxReplicas: 10\n    targetConcurrency: 50\n  vertical:\n    enabled: false\n\ncelery_autoscaler:\n  enabled: true\n  num_shards: 3\n\nkeda:\n  cooldownPeriod: 300\n\nconfig:\n  values:\n    infra:\n      prometheus_server_address: \"http://prometheus-server.istio-system.svc.cluster.local:80\"\n</code></pre>"},{"location":"internal/helm-values/#5-networking","title":"5. Networking","text":""},{"location":"internal/helm-values/#istio-virtualservice-and-destinationrule","title":"Istio VirtualService and DestinationRule","text":"<p>LLM Engine uses Istio for traffic routing when <code>config.values.launch.istio_enabled</code> is <code>true</code>. The VirtualService routes external traffic to the gateway service; the DestinationRule configures connection pool and outlier detection.</p> Value Type Default Required Description <code>virtualservice.enabled</code> bool <code>true</code> No Create an Istio VirtualService for the gateway <code>virtualservice.hostDomains</code> list <code>[llm-engine.domain.com]</code> Yes (if enabled) List of hostnames this VirtualService responds to. Must match your Istio gateway configuration <code>virtualservice.gateways</code> list <code>[default/internal-gateway]</code> Yes (if enabled) Istio Gateway resources to attach this VirtualService to. Format: <code>&lt;namespace&gt;/&lt;gateway-name&gt;</code> <code>virtualservice.annotations</code> map <code>{}</code> No Additional annotations for the VirtualService resource <code>destinationrule.enabled</code> bool <code>true</code> No Create an Istio DestinationRule for the gateway service <code>destinationrule.annotations</code> map <code>{}</code> No Additional annotations for the DestinationRule resource <code>hostDomain.prefix</code> string <code>http://</code> No URL scheme prefix used when constructing endpoint host URLs. Set to <code>https://</code> for TLS-terminated clusters <code>service.type</code> string <code>ClusterIP</code> No Kubernetes Service type for the gateway. Use <code>ClusterIP</code> with Istio; change to <code>LoadBalancer</code> only if managing ingress outside Istio <code>service.port</code> integer <code>80</code> No Port exposed by the gateway Kubernetes Service <code>config.values.launch.istio_enabled</code> bool <code>true</code> No Whether Istio service mesh is active. When <code>false</code>, VirtualService/DestinationRule resources are not used and direct service routing applies <pre><code>virtualservice:\n  enabled: true\n  hostDomains:\n    - llm-engine.example.com\n  gateways:\n    - default/internal-gateway\n\ndestinationrule:\n  enabled: true\n\nhostDomain:\n  prefix: https://\n\nservice:\n  type: ClusterIP\n  port: 80\n\nconfig:\n  values:\n    launch:\n      istio_enabled: true\n</code></pre>"},{"location":"internal/helm-values/#redis-tls-and-authentication","title":"Redis TLS and Authentication","text":"<p>These values control TLS and authentication for the Redis connection used by KEDA and endpoint metrics.</p> Value Type Default Required Description <code>redis.enableTLS</code> bool <code>false</code> No Enable TLS for the Redis connection. Required for Azure Cache for Redis (port 6380) and any Redis with TLS enforced <code>redis.enableAuth</code> bool <code>false</code> No Enable password/token authentication for Redis. Required when the Redis cluster has AUTH configured <code>redis.auth</code> string <code>null</code> No Redis AUTH password or token. Only used when <code>enableAuth: true</code>. Store this in a Kubernetes Secret rather than directly in values <code>redis.kedaSecretName</code> string <code>\"\"</code> No Name of a Kubernetes Secret containing Redis credentials for KEDA's ScaledObject. KEDA reads this directly; leave empty to use unauthenticated Redis <code>redis.unsafeSsl</code> bool <code>false</code> No Skip TLS certificate verification. Use only in development environments with self-signed certificates <pre><code>redis:\n  enableTLS: true\n  enableAuth: true\n  auth: \"\"  # set via --set redis.auth=$REDIS_TOKEN or from a secret\n  kedaSecretName: \"keda-redis-secret\"\n  unsafeSsl: false\n</code></pre>"},{"location":"internal/helm-values/#6-observability","title":"6. Observability","text":""},{"location":"internal/helm-values/#datadog-integration","title":"Datadog Integration","text":"<p>LLM Engine supports two Datadog toggles that must both be set consistently.</p> Value Type Default Required Description <code>datadog.enabled</code> bool <code>false</code> No Mount the Datadog agent socket and inject Datadog environment variables into control-plane pods. Requires the Datadog agent DaemonSet to be running on the cluster <code>dd_trace_enabled</code> bool <code>true</code> No Top-level Helm toggle that controls whether the <code>DD_TRACE_ENABLED</code> environment variable is set to <code>true</code> in control-plane containers <code>config.values.launch.dd_trace_enabled</code> bool <code>false</code> No Service-config-level toggle that controls whether the application code initializes the <code>ddtrace</code> library at startup. Must match <code>dd_trace_enabled</code> to avoid partial tracing <p>Two toggles, one feature</p> <p><code>dd_trace_enabled</code> (top-level) and <code>config.values.launch.dd_trace_enabled</code> are independent toggles that together control Datadog APM. Setting only one of them produces a broken state: traces may be emitted but not received, or the agent socket may be mounted but no spans generated. Always set both to the same value.</p> <pre><code>datadog:\n  enabled: true\n\ndd_trace_enabled: true\n\nconfig:\n  values:\n    launch:\n      dd_trace_enabled: true\n</code></pre>"},{"location":"internal/helm-values/#logging","title":"Logging","text":"Value Type Default Required Description <code>config.values.launch.sensitive_log_mode</code> bool <code>false</code> No When <code>true</code>, suppresses logging of request/response payloads and other PII-containing fields. Enable in customer environments that process sensitive data <code>debug_mode</code> bool/null <code>null</code> No Enables verbose debug logging across infrastructure components (gateway, cacher, builder). Produces high log volume \u2014 use only for troubleshooting <pre><code>config:\n  values:\n    launch:\n      sensitive_log_mode: true\n\ndebug_mode: null  # set to true only during active debugging\n</code></pre>"},{"location":"internal/helm-values/#7-security-compliance","title":"7. Security / Compliance","text":""},{"location":"internal/helm-values/#pod-security-context","title":"Pod Security Context","text":"<p>The pod security context applies to all containers within a pod and controls user/group identity and filesystem permissions. Uncomment and set these values when using a hardened base image (e.g., Chainguard).</p> Value Type Default Required Description <code>podSecurityContext.runAsUser</code> integer unset No UID to run all containers as. Chainguard images use <code>65532</code> (nonroot) <code>podSecurityContext.runAsGroup</code> integer unset No GID to run all containers as <code>podSecurityContext.runAsNonRoot</code> bool unset No Enforce that no container runs as UID 0. Set to <code>true</code> for all production deployments <code>podSecurityContext.fsGroup</code> integer unset No GID for volume mounts. Set to match <code>runAsGroup</code> so mounted secrets and configmaps are readable <pre><code>podSecurityContext:\n  runAsUser: 65532\n  runAsGroup: 65532\n  runAsNonRoot: true\n  fsGroup: 65532\n</code></pre>"},{"location":"internal/helm-values/#container-security-context","title":"Container Security Context","text":"<p>The container security context applies to each individual container and controls Linux capabilities and filesystem access.</p> Value Type Default Required Description <code>containerSecurityContext.allowPrivilegeEscalation</code> bool unset No Prevent the process from gaining additional privileges via setuid/setgid. Set to <code>false</code> in all production deployments <code>containerSecurityContext.readOnlyRootFilesystem</code> bool unset No Mount the container root filesystem as read-only. Set to <code>false</code> if the application writes to <code>/tmp</code> or other paths on the root fs <code>containerSecurityContext.capabilities.drop</code> list unset No Linux capabilities to drop. Set to <code>[\"ALL\"]</code> to remove all capabilities and then add back only what is needed <pre><code>containerSecurityContext:\n  allowPrivilegeEscalation: false\n  readOnlyRootFilesystem: false\n  capabilities:\n    drop:\n      - ALL\n</code></pre>"},{"location":"internal/helm-values/#inference-pod-security-servicetemplate","title":"Inference Pod Security (serviceTemplate)","text":"<p>These values apply to the inference endpoint pods created by the builder \u2014 not to the control-plane pods. They are injected into each endpoint's pod spec via the service template.</p> Value Type Default Required Description <code>serviceTemplate.securityContext.capabilities.drop</code> list <code>[\"all\"]</code> No Linux capabilities to drop from inference containers. Default drops all capabilities <code>serviceTemplate.mountInfraConfig</code> bool <code>true</code> No Mount the infra ConfigMap into inference pods. Required for the endpoint to read cloud configuration <code>serviceTemplate.createServiceAccount</code> bool <code>true</code> No Create a dedicated Kubernetes ServiceAccount for inference pods in the endpoint namespace <code>serviceTemplate.serviceAccountName</code> string <code>model-engine</code> No Name of the ServiceAccount created for inference pods <code>serviceTemplate.serviceAccountAnnotations</code> map \u2014 No Annotations for the inference pod ServiceAccount. On EKS, set <code>eks.amazonaws.com/role-arn</code> to the inference IAM role <pre><code>serviceTemplate:\n  securityContext:\n    capabilities:\n      drop:\n        - all\n  mountInfraConfig: true\n  createServiceAccount: true\n  serviceAccountName: model-engine\n  serviceAccountAnnotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/llm-engine\n    \"helm.sh/hook\": pre-install,pre-upgrade\n    \"helm.sh/hook-weight\": \"-2\"\n</code></pre>"},{"location":"internal/helm-values/#fips-federal-compliance","title":"FIPS / Federal Compliance","text":"Value Type Default Required Description <code>celery_enable_sha256</code> bool/null <code>null</code> No When <code>true</code>, forces Celery to use SHA-256 message signing instead of the default SHA-1. Required in FIPS-mode environments and any environment with federal compliance mandates (FedRAMP, IL4/IL5) <p>Coordinated rollout required for celery_enable_sha256</p> <p>Changing <code>celery_enable_sha256</code> requires a coordinated rollout. In-flight Celery tasks signed with SHA-1 cannot be verified by workers expecting SHA-256, and vice versa. During the transition window, drain all queues before deploying new workers. Rolling updates without draining will cause task signature verification failures and silently dropped async requests.</p> <pre><code>celery_enable_sha256: true\n</code></pre>"},{"location":"internal/helm-values/#8-replica-and-resource-tuning","title":"8. Replica and Resource Tuning","text":""},{"location":"internal/helm-values/#replica-counts","title":"Replica Counts","text":"Value Type Default Required Description <code>replicaCount.gateway</code> integer <code>2</code> No Number of gateway replicas. Minimum 2 for production HA. Overridden by HPA when <code>autoscaling.horizontal.enabled: true</code> <code>replicaCount.cacher</code> integer <code>1</code> No Number of cacher replicas. The cacher maintains a local cache of Kubernetes state (endpoint pods, services). Single replica is usually sufficient <code>replicaCount.builder</code> integer <code>1</code> No Number of builder replicas. The builder handles endpoint creation and image build jobs. Single replica is usually sufficient"},{"location":"internal/helm-values/#resources","title":"Resources","text":"Value Type Default Required Description <code>resources.requests.cpu</code> string/integer <code>2</code> No CPU request for control-plane pods (gateway, cacher, builder) <code>resources.requests.memory</code> string unset No Memory request for control-plane pods <code>resources.limits.cpu</code> string/integer unset No CPU limit for control-plane pods <code>resources.limits.memory</code> string unset No Memory limit for control-plane pods <pre><code>replicaCount:\n  gateway: 2\n  cacher: 1\n  builder: 1\n\nresources:\n  requests:\n    cpu: 2\n    memory: 4Gi\n  limits:\n    cpu: 4\n    memory: 8Gi\n</code></pre>"},{"location":"internal/helm-values/#pod-disruption-budget","title":"Pod Disruption Budget","text":"Value Type Default Required Description <code>podDisruptionBudget.enabled</code> bool <code>true</code> No Create a PodDisruptionBudget for the gateway deployment to ensure availability during node drain and rolling updates <code>podDisruptionBudget.minAvailable</code> integer/string <code>1</code> No Minimum number (or percentage) of gateway pods that must remain available during voluntary disruptions"},{"location":"internal/helm-values/#database-initialization","title":"Database Initialization","text":"Value Type Default (<code>values.yaml</code>) Default (<code>values_sample.yaml</code>) Required Description <code>db.runDbMigrationScript</code> bool <code>true</code> <code>false</code> Yes on first install Run Alembic schema migrations as a pre-install/pre-upgrade Job. Must be <code>true</code> on first install or the database schema will not be initialized <code>db.runDbInitScript</code> bool <code>false</code> <code>false</code> No Run the database initialization script (seed data). Only needed on fresh installs that require initial seed data <p>First install: set runDbMigrationScript: true</p> <p><code>values_sample.yaml</code> ships with <code>db.runDbMigrationScript: false</code>. On a brand-new install, the database schema does not exist yet. Without migrations, model creation will fail with cryptic PostgreSQL errors about missing tables. Always override this to <code>true</code> on first install. After initial migration, subsequent upgrades will apply incremental migrations automatically when set to <code>true</code>.</p>"},{"location":"internal/helm-values/#database-engine-tuning","title":"Database Engine Tuning","text":"<p>These values tune the SQLAlchemy connection pool. Defaults are appropriate for most deployments. Increase <code>pool_size</code> and <code>max_overflow</code> only when you observe connection exhaustion errors under high gateway concurrency.</p> Value Type Default Required Description <code>config.values.infra.db_engine_pool_size</code> integer <code>10</code> No Number of persistent connections in the SQLAlchemy connection pool per process <code>config.values.infra.db_engine_max_overflow</code> integer <code>10</code> No Maximum number of connections allowed above <code>pool_size</code>. Total max connections = <code>pool_size + max_overflow</code> <code>config.values.infra.db_engine_echo</code> bool <code>false</code> No Log all SQL statements. Produces extremely high log volume \u2014 use only for debugging SQL query issues <code>config.values.infra.db_engine_echo_pool</code> bool <code>false</code> No Log all connection pool events (checkout, checkin, overflow). Use only for debugging connection pool exhaustion <code>config.values.infra.db_engine_disconnect_strategy</code> string <code>pessimistic</code> No Strategy for detecting stale/broken connections. <code>pessimistic</code> tests the connection before each use (safe but adds a small latency). <code>optimistic</code> assumes connections are valid until proven otherwise <pre><code>config:\n  values:\n    infra:\n      db_engine_pool_size: 10\n      db_engine_max_overflow: 10\n      db_engine_echo: false\n      db_engine_echo_pool: false\n      db_engine_disconnect_strategy: \"pessimistic\"\n</code></pre>"},{"location":"internal/helm-values/#llm-inference-image-repositories","title":"LLM Inference Image Repositories","text":"<p>These values specify the Docker repository paths for each supported inference backend. They are combined with <code>config.values.infra.docker_repo_prefix</code> at endpoint creation time to form the full image URI.</p> <p>vllm_repository: always override in customer environments</p> <p>The default value <code>vllm</code> is a short relative path that resolves to Scale's internal ECR registry when combined with Scale's <code>docker_repo_prefix</code>. In customer environments with a different registry prefix, endpoint pods will attempt to pull from a non-existent or inaccessible image path. The pods will appear to be <code>INITIALIZING</code> with no clear error. Always set <code>vllm_repository</code> to the full repository path or a prefix-relative path that exists in your registry.</p> Value Type Default Required Description <code>config.values.launch.vllm_repository</code> string <code>vllm</code> Yes Repository path for vLLM inference images. This is the most commonly used inference backend <code>config.values.launch.tensorrt_llm_repository</code> string <code>tensorrt-llm</code> No Repository path for TensorRT-LLM inference images <code>config.values.launch.batch_inference_vllm_repository</code> string <code>llm-engine/batch-infer-vllm</code> No Repository path for batch inference images (used by batch completion endpoints) <code>config.values.launch.tgi_repository</code> string <code>text-generation-inference</code> No Repository path for HuggingFace Text Generation Inference images <code>config.values.launch.lightllm_repository</code> string <code>lightllm</code> No Repository path for LightLLM inference images <code>config.values.launch.sglang_repository</code> string <code>null</code> No Repository path for SGLang inference images. Optional; leave unset if SGLang is not used <code>config.values.launch.user_inference_base_repository</code> string <code>launch/inference</code> No Base repository for custom user-defined inference images <code>config.values.launch.user_inference_pytorch_repository</code> string <code>launch/inference/pytorch</code> No Repository for custom PyTorch inference images <code>config.values.launch.user_inference_tensorflow_repository</code> string <code>launch/inference/tf</code> No Repository for custom TensorFlow inference images <code>config.values.launch.docker_image_layer_cache_repository</code> string <code>launch-docker-build-cache</code> No Repository used as a layer cache during Docker image builds for custom endpoints <pre><code>config:\n  values:\n    launch:\n      # Always override these in customer environments\n      vllm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/vllm\"\n      tensorrt_llm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/tensorrt-llm\"\n      batch_inference_vllm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/llm-engine/batch-infer-vllm\"\n      tgi_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/text-generation-inference\"\n      lightllm_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/lightllm\"\n      user_inference_base_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/launch/inference\"\n      user_inference_pytorch_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/launch/inference/pytorch\"\n      user_inference_tensorflow_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/launch/inference/tf\"\n      docker_image_layer_cache_repository: \"111122223333.dkr.ecr.us-east-1.amazonaws.com/launch-docker-build-cache\"\n</code></pre>"},{"location":"internal/helm-values/#fine-tuning-storage","title":"Fine-Tuning Storage","text":"Value Type Default Required Description <code>config.values.launch.s3_file_llm_fine_tuning_job_repository</code> string <code>s3://llm-engine/llm-ft-job-repository</code> Yes S3 URI (or equivalent) where fine-tuning job artifacts (checkpoints, adapters) are stored <code>config.values.launch.hf_user_fine_tuned_weights_prefix</code> string <code>s3://llm-engine/fine_tuned_weights</code> Yes S3 URI prefix for storing user-uploaded fine-tuned model weights"},{"location":"internal/helm-values/#image-pull-policy","title":"Image Pull Policy","text":"Value Type Default Required Description <code>image.pullPolicy</code> string <code>Always</code> No Kubernetes image pull policy for all control-plane images. <code>Always</code> ensures the latest tag is always pulled. Set to <code>IfNotPresent</code> to avoid redundant pulls when using immutable tags"},{"location":"internal/helm-values/#aws-configmap","title":"AWS ConfigMap","text":"Value Type Default Required Description <code>aws.configMap.name</code> string <code>default-config</code> No Name of the Kubernetes ConfigMap containing the AWS CLI configuration <code>aws.configMap.create</code> bool <code>true</code> No Whether to create the AWS ConfigMap as part of the Helm release <code>aws.profileName</code> string <code>default</code> No AWS profile name to use from the ConfigMap"},{"location":"internal/helm-values/#image-builder-service-account","title":"Image Builder Service Account","text":"Value Type Default Required Description <code>imageBuilderServiceAccount.create</code> bool <code>true</code> No Create a dedicated ServiceAccount for the image builder. This account needs ECR push/pull permissions <code>imageBuilderServiceAccount.annotations</code> map \u2014 No Annotations for the image builder ServiceAccount. On EKS, set <code>eks.amazonaws.com/role-arn</code> to a role with ECR permissions <pre><code>imageBuilderServiceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::111122223333:role/k8s-main-llm-engine-image-builder\n</code></pre>"},{"location":"internal/helm-values/#miscellaneous","title":"Miscellaneous","text":"Value Type Default Required Description <code>spellbook.enabled</code> bool <code>false</code> No Enable Spellbook integration. Reserved for Scale internal use <code>context</code> string <code>production</code> No Deployment context tag. Used for labeling and log correlation. Set to a meaningful environment name (e.g., <code>staging</code>, <code>production</code>, <code>customer-prod</code>) <code>celery_broker_type_redis</code> bool/null <code>null</code> No When <code>true</code>, forces the Celery broker to use Redis regardless of the <code>celeryBrokerType</code> value. Useful for on-prem and GCP deployments where SQS is unavailable <code>keyvaultName</code> string <code>llm-engine-keyvault</code> No Azure Key Vault name. Only used when <code>cloud_provider: azure</code>"},{"location":"internal/smoke-tests/","title":"Smoke Tests","text":"<p>Post-deploy validation checklist for Model Engine. Run these checks after every <code>helm install</code> or <code>helm upgrade</code> to verify the deployment is working correctly.</p> <p>Last verified: [DATE] Verified by: [AUTHOR]</p>"},{"location":"internal/smoke-tests/#test-tiers","title":"Test Tiers","text":"<p>Two tiers are defined to allow partial validation when GPU nodes are unavailable:</p> Tier Hardware Model What it covers Tier A \u2014 CPU only Any node, no GPU required <code>model_engine_server.inference.forwarding.echo_server</code> (built into the model-engine image) Service Builder flow, endpoint lifecycle, sync inference, async inference Tier B \u2014 GPU + LLM GPU node with NVIDIA driver <code>llama-3.1-8b</code> via vLLM vLLM image pull from customer registry, GPU scheduling, <code>POST /v2/chat/completions</code> sync and streaming <p>Run Tier A first. If Tier A passes, the control plane, broker, database, and Service Builder are all functioning. Tier B additionally validates GPU scheduling and the LLM API layer.</p> <p>v2 API only</p> <p>Tier B only tests <code>POST /v2/chat/completions</code>. Do not use v1 completions endpoints in these smoke tests.</p>"},{"location":"internal/smoke-tests/#required-environment-variables","title":"Required Environment Variables","text":"<p>Set these before running any commands in this document:</p> <pre><code>export GATEWAY_URL=\"http://model-engine.{NAMESPACE}.svc.cluster.local\"  # k8s cluster DNS\nexport AUTH_TOKEN=\"{your-auth-token}\"\nexport NAMESPACE=\"{your-namespace}\"\n</code></pre>"},{"location":"internal/smoke-tests/#phase-1-pre-flight","title":"Phase 1: Pre-flight","text":"<p>Run these checks before <code>helm install</code>.</p>"},{"location":"internal/smoke-tests/#11-redis-reachability","title":"1.1 Redis reachability","text":"<ul> <li>[ ] Redis responds to ping:</li> </ul> <pre><code>redis-cli -h {REDIS_HOST} -p {REDIS_PORT} ping\n# Expected: PONG\n</code></pre>"},{"location":"internal/smoke-tests/#12-database-reachability","title":"1.2 Database reachability","text":"<ul> <li>[ ] Database accepts connections:</li> </ul> <pre><code>psql \"{DB_URL}\" -c \"SELECT 1\"\n# Expected: returns 1\n</code></pre>"},{"location":"internal/smoke-tests/#13-vllm-image-pullable-from-customer-registry","title":"1.3 vLLM image pullable from customer registry","text":"<p>Warning</p> <p>Verify the image is accessible from your mirrored registry, not Scale's internal ECR. This is the #1 silent deployment failure.</p> <ul> <li>[ ] vLLM image is pullable:</li> </ul> <pre><code>kubectl run vllm-pull-test \\\n  --image={VLLM_REPOSITORY}:{VLLM_TAG} \\\n  --restart=Never \\\n  --command -- echo \"pull ok\" \\\n  -n {NAMESPACE}\nkubectl logs vllm-pull-test -n {NAMESPACE}\nkubectl delete pod vllm-pull-test -n {NAMESPACE}\n# Expected: \"pull ok\"\n</code></pre>"},{"location":"internal/smoke-tests/#14-gpu-driver-functional-on-gpu-nodes","title":"1.4 GPU driver functional on GPU nodes","text":"<ul> <li>[ ] NVIDIA driver is working on GPU nodes:</li> </ul> <pre><code># List GPU nodes\nkubectl get nodes -l k8s.amazonaws.com/accelerator={GPU_TYPE}\n\n# Run nvidia-smi on a GPU node\nkubectl debug node/{GPU_NODE_NAME} \\\n  -it \\\n  --image=nvidia/cuda:12.0-base \\\n  -- nvidia-smi\n# Expected: GPU device listed with driver version and CUDA version\n</code></pre>"},{"location":"internal/smoke-tests/#15-gpu-nodes-labeled-correctly","title":"1.5 GPU nodes labeled correctly","text":"<ul> <li>[ ] Nodes carry the expected accelerator label:</li> </ul> <pre><code>kubectl get nodes --show-labels | grep accelerator\n# Expected: nodes labeled with k8s.amazonaws.com/accelerator={GPU_TYPE}\n</code></pre>"},{"location":"internal/smoke-tests/#16-broker-reachability","title":"1.6 Broker reachability","text":"AWS (SQS)Azure (Service Bus)GCP / On-prem (Redis) <ul> <li>[ ] IAM/IRSA SQS access works from the service account: <pre><code>kubectl run sqs-test \\\n  --image=amazon/aws-cli \\\n  --restart=Never \\\n  --serviceaccount={MODEL_ENGINE_SERVICE_ACCOUNT} \\\n  -n {NAMESPACE} \\\n  -- sqs list-queues --region {AWS_REGION}\nkubectl logs sqs-test -n {NAMESPACE}\nkubectl delete pod sqs-test -n {NAMESPACE}\n</code></pre></li> </ul> <ul> <li>[ ] Service Bus secret exists: <pre><code>kubectl get secret {ASB_SECRET_NAME} -n {NAMESPACE}\n</code></pre></li> </ul> <p>Warning</p> <p>Azure Service Bus drops idle AMQP connections after 300 seconds. Ensure <code>broker_pool_limit=0</code> is set in helm values. See Cloud Support Matrix.</p> <ul> <li>[ ] Redis broker reachable: <pre><code>redis-cli -h {BROKER_REDIS_HOST} -p {BROKER_REDIS_PORT} ping\n# Expected: PONG\n</code></pre></li> </ul>"},{"location":"internal/smoke-tests/#phase-2-post-install-infrastructure","title":"Phase 2: Post-install Infrastructure","text":"<p>Run these checks immediately after <code>helm install</code> completes.</p>"},{"location":"internal/smoke-tests/#21-all-pods-running-and-ready","title":"2.1 All pods Running and Ready","text":"<ul> <li>[ ] All model-engine pods are <code>Running</code>:</li> </ul> <pre><code>kubectl get pods -n {NAMESPACE} -l app=model-engine-gateway\nkubectl get pods -n {NAMESPACE} -l app=model-engine-builder\nkubectl get pods -n {NAMESPACE} -l app=model-engine-cacher\nkubectl get pods -n {NAMESPACE} -l app=model-engine-celery-autoscaler\n</code></pre> <ul> <li>[ ] No pods in error states:</li> </ul> <pre><code>kubectl get pods -n {NAMESPACE} | grep -vE \"Running|Completed|NAME\"\n# Expected: no output\n</code></pre>"},{"location":"internal/smoke-tests/#22-gateway-hpa-exists","title":"2.2 Gateway HPA exists","text":"<ul> <li>[ ] HPA is configured for the gateway:</li> </ul> <pre><code>kubectl get hpa -n {NAMESPACE}\n# Expected: model-engine-gateway HPA present\n</code></pre>"},{"location":"internal/smoke-tests/#23-celery-autoscaler-statefulset-ready","title":"2.3 Celery Autoscaler StatefulSet ready","text":"<ul> <li>[ ] StatefulSet shows desired replicas ready:</li> </ul> <pre><code>kubectl get statefulset -n {NAMESPACE} -l app=model-engine-celery-autoscaler\n# Expected: READY column matches DESIRED (e.g. 1/1)\n</code></pre>"},{"location":"internal/smoke-tests/#24-cacher-log-check","title":"2.4 Cacher log check","text":"<ul> <li>[ ] Cacher is completing cache cycles (should appear within 15 seconds):</li> </ul> <pre><code>kubectl logs -n {NAMESPACE} -l app=model-engine-cacher --tail=50 | grep -i \"cache\"\n</code></pre> <ul> <li>[ ] No Redis auth errors:</li> </ul> <pre><code>kubectl logs -n {NAMESPACE} -l app=model-engine-cacher --tail=100 \\\n  | grep -iE \"auth|error|exception|redis\"\n# Expected: no authentication errors\n</code></pre>"},{"location":"internal/smoke-tests/#25-builder-celery-worker-ready","title":"2.5 Builder Celery worker ready","text":"<ul> <li>[ ] Builder shows Celery worker connected and ready:</li> </ul> <pre><code>kubectl logs -n {NAMESPACE} -l app=model-engine-builder --tail=100 \\\n  | grep -iE \"ready|celery|connected|worker\"\n</code></pre> <ul> <li>[ ] No broker connection errors:</li> </ul> <pre><code>kubectl logs -n {NAMESPACE} -l app=model-engine-builder --tail=100 \\\n  | grep -iE \"error|refused|timeout|cannot connect\"\n# Expected: no output\n</code></pre>"},{"location":"internal/smoke-tests/#26-configmap-non-empty","title":"2.6 ConfigMap non-empty","text":"<ul> <li>[ ] <code>model-engine-config</code> ConfigMap exists and has a populated data section:</li> </ul> <pre><code>kubectl get configmap model-engine-config -n {NAMESPACE} -o yaml\n# Expected: data section is non-empty\n</code></pre> <ul> <li>[ ] Embedded kubeconfig has valid clusters and users:</li> </ul> <pre><code>kubectl get configmap model-engine-config -n {NAMESPACE} \\\n  -o jsonpath='{.data.kubeconfig}' \\\n  | python3 -c \"\nimport sys, yaml\nc = yaml.safe_load(sys.stdin)\nprint('clusters:', len(c.get('clusters', [])), 'users:', len(c.get('users', [])))\n\"\n# Expected: clusters: 1 users: 1\n</code></pre>"},{"location":"internal/smoke-tests/#phase-3-control-plane","title":"Phase 3: Control Plane","text":"<p>Commands use k8s cluster DNS. To run from outside the cluster, port-forward first:</p> <pre><code>kubectl port-forward svc/model-engine 8080:80 -n {NAMESPACE}\n# Then set GATEWAY_URL=http://localhost:8080\n</code></pre>"},{"location":"internal/smoke-tests/#31-gateway-health-check","title":"3.1 Gateway health check","text":"<ul> <li>[ ] Gateway returns 200:</li> </ul> <pre><code>curl -sf \"{GATEWAY_URL}/healthz\" &amp;&amp; echo \"OK\"\n# Expected: HTTP 200\n</code></pre>"},{"location":"internal/smoke-tests/#32-authenticated-list-endpoints","title":"3.2 Authenticated list endpoints","text":"<ul> <li>[ ] Returns 200 with valid list (empty is fine):</li> </ul> <pre><code>curl -sf \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  \"{GATEWAY_URL}/v1/model-endpoints\"\n# Expected: HTTP 200, body: {\"model_endpoints\": []}\n</code></pre>"},{"location":"internal/smoke-tests/#phase-4-endpoint-lifecycle-tier-a-cpu-only","title":"Phase 4: Endpoint Lifecycle \u2014 Tier A (CPU Only)","text":"<p>The echo server (<code>model_engine_server.inference.forwarding.echo_server</code>) is built into the model-engine image. It echoes request bodies unchanged \u2014 no GPU or model weights required.</p>"},{"location":"internal/smoke-tests/#41-create-sync-cpu-echo-endpoint","title":"4.1 Create sync CPU echo endpoint","text":"<ul> <li>[ ] Create endpoint:</li> </ul> <pre><code>curl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"smoke-test-echo-sync\",\n    \"model_bundle\": {\n      \"flavor\": \"streaming_enhanced_runnable_image\",\n      \"repository\": \"{MODEL_ENGINE_IMAGE_REPOSITORY}\",\n      \"tag\": \"{MODEL_ENGINE_IMAGE_TAG}\",\n      \"command\": [\n        \"python\", \"-m\",\n        \"model_engine_server.inference.forwarding.echo_server\",\n        \"--port\", \"5005\"\n      ],\n      \"streaming_command\": [\n        \"python\", \"-m\",\n        \"model_engine_server.inference.forwarding.echo_server\",\n        \"--port\", \"5005\"\n      ],\n      \"env\": {\n        \"HTTP_HOST\": \"0.0.0.0\",\n        \"ML_INFRA_SERVICES_CONFIG_PATH\": \"/workspace/model-engine/model_engine_server/core/configs/default.yaml\"\n      },\n      \"protocol\": \"http\",\n      \"readiness_initial_delay_seconds\": 20\n    },\n    \"endpoint_type\": \"sync\",\n    \"cpus\": \"1\",\n    \"memory\": \"1Gi\",\n    \"storage\": \"2Gi\",\n    \"gpus\": 0,\n    \"min_workers\": 1,\n    \"max_workers\": 1,\n    \"per_worker\": 1,\n    \"labels\": {\"team\": \"infra\", \"product\": \"smoke-test\"},\n    \"metadata\": {}\n  }' \\\n  \"{GATEWAY_URL}/v1/model-endpoints\"\n# Expected: HTTP 200, {\"endpoint_creation_task_id\": \"...\"}\n</code></pre>"},{"location":"internal/smoke-tests/#42-poll-until-ready","title":"4.2 Poll until READY","text":"<ul> <li>[ ] Wait for status <code>READY</code> (typically 3\u20138 minutes for CPU):</li> </ul> <pre><code>for i in $(seq 1 50); do\n  STATUS=$(curl -sf \\\n    -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n    \"{GATEWAY_URL}/v1/model-endpoints?name=smoke-test-echo-sync\" \\\n    | python3 -c \"\nimport sys, json\neps = json.load(sys.stdin)['model_endpoints']\nprint(eps[0]['status'] if eps else 'NOT_FOUND')\n\")\n  echo \"[$(date)] Status: $STATUS\"\n  [ \"$STATUS\" = \"READY\" ] &amp;&amp; break\n  sleep 30\ndone\n</code></pre> <p>Stuck INITIALIZING &gt;15 min</p> <p>Service Builder cannot reach the broker. Check builder logs: <code>kubectl logs -n {NAMESPACE} -l app=model-engine-builder --tail=200</code></p> <p>Status unknown</p> <p>Redis auth is broken in the cacher. The endpoint may be running but the Gateway cannot read its state. Check cacher logs: <code>kubectl logs -n {NAMESPACE} -l app=model-engine-cacher --tail=200</code></p>"},{"location":"internal/smoke-tests/#43-send-echo-request-and-verify-response","title":"4.3 Send echo request and verify response","text":"<ul> <li>[ ] Get endpoint ID and send request:</li> </ul> <pre><code>ENDPOINT_ID=$(curl -sf \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  \"{GATEWAY_URL}/v1/model-endpoints?name=smoke-test-echo-sync\" \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['model_endpoints'][0]['id'])\")\n\ncurl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"args\": {\"hello\": \"world\"}}' \\\n  \"{GATEWAY_URL}/v1/sync-tasks?model_endpoint_id=$ENDPOINT_ID\"\n# Expected: {\"status\": \"SUCCESS\", \"result\": {\"hello\": \"world\"}, ...}\n</code></pre>"},{"location":"internal/smoke-tests/#44-create-async-cpu-echo-endpoint","title":"4.4 Create async CPU echo endpoint","text":"<ul> <li>[ ] Create async endpoint (same payload, <code>endpoint_type: async</code>):</li> </ul> <pre><code>curl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"smoke-test-echo-async\",\n    \"model_bundle\": {\n      \"flavor\": \"streaming_enhanced_runnable_image\",\n      \"repository\": \"{MODEL_ENGINE_IMAGE_REPOSITORY}\",\n      \"tag\": \"{MODEL_ENGINE_IMAGE_TAG}\",\n      \"command\": [\n        \"python\", \"-m\",\n        \"model_engine_server.inference.forwarding.echo_server\",\n        \"--port\", \"5005\"\n      ],\n      \"streaming_command\": [\n        \"python\", \"-m\",\n        \"model_engine_server.inference.forwarding.echo_server\",\n        \"--port\", \"5005\"\n      ],\n      \"env\": {\n        \"HTTP_HOST\": \"0.0.0.0\",\n        \"ML_INFRA_SERVICES_CONFIG_PATH\": \"/workspace/model-engine/model_engine_server/core/configs/default.yaml\"\n      },\n      \"protocol\": \"http\",\n      \"readiness_initial_delay_seconds\": 20\n    },\n    \"endpoint_type\": \"async\",\n    \"cpus\": \"1\",\n    \"memory\": \"1Gi\",\n    \"storage\": \"2Gi\",\n    \"gpus\": 0,\n    \"min_workers\": 1,\n    \"max_workers\": 1,\n    \"per_worker\": 1,\n    \"labels\": {\"team\": \"infra\", \"product\": \"smoke-test\"},\n    \"metadata\": {}\n  }' \\\n  \"{GATEWAY_URL}/v1/model-endpoints\"\n</code></pre> <ul> <li>[ ] Poll until <code>smoke-test-echo-async</code> is <code>READY</code> (same pattern as 4.2).</li> </ul>"},{"location":"internal/smoke-tests/#45-send-async-request-and-poll-for-success","title":"4.5 Send async request and poll for SUCCESS","text":"<ul> <li>[ ] Submit task and poll:</li> </ul> <pre><code>ASYNC_ENDPOINT_ID=$(curl -sf \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  \"{GATEWAY_URL}/v1/model-endpoints?name=smoke-test-echo-async\" \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['model_endpoints'][0]['id'])\")\n\nTASK_ID=$(curl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"args\": {\"y\": 1}, \"url\": null}' \\\n  \"{GATEWAY_URL}/v1/async-tasks?model_endpoint_id=$ASYNC_ENDPOINT_ID\" \\\n  | python3 -c \"import sys,json; print(json.load(sys.stdin)['task_id'])\")\necho \"Task ID: $TASK_ID\"\n\nfor i in $(seq 1 30); do\n  RESULT=$(curl -sf \\\n    -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n    \"{GATEWAY_URL}/v1/async-tasks/$TASK_ID\")\n  STATUS=$(echo \"$RESULT\" | python3 -c \"import sys,json; print(json.load(sys.stdin)['status'])\")\n  echo \"[$(date)] Task status: $STATUS\"\n  [ \"$STATUS\" = \"SUCCESS\" ] &amp;&amp; echo \"Result: $RESULT\" &amp;&amp; break\n  [ \"$STATUS\" = \"FAILURE\" ] &amp;&amp; echo \"Task failed: $RESULT\" &amp;&amp; break\n  sleep 10\ndone\n# Expected final status: SUCCESS\n</code></pre>"},{"location":"internal/smoke-tests/#46-cleanup-tier-a","title":"4.6 Cleanup \u2014 Tier A","text":"<ul> <li>[ ] Delete both endpoints:</li> </ul> <pre><code>for NAME in smoke-test-echo-sync smoke-test-echo-async; do\n  ID=$(curl -sf \\\n    -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n    \"{GATEWAY_URL}/v1/model-endpoints?name=$NAME\" \\\n    | python3 -c \"import sys,json; eps=json.load(sys.stdin)['model_endpoints']; print(eps[0]['id'] if eps else '')\")\n  [ -n \"$ID\" ] &amp;&amp; curl -sf -X DELETE \\\n    -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n    \"{GATEWAY_URL}/v1/model-endpoints/$ID\"\n  echo \"Deleted $NAME\"\ndone\n# Expected: {\"deleted\": true} for each\n</code></pre>"},{"location":"internal/smoke-tests/#phase-5-llm-inference-tier-b-gpu-llm","title":"Phase 5: LLM Inference \u2014 Tier B (GPU + LLM)","text":"<p>GPU required</p> <p>Complete Phase 1 checks 1.3\u20131.5 before proceeding. GPU nodes must be available and the vLLM image must be mirrored to the customer registry.</p>"},{"location":"internal/smoke-tests/#51-create-llama-31-8b-llm-endpoint","title":"5.1 Create llama-3.1-8b LLM endpoint","text":"<ul> <li>[ ] Create endpoint with <code>min_workers=1</code>:</li> </ul> <pre><code>curl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"name\": \"smoke-test-llama-3-1-8b\",\n    \"model_name\": \"llama-3.1-8b\",\n    \"source\": \"hugging_face\",\n    \"inference_framework\": \"vllm\",\n    \"inference_framework_image_tag\": \"{VLLM_TAG}\",\n    \"endpoint_type\": \"streaming\",\n    \"cpus\": 20,\n    \"gpus\": 1,\n    \"gpu_type\": \"{GPU_TYPE}\",\n    \"memory\": \"20Gi\",\n    \"storage\": \"40Gi\",\n    \"optimize_costs\": false,\n    \"min_workers\": 1,\n    \"max_workers\": 1,\n    \"per_worker\": 1,\n    \"labels\": {\"team\": \"infra\", \"product\": \"smoke-test\"},\n    \"metadata\": {},\n    \"public_inference\": false\n  }' \\\n  \"{GATEWAY_URL}/v1/llm/model-endpoints\"\n# Expected: HTTP 200\n</code></pre>"},{"location":"internal/smoke-tests/#52-poll-until-ready","title":"5.2 Poll until READY","text":"<ul> <li>[ ] Allow up to 30 minutes for GPU image pull and model load:</li> </ul> <pre><code>for i in $(seq 1 60); do\n  STATUS=$(curl -sf \\\n    -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n    \"{GATEWAY_URL}/v1/llm/model-endpoints/smoke-test-llama-3-1-8b\" \\\n    | python3 -c \"import sys,json; print(json.load(sys.stdin).get('status','NOT_FOUND'))\")\n  echo \"[$(date)] Status: $STATUS\"\n  [ \"$STATUS\" = \"READY\" ] &amp;&amp; break\n  sleep 30\ndone\n</code></pre> <p>Status unknown</p> <p>Redis authentication is broken in the K8s Cacher. Check cacher logs immediately: <pre><code>kubectl logs -n {NAMESPACE} -l app=model-engine-cacher --tail=100 \\\n  | grep -iE \"auth|redis|error\"\n</code></pre></p> <p>Stuck INITIALIZING \u2014 vLLM pod Pending</p> <p>Check whether the vLLM pod has a scheduling or image pull issue: <pre><code>kubectl get pods -n {ENDPOINT_NAMESPACE} | grep smoke-test-llama\nkubectl describe pod {VLLM_POD_NAME} -n {ENDPOINT_NAMESPACE} | grep -A 10 Events\n</code></pre></p>"},{"location":"internal/smoke-tests/#53-post-v2chatcompletions-sync","title":"5.3 POST /v2/chat/completions \u2014 sync","text":"<ul> <li>[ ] Send sync chat completions request and verify non-empty response:</li> </ul> <pre><code>curl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"smoke-test-llama-3-1-8b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}],\n    \"max_tokens\": 50,\n    \"temperature\": 0.0\n  }' \\\n  \"{GATEWAY_URL}/v2/chat/completions\" \\\n  | python3 -c \"\nimport sys, json\nr = json.load(sys.stdin)\ncontent = r['choices'][0]['message']['content']\nassert content, 'Empty response content'\nprint('Response:', content)\n\"\n# Expected: non-empty assistant message\n</code></pre>"},{"location":"internal/smoke-tests/#54-post-v2chatcompletions-streaming-sse","title":"5.4 POST /v2/chat/completions \u2014 streaming (SSE)","text":"<ul> <li>[ ] Verify SSE chunks arrive:</li> </ul> <pre><code>curl -sf -X POST \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Accept: text/event-stream\" \\\n  -d '{\n    \"model\": \"smoke-test-llama-3-1-8b\",\n    \"messages\": [{\"role\": \"user\", \"content\": \"Count from one to five.\"}],\n    \"max_tokens\": 50,\n    \"temperature\": 0.0,\n    \"stream\": true\n  }' \\\n  \"{GATEWAY_URL}/v2/chat/completions\" \\\n  | grep \"^data:\" | grep -v \"\\[DONE\\]\" | head -5\n# Expected: one or more lines of the form:\n#   data: {\"id\":\"...\",\"object\":\"chat.completion.chunk\",\"choices\":[{\"delta\":{\"content\":\"...\"}}]}\n</code></pre>"},{"location":"internal/smoke-tests/#55-cleanup-tier-b","title":"5.5 Cleanup \u2014 Tier B","text":"<ul> <li>[ ] Delete the LLM endpoint:</li> </ul> <pre><code>curl -sf -X DELETE \\\n  -H \"Authorization: Bearer {AUTH_TOKEN}\" \\\n  \"{GATEWAY_URL}/v1/llm/model-endpoints/smoke-test-llama-3-1-8b\"\n# Expected: {\"deleted\": true}\n</code></pre> <ul> <li>[ ] Verify GPU worker pods are terminated:</li> </ul> <pre><code>kubectl get pods -n {ENDPOINT_NAMESPACE} | grep smoke-test-llama\n# Expected: no output\n</code></pre>"},{"location":"internal/smoke-tests/#common-failure-signatures","title":"Common Failure Signatures","text":"<p>Seeded from real deployment incidents.</p> Symptom Likely cause Where to look Endpoint stuck <code>INITIALIZING</code> &gt;15 min Service Builder cannot reach message broker <code>kubectl logs -n {NAMESPACE} -l app=model-engine-builder --tail=200</code> Endpoint status <code>unknown</code> Redis auth failure in K8s Cacher \u2014 Gateway cannot read endpoint state <code>kubectl logs -n {NAMESPACE} -l app=model-engine-cacher --tail=200</code> \u2014 look for <code>AUTH</code> or <code>NOAUTH</code> errors <code>ImagePullBackOff</code> on vLLM pods <code>vllm_repository</code> points to Scale's internal ECR; image not mirrored <code>kubectl describe pod {POD} -n {NS}</code> Events; check <code>vllm_repository</code> in helm values GPU worker pods <code>Pending</code> indefinitely NVIDIA driver not initialized, driver image not pullable, or no matching accelerator label <code>kubectl describe pod {POD}</code> Events; <code>kubectl debug node/{NODE} -- nvidia-smi</code> Random 503s on inference after idle period Azure Service Bus drops idle AMQP connections after 300s Gateway logs for <code>503</code>/<code>timeout</code>; verify <code>broker_pool_limit=0</code> in helm values Permission errors / empty kubeconfig on endpoint creation <code>model-engine-config</code> ConfigMap has empty kubeconfig due to race condition at startup <code>kubectl get configmap model-engine-config -n {NAMESPACE} -o yaml</code> \u2014 check <code>kubeconfig</code> key <code>GET /v1/model-endpoints</code> returns 500 DB unreachable or schema not migrated (<code>db.runDbMigrationScript: false</code> on first install) Gateway pod logs \u2014 look for <code>psycopg2</code> or <code>sqlalchemy</code> errors Builder logs show <code>KombuError</code> or <code>OperationalError</code> <code>celeryBrokerType</code> mismatch or broker credentials wrong Check <code>celeryBrokerType</code> in helm values matches deployed broker"}]}