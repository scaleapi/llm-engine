# Default Configs

# Endpoint config
# K8s namespace the endpoints will be created in
endpoint_namespace: llm-engine

# Asynchronous endpoints
sqs_profile: default
sqs_queue_policy_template: >
  {
    "Version": "2012-10-17",
    "Id": "__default_policy_ID",
    "Statement": [
      {
        "Sid": "__owner_statement",
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::000000000000:root"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:000000000000:${queue_name}"
      },
      {
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::000000000000:role/default"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:000000000000:${queue_name}"
      },
      {
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::000000000000:role/ml_llm_engine"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:000000000000:${queue_name}"
      }
    ]
  }

sqs_queue_tag_template: >
  {
    "infra.scale.com/product": "MLInfraLLMEngineSQS",
    "infra.scale.com/team": "${team}",
    "infra.scale.com/contact": "yi.xu@scale.com",
    "infra.scale.com/customer": "AllCustomers",
    "infra.scale.com/financialOwner": "yi.xu@scale.com",
    "Spellbook-Serve-Endpoint-Id": "${endpoint_id}",
    "Spellbook-Serve-Endpoint-Name": "${endpoint_name}",
    "Spellbook-Serve-Endpoint-Created-By": "${endpoint_created_by}"
  }

# resultsS3Bucket (i.e. where HMI will store model inference results) is currently determined on endpoint creation
# via a request

# modelBundleS3Bucket (i.e. where model bundles are stored) is not determined by any HMI code, but instead
# by some scaleapi routing layer code for scale-hosted HMI, and by request parameters in general.

# Currently, the celery redis used is defaulted to scale's celery redis, and is hardcoded inside scaleml's celery impl.
# We'll need to bundle this celery implementation along for open-source hosting.

# There's a separate piece of infra that caches k8s state onto redis, so we need a url to it
cache_redis_url: redis://redis-elasticache-message-broker.ml-internal.scale.com:6379/15
s3_file_llm_fine_tuning_job_repository: "s3://scale-ml/hosted-model-inference/llm-ft-job-repository/circleci"
