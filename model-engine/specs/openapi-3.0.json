{
  "openapi": "3.1.0",
  "info": {
    "title": "launch",
    "version": "1.0.0"
  },
  "paths": {
    "/v1/batch-jobs": {
      "post": {
        "summary": "Create Batch Job",
        "description": "Runs a batch job.",
        "operationId": "create_batch_job_v1_batch_jobs_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateBatchJobV1Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateBatchJobV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/batch-jobs/{batch_job_id}": {
      "get": {
        "summary": "Get Batch Job",
        "description": "Gets a batch job.",
        "operationId": "get_batch_job_v1_batch_jobs__batch_job_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_job_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Job Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetBatchJobV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "put": {
        "summary": "Update Batch Job",
        "description": "Updates a batch job.",
        "operationId": "update_batch_job_v1_batch_jobs__batch_job_id__put",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_job_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Job Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/UpdateBatchJobV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UpdateBatchJobV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/docker-image-batch-jobs": {
      "post": {
        "summary": "Create Docker Image Batch Job",
        "operationId": "create_docker_image_batch_job_v1_docker_image_batch_jobs_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateDockerImageBatchJobV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateDockerImageBatchJobV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "get": {
        "summary": "List Docker Image Batch Jobs",
        "description": "Lists docker image batch jobs spawned by trigger with given ID",
        "operationId": "list_docker_image_batch_jobs_v1_docker_image_batch_jobs_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "trigger_id",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Trigger Id",
              "type": "string",
              "nullable": true
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListDockerImageBatchJobsV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/docker-image-batch-jobs/{batch_job_id}": {
      "get": {
        "summary": "Get Docker Image Batch Job",
        "operationId": "get_docker_image_batch_job_v1_docker_image_batch_jobs__batch_job_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_job_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Job Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetDockerImageBatchJobV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "put": {
        "summary": "Update Docker Image Batch Job",
        "operationId": "update_docker_image_batch_job_v1_docker_image_batch_jobs__batch_job_id__put",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_job_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Job Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/UpdateDockerImageBatchJobV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UpdateDockerImageBatchJobV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/async-tasks": {
      "post": {
        "summary": "Create Async Inference Task",
        "description": "Runs an async inference prediction.",
        "operationId": "create_async_inference_task_v1_async_tasks_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/EndpointPredictV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateAsyncTaskV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/async-tasks/{task_id}": {
      "get": {
        "summary": "Get Async Inference Task",
        "description": "Gets the status of an async inference task.",
        "operationId": "get_async_inference_task_v1_async_tasks__task_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "task_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Task Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetAsyncTaskV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/sync-tasks": {
      "post": {
        "summary": "Create Sync Inference Task",
        "description": "Runs a sync inference prediction.",
        "operationId": "create_sync_inference_task_v1_sync_tasks_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/SyncEndpointPredictV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/SyncEndpointPredictV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/streaming-tasks": {
      "post": {
        "summary": "Create Streaming Inference Task",
        "description": "Runs a streaming inference prediction.",
        "operationId": "create_streaming_inference_task_v1_streaming_tasks_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/SyncEndpointPredictV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-bundles": {
      "post": {
        "summary": "Create Model Bundle",
        "description": "Creates a ModelBundle for the current user.",
        "operationId": "create_model_bundle_v1_model_bundles_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateModelBundleV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateModelBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "get": {
        "summary": "List Model Bundles",
        "description": "Lists the ModelBundles owned by the current owner.",
        "operationId": "list_model_bundles_v1_model_bundles_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_name",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Model Name",
              "type": "string",
              "nullable": true
            }
          },
          {
            "name": "order_by",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Order By",
              "$ref": "#/components/schemas/ModelBundleOrderBy",
              "nullable": true
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListModelBundlesV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-bundles/clone-with-changes": {
      "post": {
        "summary": "Clone Model Bundle With Changes",
        "description": "Creates a ModelBundle by cloning an existing one and then applying changes on top.",
        "operationId": "clone_model_bundle_with_changes_v1_model_bundles_clone_with_changes_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CloneModelBundleV1Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateModelBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/model-bundles/latest": {
      "get": {
        "summary": "Get Latest Model Bundle",
        "description": "Gets the latest Model Bundle with the given name owned by the current owner.",
        "operationId": "get_latest_model_bundle_v1_model_bundles_latest_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_name",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Name"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ModelBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-bundles/{model_bundle_id}": {
      "get": {
        "summary": "Get Model Bundle",
        "description": "Gets the details for a given ModelBundle owned by the current owner.",
        "operationId": "get_model_bundle_v1_model_bundles__model_bundle_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_bundle_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Bundle Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ModelBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v2/model-bundles": {
      "post": {
        "summary": "Create Model Bundle",
        "description": "Creates a ModelBundle for the current user.",
        "operationId": "create_model_bundle_v2_model_bundles_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateModelBundleV2Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateModelBundleV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "get": {
        "summary": "List Model Bundles",
        "description": "Lists the ModelBundles owned by the current owner.",
        "operationId": "list_model_bundles_v2_model_bundles_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_name",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Model Name",
              "type": "string",
              "nullable": true
            }
          },
          {
            "name": "order_by",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Order By",
              "$ref": "#/components/schemas/ModelBundleOrderBy",
              "nullable": true
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListModelBundlesV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v2/model-bundles/clone-with-changes": {
      "post": {
        "summary": "Clone Model Bundle With Changes",
        "description": "Creates a ModelBundle by cloning an existing one and then applying changes on top.",
        "operationId": "clone_model_bundle_with_changes_v2_model_bundles_clone_with_changes_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CloneModelBundleV2Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateModelBundleV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v2/model-bundles/latest": {
      "get": {
        "summary": "Get Latest Model Bundle",
        "description": "Gets the latest Model Bundle with the given name owned by the current owner.",
        "operationId": "get_latest_model_bundle_v2_model_bundles_latest_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_name",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Name"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ModelBundleV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v2/model-bundles/{model_bundle_id}": {
      "get": {
        "summary": "Get Model Bundle",
        "description": "Gets the details for a given ModelBundle owned by the current owner.",
        "operationId": "get_model_bundle_v2_model_bundles__model_bundle_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_bundle_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Bundle Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ModelBundleV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-endpoints": {
      "post": {
        "summary": "Create Model Endpoint",
        "description": "Creates a Model for the current user.",
        "operationId": "create_model_endpoint_v1_model_endpoints_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateModelEndpointV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "get": {
        "summary": "List Model Endpoints",
        "description": "Lists the Models owned by the current owner.",
        "operationId": "list_model_endpoints_v1_model_endpoints_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "name",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Name",
              "type": "string",
              "nullable": true
            }
          },
          {
            "name": "order_by",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Order By",
              "$ref": "#/components/schemas/ModelEndpointOrderBy",
              "nullable": true
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListModelEndpointsV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-endpoints/{model_endpoint_id}": {
      "get": {
        "summary": "Get Model Endpoint",
        "description": "Describe the Model endpoint with given ID.",
        "operationId": "get_model_endpoint_v1_model_endpoints__model_endpoint_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "put": {
        "summary": "Update Model Endpoint",
        "description": "Updates the Model endpoint.",
        "operationId": "update_model_endpoint_v1_model_endpoints__model_endpoint_id__put",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/UpdateModelEndpointV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UpdateModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "delete": {
        "summary": "Delete Model Endpoint",
        "description": "Lists the Models owned by the current owner.",
        "operationId": "delete_model_endpoint_v1_model_endpoints__model_endpoint_id__delete",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/DeleteModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-endpoints/{model_endpoint_id}/restart": {
      "post": {
        "summary": "Restart Model Endpoint",
        "description": "Restarts the Model endpoint.",
        "operationId": "restart_model_endpoint_v1_model_endpoints__model_endpoint_id__restart_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/RestartModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/model-endpoints-schema.json": {
      "get": {
        "summary": "Get Model Endpoints Schema",
        "description": "Lists the schemas of the Model Endpoints owned by the current owner.",
        "operationId": "get_model_endpoints_schema_v1_model_endpoints_schema_json_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/model-endpoints-api": {
      "get": {
        "summary": "Get Model Endpoints Api",
        "description": "Shows the API of the Model Endpoints owned by the current owner.",
        "operationId": "get_model_endpoints_api_v1_model_endpoints_api_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/docker-image-batch-job-bundles": {
      "post": {
        "summary": "Create Docker Image Batch Job Bundle",
        "description": "Creates a docker iamge batch job bundle",
        "operationId": "create_docker_image_batch_job_bundle_v1_docker_image_batch_job_bundles_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateDockerImageBatchJobBundleV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateDockerImageBatchJobBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "get": {
        "summary": "List Docker Image Batch Job Model Bundles",
        "description": "Lists docker image batch job bundles owned by current owner",
        "operationId": "list_docker_image_batch_job_model_bundles_v1_docker_image_batch_job_bundles_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "bundle_name",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Bundle Name",
              "type": "string",
              "nullable": true
            }
          },
          {
            "name": "order_by",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Order By",
              "$ref": "#/components/schemas/ModelBundleOrderBy",
              "nullable": true
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListDockerImageBatchJobBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/docker-image-batch-job-bundles/latest": {
      "get": {
        "summary": "Get Latest Docker Image Batch Job Bundle",
        "description": "Gets latest Docker Image Batch Job Bundle with given name owned by the current owner",
        "operationId": "get_latest_docker_image_batch_job_bundle_v1_docker_image_batch_job_bundles_latest_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "bundle_name",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Bundle Name"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/DockerImageBatchJobBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/docker-image-batch-job-bundles/{docker_image_batch_job_bundle_id}": {
      "get": {
        "summary": "Get Docker Image Batch Job Model Bundle",
        "description": "Get details for a given DockerImageBatchJobBundle owned by the current owner",
        "operationId": "get_docker_image_batch_job_model_bundle_v1_docker_image_batch_job_bundles__docker_image_batch_job_bundle_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "docker_image_batch_job_bundle_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Docker Image Batch Job Bundle Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/DockerImageBatchJobBundleV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/model-endpoints": {
      "post": {
        "summary": "Create Model Endpoint",
        "operationId": "create_model_endpoint_v1_llm_model_endpoints_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateLLMModelEndpointV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateLLMModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "get": {
        "summary": "List Model Endpoints",
        "description": "Lists the LLM model endpoints owned by the current owner, plus all public_inference LLMs.",
        "operationId": "list_model_endpoints_v1_llm_model_endpoints_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "name",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Name",
              "type": "string",
              "nullable": true
            }
          },
          {
            "name": "order_by",
            "in": "query",
            "required": false,
            "schema": {
              "title": "Order By",
              "$ref": "#/components/schemas/ModelEndpointOrderBy",
              "nullable": true
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListLLMModelEndpointsV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/model-endpoints/{model_endpoint_name}": {
      "get": {
        "summary": "Get Model Endpoint",
        "description": "Describe the LLM Model endpoint with given name.",
        "operationId": "get_model_endpoint_v1_llm_model_endpoints__model_endpoint_name__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_name",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Name"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetLLMModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "put": {
        "summary": "Update Model Endpoint",
        "description": "Updates an LLM endpoint for the current user.",
        "operationId": "update_model_endpoint_v1_llm_model_endpoints__model_endpoint_name__put",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_name",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Name"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/UpdateLLMModelEndpointV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UpdateLLMModelEndpointV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "delete": {
        "summary": "Delete Llm Model Endpoint",
        "operationId": "delete_llm_model_endpoint_v1_llm_model_endpoints__model_endpoint_name__delete",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_name",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Name"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/DeleteLLMEndpointResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/completions-sync": {
      "post": {
        "summary": "Create Completion Sync Task",
        "description": "Runs a sync prompt completion on an LLM.",
        "operationId": "create_completion_sync_task_v1_llm_completions_sync_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_name",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Name"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CompletionSyncV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CompletionSyncV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/completions-stream": {
      "post": {
        "summary": "Create Completion Stream Task",
        "description": "Runs a stream prompt completion on an LLM.",
        "operationId": "create_completion_stream_task_v1_llm_completions_stream_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "model_endpoint_name",
            "in": "query",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Model Endpoint Name"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CompletionStreamV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CompletionStreamV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/fine-tunes": {
      "get": {
        "summary": "List Fine Tunes",
        "operationId": "list_fine_tunes_v1_llm_fine_tunes_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListFineTunesResponse"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      },
      "post": {
        "summary": "Create Fine Tune",
        "operationId": "create_fine_tune_v1_llm_fine_tunes_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateFineTuneRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateFineTuneResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/llm/fine-tunes/{fine_tune_id}": {
      "get": {
        "summary": "Get Fine Tune",
        "operationId": "get_fine_tune_v1_llm_fine_tunes__fine_tune_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "fine_tune_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Fine Tune Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetFineTuneResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/fine-tunes/{fine_tune_id}/cancel": {
      "put": {
        "summary": "Cancel Fine Tune",
        "operationId": "cancel_fine_tune_v1_llm_fine_tunes__fine_tune_id__cancel_put",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "fine_tune_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Fine Tune Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CancelFineTuneResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/fine-tunes/{fine_tune_id}/events": {
      "get": {
        "summary": "Get Fine Tune Events",
        "operationId": "get_fine_tune_events_v1_llm_fine_tunes__fine_tune_id__events_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "fine_tune_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Fine Tune Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetFineTuneEventsResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/llm/model-endpoints/download": {
      "post": {
        "summary": "Download Model Endpoint",
        "operationId": "download_model_endpoint_v1_llm_model_endpoints_download_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/ModelDownloadRequest"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ModelDownloadResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/llm/batch-completions": {
      "post": {
        "summary": "Create Batch Completions",
        "operationId": "create_batch_completions_v1_llm_batch_completions_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateBatchCompletionsV1Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateBatchCompletionsV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/files": {
      "get": {
        "summary": "List Files",
        "operationId": "list_files_v1_files_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListFilesResponse"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      },
      "post": {
        "summary": "Upload File",
        "operationId": "upload_file_v1_files_post",
        "requestBody": {
          "content": {
            "multipart/form-data": {
              "schema": {
                "$ref": "#/components/schemas/Body_upload_file_v1_files_post"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UploadFileResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/files/{file_id}": {
      "get": {
        "summary": "Get File",
        "operationId": "get_file_v1_files__file_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "file_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "File Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetFileResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "delete": {
        "summary": "Delete File",
        "operationId": "delete_file_v1_files__file_id__delete",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "file_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "File Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/DeleteFileResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/files/{file_id}/content": {
      "get": {
        "summary": "Get File Content",
        "description": "Describe the LLM Model endpoint with given name.",
        "operationId": "get_file_content_v1_files__file_id__content_get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "file_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "File Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetFileContentResponse"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v1/triggers": {
      "get": {
        "summary": "List Triggers",
        "description": "Lists descriptions of all triggers",
        "operationId": "list_triggers_v1_triggers_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ListTriggersV1Response"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      },
      "post": {
        "summary": "Create Trigger",
        "description": "Creates and runs a trigger",
        "operationId": "create_trigger_v1_triggers_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateTriggerV1Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CreateTriggerV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v1/triggers/{trigger_id}": {
      "get": {
        "summary": "Get Trigger",
        "description": "Describes the trigger with the given ID",
        "operationId": "get_trigger_v1_triggers__trigger_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "trigger_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Trigger Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetTriggerV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "put": {
        "summary": "Update Trigger",
        "description": "Updates the trigger with the given ID",
        "operationId": "update_trigger_v1_triggers__trigger_id__put",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "trigger_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Trigger Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/UpdateTriggerV1Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UpdateTriggerV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "delete": {
        "summary": "Delete Trigger",
        "description": "Deletes the trigger with the given ID",
        "operationId": "delete_trigger_v1_triggers__trigger_id__delete",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "trigger_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Trigger Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/DeleteTriggerV1Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v2/batch-completions": {
      "post": {
        "summary": "Batch Completions",
        "operationId": "batch_completions_v2_batch_completions_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CreateBatchCompletionsV2Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/BatchCompletionsJob"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v2/batch-completions/{batch_completion_id}": {
      "get": {
        "summary": "Get Batch Completion",
        "operationId": "get_batch_completion_v2_batch_completions__batch_completion_id__get",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_completion_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Completion Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/GetBatchCompletionV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      },
      "post": {
        "summary": "Update Batch Completion",
        "operationId": "update_batch_completion_v2_batch_completions__batch_completion_id__post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_completion_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Completion Id"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/UpdateBatchCompletionsV2Request"
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/UpdateBatchCompletionsV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v2/batch-completions/{batch_completion_id}/actions/cancel": {
      "post": {
        "summary": "Cancel Batch Completion",
        "operationId": "cancel_batch_completion_v2_batch_completions__batch_completion_id__actions_cancel_post",
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ],
        "parameters": [
          {
            "name": "batch_completion_id",
            "in": "path",
            "required": true,
            "schema": {
              "type": "string",
              "title": "Batch Completion Id"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/CancelBatchCompletionsV2Response"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        }
      }
    },
    "/v2/chat/completions": {
      "post": {
        "summary": "Chat Completion",
        "operationId": "chat_completion_v2_chat_completions_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/ChatCompletionV2Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "anyOf": [
                    {
                      "$ref": "#/components/schemas/CreateChatCompletionResponse"
                    },
                    {
                      "$ref": "#/components/schemas/CreateChatCompletionStreamResponse"
                    },
                    {
                      "$ref": "#/components/schemas/ChatCompletionV2StreamErrorChunk"
                    }
                  ],
                  "title": "Response Chat Completion V2 Chat Completions Post"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/v2/completions": {
      "post": {
        "summary": "Completion",
        "operationId": "completion_v2_completions_post",
        "requestBody": {
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/CompletionV2Request"
              }
            }
          },
          "required": true
        },
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {
                  "anyOf": [
                    {
                      "$ref": "#/components/schemas/CreateCompletionResponse"
                    },
                    {
                      "$ref": "#/components/schemas/CompletionV2StreamErrorChunk"
                    }
                  ],
                  "title": "Response Completion V2 Completions Post"
                }
              }
            }
          },
          "422": {
            "description": "Validation Error",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/HTTPValidationError"
                }
              }
            }
          }
        },
        "security": [
          {
            "HTTPBasic": []
          },
          {
            "OAuth2PasswordBearer": []
          }
        ]
      }
    },
    "/healthcheck": {
      "get": {
        "summary": "Healthcheck",
        "description": "Returns 200 if the app is healthy.",
        "operationId": "healthcheck_healthcheck_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          }
        }
      }
    },
    "/healthz": {
      "get": {
        "summary": "Healthcheck",
        "description": "Returns 200 if the app is healthy.",
        "operationId": "healthcheck_healthz_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          }
        }
      }
    },
    "/readyz": {
      "get": {
        "summary": "Healthcheck",
        "description": "Returns 200 if the app is healthy.",
        "operationId": "healthcheck_readyz_get",
        "responses": {
          "200": {
            "description": "Successful Response",
            "content": {
              "application/json": {
                "schema": {}
              }
            }
          }
        }
      }
    }
  },
  "components": {
    "schemas": {
      "Annotation": {
        "properties": {
          "type": {
            "type": "string",
            "const": "url_citation",
            "title": "Type",
            "description": "The type of the URL citation. Always `url_citation`."
          },
          "url_citation": {
            "$ref": "#/components/schemas/UrlCitation",
            "description": "A URL citation when using web search."
          }
        },
        "type": "object",
        "required": [
          "type",
          "url_citation"
        ],
        "title": "Annotation"
      },
      "Audio": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "Unique identifier for a previous audio response from the model.\n"
          }
        },
        "type": "object",
        "required": [
          "id"
        ],
        "title": "Audio"
      },
      "Audio1": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "Unique identifier for this audio response."
          },
          "expires_at": {
            "type": "integer",
            "title": "Expires At",
            "description": "The Unix timestamp (in seconds) for when this audio response will\nno longer be accessible on the server for use in multi-turn\nconversations.\n"
          },
          "data": {
            "type": "string",
            "title": "Data",
            "description": "Base64 encoded audio bytes generated by the model, in the format\nspecified in the request.\n"
          },
          "transcript": {
            "type": "string",
            "title": "Transcript",
            "description": "Transcript of the audio generated by the model."
          }
        },
        "type": "object",
        "required": [
          "id",
          "expires_at",
          "data",
          "transcript"
        ],
        "title": "Audio1"
      },
      "Audio2": {
        "properties": {
          "voice": {
            "$ref": "#/components/schemas/VoiceIdsShared",
            "description": "The voice the model uses to respond. Supported voices are \n`alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `nova`, `onyx`, `sage`, and `shimmer`.\n"
          },
          "format": {
            "type": "string",
            "enum": [
              "wav",
              "aac",
              "mp3",
              "flac",
              "opus",
              "pcm16"
            ],
            "title": "Format",
            "description": "Specifies the output audio format. Must be one of `wav`, `mp3`, `flac`,\n`opus`, or `pcm16`.\n"
          }
        },
        "type": "object",
        "required": [
          "voice",
          "format"
        ],
        "title": "Audio2"
      },
      "BatchCompletionsJob": {
        "properties": {
          "job_id": {
            "type": "string",
            "title": "Job Id"
          },
          "input_data_path": {
            "title": "Input Data Path",
            "description": "Path to the input file. The input file should be a JSON file of type List[CreateBatchCompletionsRequestContent].",
            "type": "string",
            "nullable": true
          },
          "output_data_path": {
            "type": "string",
            "title": "Output Data Path",
            "description": "Path to the output file. The output file will be a JSON file of type List[CompletionOutput]."
          },
          "model_config": {
            "$ref": "#/components/schemas/BatchCompletionsModelConfig",
            "description": "Model configuration for the batch inference. Hardware configurations are inferred."
          },
          "priority": {
            "title": "Priority",
            "description": "Priority of the batch inference job. Default to None.",
            "type": "string",
            "nullable": true
          },
          "status": {
            "$ref": "#/components/schemas/BatchCompletionsJobStatus"
          },
          "created_at": {
            "type": "string",
            "title": "Created At"
          },
          "expires_at": {
            "type": "string",
            "title": "Expires At"
          },
          "completed_at": {
            "title": "Completed At",
            "type": "string",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "job_id",
          "output_data_path",
          "model_config",
          "status",
          "created_at",
          "expires_at",
          "completed_at",
          "metadata"
        ],
        "title": "BatchCompletionsJob"
      },
      "BatchCompletionsJobStatus": {
        "type": "string",
        "enum": [
          "queued",
          "running",
          "completed",
          "failed",
          "cancelled",
          "unknown"
        ],
        "title": "BatchCompletionsJobStatus"
      },
      "BatchCompletionsModelConfig": {
        "properties": {
          "max_model_len": {
            "title": "Max Model Len",
            "description": "Model context length, If unspecified, will be automatically derived from the model config",
            "type": "integer",
            "nullable": true
          },
          "max_num_seqs": {
            "title": "Max Num Seqs",
            "description": "Maximum number of sequences per iteration",
            "type": "integer",
            "nullable": true
          },
          "enforce_eager": {
            "title": "Enforce Eager",
            "description": "Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal perforamnce and flexibility",
            "type": "boolean",
            "nullable": true
          },
          "trust_remote_code": {
            "title": "Trust Remote Code",
            "description": "Whether to trust remote code from Hugging face hub. This is only applicable to models whose code is not supported natively by the transformers library (e.g. deepseek). Default to False.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "pipeline_parallel_size": {
            "title": "Pipeline Parallel Size",
            "description": "Number of pipeline stages. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "tensor_parallel_size": {
            "title": "Tensor Parallel Size",
            "description": "Number of tensor parallel replicas. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "quantization": {
            "title": "Quantization",
            "description": "Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.",
            "type": "string",
            "nullable": true
          },
          "disable_log_requests": {
            "title": "Disable Log Requests",
            "description": "Disable logging requests. Default to None.",
            "type": "boolean",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "tool_call_parser": {
            "title": "Tool Call Parser",
            "description": "Tool call parser",
            "type": "string",
            "nullable": true
          },
          "enable_auto_tool_choice": {
            "title": "Enable Auto Tool Choice",
            "description": "Enable auto tool choice",
            "type": "boolean",
            "nullable": true
          },
          "load_format": {
            "title": "Load Format",
            "description": "The format of the model weights to load.\n\n* \"auto\" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\n* \"pt\" will load the weights in the pytorch bin format.\n* \"safetensors\" will load the weights in the safetensors format.\n* \"npcache\" will load the weights in pytorch format and store a numpy cache to speed up the loading.\n* \"dummy\" will initialize the weights with random values, which is mainly for profiling.\n* \"tensorizer\" will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information.\n* \"bitsandbytes\" will load the weights using bitsandbytes quantization.\n",
            "type": "string",
            "nullable": true
          },
          "config_format": {
            "title": "Config Format",
            "description": "The config format which shall be loaded.  Defaults to 'auto' which defaults to 'hf'.",
            "type": "string",
            "nullable": true
          },
          "tokenizer_mode": {
            "title": "Tokenizer Mode",
            "description": "Tokenizer mode. 'auto' will use the fast tokenizer ifavailable, 'slow' will always use the slow tokenizer, and'mistral' will always use the tokenizer from `mistral_common`.",
            "type": "string",
            "nullable": true
          },
          "limit_mm_per_prompt": {
            "title": "Limit Mm Per Prompt",
            "description": "Maximum number of data instances per modality per prompt. Only applicable for multimodal models.",
            "type": "string",
            "nullable": true
          },
          "max_num_batched_tokens": {
            "title": "Max Num Batched Tokens",
            "description": "Maximum number of batched tokens per iteration",
            "type": "integer",
            "nullable": true
          },
          "tokenizer": {
            "title": "Tokenizer",
            "description": "Name or path of the huggingface tokenizer to use.",
            "type": "string",
            "nullable": true
          },
          "dtype": {
            "title": "Dtype",
            "description": "Data type for model weights and activations. The 'auto' option will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.",
            "type": "string",
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "Random seed for the model.",
            "type": "integer",
            "nullable": true
          },
          "revision": {
            "title": "Revision",
            "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "code_revision": {
            "title": "Code Revision",
            "description": "The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "rope_scaling": {
            "title": "Rope Scaling",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. When using this flag, don't update `max_position_embeddings` to the expected new maximum.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "tokenizer_revision": {
            "title": "Tokenizer Revision",
            "description": "The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "quantization_param_path": {
            "title": "Quantization Param Path",
            "description": "Path to JSON file containing scaling factors. Used to load KV cache scaling factors into the model when KV cache type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also be used to load activation and weight scaling factors when the model dtype is FP8_E4M3 on ROCm.",
            "type": "string",
            "nullable": true
          },
          "max_seq_len_to_capture": {
            "title": "Max Seq Len To Capture",
            "description": "Maximum sequence len covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. Additionally for encoder-decoder models, if the sequence length of the encoder input is larger than this, we fall back to the eager mode.",
            "type": "integer",
            "nullable": true
          },
          "disable_sliding_window": {
            "title": "Disable Sliding Window",
            "description": "Whether to disable sliding window. If True, we will disable the sliding window functionality of the model. If the model does not support sliding window, this argument is ignored.",
            "type": "boolean",
            "nullable": true
          },
          "skip_tokenizer_init": {
            "title": "Skip Tokenizer Init",
            "description": "If true, skip initialization of tokenizer and detokenizer.",
            "type": "boolean",
            "nullable": true
          },
          "served_model_name": {
            "title": "Served Model Name",
            "description": "The model name used in metrics tag `model_name`, matches the model name exposed via the APIs. If multiple model names provided, the first name will be used. If not specified, the model name will be the same as `model`.",
            "type": "string",
            "nullable": true
          },
          "override_neuron_config": {
            "title": "Override Neuron Config",
            "description": "Initialize non default neuron config or override default neuron config that are specific to Neuron devices, this argument will be used to configure the neuron config that can not be gathered from the vllm arguments.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "mm_processor_kwargs": {
            "title": "Mm Processor Kwargs",
            "description": "Arguments to be forwarded to the model's processor for multi-modal data, e.g., image processor.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "block_size": {
            "title": "Block Size",
            "description": "Size of a cache block in number of tokens.",
            "type": "integer",
            "nullable": true
          },
          "gpu_memory_utilization": {
            "title": "Gpu Memory Utilization",
            "description": "Fraction of GPU memory to use for the vLLM execution.",
            "type": "number",
            "nullable": true
          },
          "swap_space": {
            "title": "Swap Space",
            "description": "Size of the CPU swap space per GPU (in GiB).",
            "type": "number",
            "nullable": true
          },
          "cache_dtype": {
            "title": "Cache Dtype",
            "description": "Data type for kv cache storage.",
            "type": "string",
            "nullable": true
          },
          "num_gpu_blocks_override": {
            "title": "Num Gpu Blocks Override",
            "description": "Number of GPU blocks to use. This overrides the profiled num_gpu_blocks if specified. Does nothing if None.",
            "type": "integer",
            "nullable": true
          },
          "enable_prefix_caching": {
            "title": "Enable Prefix Caching",
            "description": "Enables automatic prefix caching.",
            "type": "boolean",
            "nullable": true
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "ID of the model to use.",
            "examples": [
              "mixtral-8x7b-instruct"
            ]
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "description": "Path to the checkpoint to load the model from.",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "description": "\nSuggested number of shards to distribute the model. When not specified, will infer the number of shards based on model config.\nSystem may decide to use a different number than the given value.\n",
            "default": 1,
            "type": "integer",
            "minimum": 1.0,
            "nullable": true
          },
          "max_context_length": {
            "title": "Max Context Length",
            "description": "Maximum context length to use for the model. Defaults to the max allowed by the model. Deprecated in favor of max_model_len.",
            "type": "integer",
            "minimum": 1.0,
            "nullable": true
          },
          "response_role": {
            "title": "Response Role",
            "description": "Role of the response in the conversation. Only supported in chat completions.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "model"
        ],
        "title": "BatchCompletionsModelConfig"
      },
      "BatchJobSerializationFormat": {
        "type": "string",
        "enum": [
          "JSON",
          "PICKLE"
        ],
        "title": "BatchJobSerializationFormat"
      },
      "BatchJobStatus": {
        "type": "string",
        "enum": [
          "PENDING",
          "RUNNING",
          "SUCCESS",
          "FAILURE",
          "CANCELLED",
          "UNDEFINED",
          "TIMEOUT"
        ],
        "title": "BatchJobStatus"
      },
      "Body_upload_file_v1_files_post": {
        "properties": {
          "file": {
            "type": "string",
            "format": "binary",
            "title": "File"
          }
        },
        "type": "object",
        "required": [
          "file"
        ],
        "title": "Body_upload_file_v1_files_post"
      },
      "CallbackAuth": {
        "oneOf": [
          {
            "$ref": "#/components/schemas/CallbackBasicAuth"
          },
          {
            "$ref": "#/components/schemas/CallbackmTLSAuth"
          }
        ],
        "title": "CallbackAuth",
        "discriminator": {
          "propertyName": "kind",
          "mapping": {
            "basic": "#/components/schemas/CallbackBasicAuth",
            "mtls": "#/components/schemas/CallbackmTLSAuth"
          }
        }
      },
      "CallbackBasicAuth": {
        "properties": {
          "kind": {
            "type": "string",
            "const": "basic",
            "title": "Kind"
          },
          "username": {
            "type": "string",
            "title": "Username"
          },
          "password": {
            "type": "string",
            "title": "Password"
          }
        },
        "type": "object",
        "required": [
          "kind",
          "username",
          "password"
        ],
        "title": "CallbackBasicAuth"
      },
      "CallbackmTLSAuth": {
        "properties": {
          "kind": {
            "type": "string",
            "const": "mtls",
            "title": "Kind"
          },
          "cert": {
            "type": "string",
            "title": "Cert"
          },
          "key": {
            "type": "string",
            "title": "Key"
          }
        },
        "type": "object",
        "required": [
          "kind",
          "cert",
          "key"
        ],
        "title": "CallbackmTLSAuth"
      },
      "CancelBatchCompletionsV2Response": {
        "properties": {
          "success": {
            "type": "boolean",
            "title": "Success",
            "description": "Whether the cancellation was successful"
          }
        },
        "type": "object",
        "required": [
          "success"
        ],
        "title": "CancelBatchCompletionsV2Response"
      },
      "CancelFineTuneResponse": {
        "properties": {
          "success": {
            "type": "boolean",
            "title": "Success"
          }
        },
        "type": "object",
        "required": [
          "success"
        ],
        "title": "CancelFineTuneResponse"
      },
      "ChatCompletionFunctionCallOption": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to call."
          }
        },
        "type": "object",
        "required": [
          "name"
        ],
        "title": "ChatCompletionFunctionCallOption"
      },
      "ChatCompletionFunctions": {
        "properties": {
          "description": {
            "title": "Description",
            "description": "A description of what the function does, used by the model to choose when and how to call the function.",
            "type": "string",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."
          },
          "parameters": {
            "$ref": "#/components/schemas/FunctionParameters",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name"
        ],
        "title": "ChatCompletionFunctions"
      },
      "ChatCompletionMessageToolCall": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "The ID of the tool call."
          },
          "type": {
            "type": "string",
            "const": "function",
            "title": "Type",
            "description": "The type of the tool. Currently, only `function` is supported."
          },
          "function": {
            "$ref": "#/components/schemas/Function1",
            "description": "The function that the model called."
          }
        },
        "type": "object",
        "required": [
          "id",
          "type",
          "function"
        ],
        "title": "ChatCompletionMessageToolCall"
      },
      "ChatCompletionMessageToolCallChunk": {
        "properties": {
          "index": {
            "type": "integer",
            "title": "Index"
          },
          "id": {
            "title": "Id",
            "description": "The ID of the tool call.",
            "type": "string",
            "nullable": true
          },
          "type": {
            "title": "Type",
            "description": "The type of the tool. Currently, only `function` is supported.",
            "type": "string",
            "const": "function",
            "nullable": true
          },
          "function": {
            "$ref": "#/components/schemas/Function2",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "index"
        ],
        "title": "ChatCompletionMessageToolCallChunk"
      },
      "ChatCompletionMessageToolCalls-Input": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionMessageToolCall"
        },
        "type": "array",
        "title": "ChatCompletionMessageToolCalls",
        "description": "The tool calls generated by the model, such as function calls."
      },
      "ChatCompletionMessageToolCalls-Output": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionMessageToolCall"
        },
        "type": "array",
        "title": "ChatCompletionMessageToolCalls",
        "description": "The tool calls generated by the model, such as function calls."
      },
      "ChatCompletionNamedToolChoice": {
        "properties": {
          "type": {
            "type": "string",
            "const": "function",
            "title": "Type",
            "description": "The type of the tool. Currently, only `function` is supported."
          },
          "function": {
            "$ref": "#/components/schemas/Function3"
          }
        },
        "type": "object",
        "required": [
          "type",
          "function"
        ],
        "title": "ChatCompletionNamedToolChoice"
      },
      "ChatCompletionRequestAssistantMessage": {
        "properties": {
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/components/schemas/Content"
              }
            ],
            "title": "Content",
            "description": "The contents of the assistant message. Required unless `tool_calls` or `function_call` is specified.\n",
            "nullable": true
          },
          "refusal": {
            "title": "Refusal",
            "description": "The refusal message by the assistant.",
            "type": "string",
            "nullable": true
          },
          "role": {
            "type": "string",
            "const": "assistant",
            "title": "Role",
            "description": "The role of the messages author, in this case `assistant`."
          },
          "name": {
            "title": "Name",
            "description": "An optional name for the participant. Provides the model information to differentiate between participants of the same role.",
            "type": "string",
            "nullable": true
          },
          "audio": {
            "description": "Data about a previous audio response from the model. \n[Learn more](/docs/guides/audio).\n",
            "$ref": "#/components/schemas/Audio",
            "nullable": true
          },
          "tool_calls": {
            "$ref": "#/components/schemas/ChatCompletionMessageToolCalls-Input",
            "nullable": true
          },
          "function_call": {
            "description": "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.",
            "$ref": "#/components/schemas/FunctionCall",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "role"
        ],
        "title": "ChatCompletionRequestAssistantMessage"
      },
      "ChatCompletionRequestAssistantMessageContentPart": {
        "anyOf": [
          {
            "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartText"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartRefusal"
          }
        ],
        "title": "ChatCompletionRequestAssistantMessageContentPart"
      },
      "ChatCompletionRequestDeveloperMessage": {
        "properties": {
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/components/schemas/Content1"
              }
            ],
            "title": "Content",
            "description": "The contents of the developer message."
          },
          "role": {
            "type": "string",
            "const": "developer",
            "title": "Role",
            "description": "The role of the messages author, in this case `developer`."
          },
          "name": {
            "title": "Name",
            "description": "An optional name for the participant. Provides the model information to differentiate between participants of the same role.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "content",
          "role"
        ],
        "title": "ChatCompletionRequestDeveloperMessage"
      },
      "ChatCompletionRequestFunctionMessage": {
        "properties": {
          "role": {
            "type": "string",
            "const": "function",
            "title": "Role",
            "description": "The role of the messages author, in this case `function`."
          },
          "content": {
            "title": "Content",
            "description": "The contents of the function message.",
            "type": "string",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to call."
          }
        },
        "type": "object",
        "required": [
          "role",
          "name"
        ],
        "title": "ChatCompletionRequestFunctionMessage"
      },
      "ChatCompletionRequestMessage": {
        "anyOf": [
          {
            "$ref": "#/components/schemas/ChatCompletionRequestDeveloperMessage"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestSystemMessage"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestUserMessage"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestAssistantMessage"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestToolMessage"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestFunctionMessage"
          }
        ],
        "title": "ChatCompletionRequestMessage"
      },
      "ChatCompletionRequestMessageContentPartAudio": {
        "properties": {
          "type": {
            "type": "string",
            "const": "input_audio",
            "title": "Type",
            "description": "The type of the content part. Always `input_audio`."
          },
          "input_audio": {
            "$ref": "#/components/schemas/InputAudio"
          }
        },
        "type": "object",
        "required": [
          "type",
          "input_audio"
        ],
        "title": "ChatCompletionRequestMessageContentPartAudio"
      },
      "ChatCompletionRequestMessageContentPartFile": {
        "properties": {
          "type": {
            "type": "string",
            "const": "file",
            "title": "Type",
            "description": "The type of the content part. Always `file`."
          },
          "file": {
            "$ref": "#/components/schemas/File"
          }
        },
        "type": "object",
        "required": [
          "type",
          "file"
        ],
        "title": "ChatCompletionRequestMessageContentPartFile"
      },
      "ChatCompletionRequestMessageContentPartImage": {
        "properties": {
          "type": {
            "type": "string",
            "const": "image_url",
            "title": "Type",
            "description": "The type of the content part."
          },
          "image_url": {
            "$ref": "#/components/schemas/ImageUrl"
          }
        },
        "type": "object",
        "required": [
          "type",
          "image_url"
        ],
        "title": "ChatCompletionRequestMessageContentPartImage"
      },
      "ChatCompletionRequestMessageContentPartRefusal": {
        "properties": {
          "type": {
            "type": "string",
            "const": "refusal",
            "title": "Type",
            "description": "The type of the content part."
          },
          "refusal": {
            "type": "string",
            "title": "Refusal",
            "description": "The refusal message generated by the model."
          }
        },
        "type": "object",
        "required": [
          "type",
          "refusal"
        ],
        "title": "ChatCompletionRequestMessageContentPartRefusal"
      },
      "ChatCompletionRequestMessageContentPartText": {
        "properties": {
          "type": {
            "type": "string",
            "const": "text",
            "title": "Type",
            "description": "The type of the content part."
          },
          "text": {
            "type": "string",
            "title": "Text",
            "description": "The text content."
          }
        },
        "type": "object",
        "required": [
          "type",
          "text"
        ],
        "title": "ChatCompletionRequestMessageContentPartText"
      },
      "ChatCompletionRequestSystemMessage": {
        "properties": {
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/components/schemas/Content2"
              }
            ],
            "title": "Content",
            "description": "The contents of the system message."
          },
          "role": {
            "type": "string",
            "const": "system",
            "title": "Role",
            "description": "The role of the messages author, in this case `system`."
          },
          "name": {
            "title": "Name",
            "description": "An optional name for the participant. Provides the model information to differentiate between participants of the same role.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "content",
          "role"
        ],
        "title": "ChatCompletionRequestSystemMessage"
      },
      "ChatCompletionRequestSystemMessageContentPart": {
        "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartText",
        "title": "ChatCompletionRequestSystemMessageContentPart"
      },
      "ChatCompletionRequestToolMessage": {
        "properties": {
          "role": {
            "type": "string",
            "const": "tool",
            "title": "Role",
            "description": "The role of the messages author, in this case `tool`."
          },
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/components/schemas/Content3"
              }
            ],
            "title": "Content",
            "description": "The contents of the tool message."
          },
          "tool_call_id": {
            "type": "string",
            "title": "Tool Call Id",
            "description": "Tool call that this message is responding to."
          }
        },
        "type": "object",
        "required": [
          "role",
          "content",
          "tool_call_id"
        ],
        "title": "ChatCompletionRequestToolMessage"
      },
      "ChatCompletionRequestToolMessageContentPart": {
        "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartText",
        "title": "ChatCompletionRequestToolMessageContentPart"
      },
      "ChatCompletionRequestUserMessage": {
        "properties": {
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/components/schemas/Content4"
              }
            ],
            "title": "Content",
            "description": "The contents of the user message.\n"
          },
          "role": {
            "type": "string",
            "const": "user",
            "title": "Role",
            "description": "The role of the messages author, in this case `user`."
          },
          "name": {
            "title": "Name",
            "description": "An optional name for the participant. Provides the model information to differentiate between participants of the same role.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "content",
          "role"
        ],
        "title": "ChatCompletionRequestUserMessage"
      },
      "ChatCompletionRequestUserMessageContentPart": {
        "anyOf": [
          {
            "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartText"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartImage"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartAudio"
          },
          {
            "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartFile"
          }
        ],
        "title": "ChatCompletionRequestUserMessageContentPart"
      },
      "ChatCompletionResponseMessage": {
        "properties": {
          "content": {
            "title": "Content",
            "description": "The contents of the message.",
            "type": "string",
            "nullable": true
          },
          "refusal": {
            "title": "Refusal",
            "description": "The refusal message generated by the model.",
            "type": "string",
            "nullable": true
          },
          "tool_calls": {
            "$ref": "#/components/schemas/ChatCompletionMessageToolCalls-Output",
            "nullable": true
          },
          "annotations": {
            "title": "Annotations",
            "description": "Annotations for the message, when applicable, as when using the\n[web search tool](/docs/guides/tools-web-search?api-mode=chat).\n",
            "items": {
              "$ref": "#/components/schemas/Annotation"
            },
            "type": "array",
            "nullable": true
          },
          "role": {
            "type": "string",
            "const": "assistant",
            "title": "Role",
            "description": "The role of the author of this message."
          },
          "function_call": {
            "description": "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.",
            "$ref": "#/components/schemas/FunctionCall",
            "nullable": true
          },
          "audio": {
            "description": "If the audio output modality is requested, this object contains data\nabout the audio response from the model. [Learn more](/docs/guides/audio).\n",
            "$ref": "#/components/schemas/Audio1",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "role"
        ],
        "title": "ChatCompletionResponseMessage"
      },
      "ChatCompletionStreamOptions": {
        "properties": {
          "include_usage": {
            "title": "Include Usage",
            "description": "If set, an additional chunk will be streamed before the `data: [DONE]`\nmessage. The `usage` field on this chunk shows the token usage statistics\nfor the entire request, and the `choices` field will always be an empty\narray. \n\nAll other chunks will also include a `usage` field, but with a null\nvalue. **NOTE:** If the stream is interrupted, you may not receive the\nfinal usage chunk which contains the total token usage for the request.\n",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "title": "ChatCompletionStreamOptions"
      },
      "ChatCompletionStreamResponseDelta": {
        "properties": {
          "content": {
            "title": "Content",
            "description": "The contents of the chunk message.",
            "type": "string",
            "nullable": true
          },
          "function_call": {
            "description": "Deprecated and replaced by `tool_calls`. The name and arguments of a function that should be called, as generated by the model.",
            "$ref": "#/components/schemas/FunctionCall2",
            "nullable": true
          },
          "tool_calls": {
            "title": "Tool Calls",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionMessageToolCallChunk"
            },
            "type": "array",
            "nullable": true
          },
          "role": {
            "title": "Role",
            "description": "The role of the author of this message.",
            "type": "string",
            "enum": [
              "developer",
              "system",
              "user",
              "assistant",
              "tool"
            ],
            "nullable": true
          },
          "refusal": {
            "title": "Refusal",
            "description": "The refusal message generated by the model.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "ChatCompletionStreamResponseDelta"
      },
      "ChatCompletionTokenLogprob": {
        "properties": {
          "token": {
            "type": "string",
            "title": "Token",
            "description": "The token."
          },
          "logprob": {
            "type": "number",
            "title": "Logprob",
            "description": "The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely."
          },
          "bytes": {
            "title": "Bytes",
            "description": "A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "top_logprobs": {
            "items": {
              "$ref": "#/components/schemas/TopLogprob"
            },
            "type": "array",
            "title": "Top Logprobs",
            "description": "List of the most likely tokens and their log probability, at this token position. In rare cases, there may be fewer than the number of requested `top_logprobs` returned."
          }
        },
        "type": "object",
        "required": [
          "token",
          "logprob",
          "bytes",
          "top_logprobs"
        ],
        "title": "ChatCompletionTokenLogprob"
      },
      "ChatCompletionTool": {
        "properties": {
          "type": {
            "type": "string",
            "const": "function",
            "title": "Type",
            "description": "The type of the tool. Currently, only `function` is supported."
          },
          "function": {
            "$ref": "#/components/schemas/FunctionObject"
          }
        },
        "type": "object",
        "required": [
          "type",
          "function"
        ],
        "title": "ChatCompletionTool"
      },
      "ChatCompletionToolChoiceOption": {
        "anyOf": [
          {
            "type": "string",
            "enum": [
              "none",
              "auto",
              "required"
            ]
          },
          {
            "$ref": "#/components/schemas/ChatCompletionNamedToolChoice"
          }
        ],
        "title": "ChatCompletionToolChoiceOption",
        "description": "Controls which (if any) tool is called by the model.\n`none` means the model will not call any tool and instead generates a message.\n`auto` means the model can pick between generating a message or calling one or more tools.\n`required` means the model must call one or more tools.\nSpecifying a particular tool via `{\"type\": \"function\", \"function\": {\"name\": \"my_function\"}}` forces the model to call that tool.\n\n`none` is the default when no tools are present. `auto` is the default if tools are present.\n"
      },
      "ChatCompletionV2Request": {
        "properties": {
          "best_of": {
            "title": "Best Of",
            "description": "Number of output sequences that are generated from the prompt.\n            From these `best_of` sequences, the top `n` sequences are returned.\n            `best_of` must be greater than or equal to `n`. This is treated as\n            the beam width when `use_beam_search` is True. By default, `best_of`\n            is set to `n`.",
            "type": "integer",
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "description": "Controls the number of top tokens to consider. -1 means consider all tokens.",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "min_p": {
            "title": "Min P",
            "description": "Float that represents the minimum probability for a token to be\n            considered, relative to the probability of the most likely token.\n            Must be in [0, 1]. Set to 0 to disable this.",
            "type": "number",
            "nullable": true
          },
          "use_beam_search": {
            "title": "Use Beam Search",
            "description": "Whether to use beam search for sampling.",
            "type": "boolean",
            "nullable": true
          },
          "length_penalty": {
            "title": "Length Penalty",
            "description": "Float that penalizes sequences based on their length.\n            Used in beam search.",
            "type": "number",
            "nullable": true
          },
          "repetition_penalty": {
            "title": "Repetition Penalty",
            "description": "Float that penalizes new tokens based on whether\n            they appear in the prompt and the generated text so far. Values > 1\n            encourage the model to use new tokens, while values < 1 encourage\n            the model to repeat tokens.",
            "type": "number",
            "nullable": true
          },
          "early_stopping": {
            "title": "Early Stopping",
            "description": "Controls the stopping condition for beam search. It\n            accepts the following values: `True`, where the generation stops as\n            soon as there are `best_of` complete candidates; `False`, where an\n            heuristic is applied and the generation stops when is it very\n            unlikely to find better candidates; `\"never\"`, where the beam search\n            procedure only stops when there cannot be better candidates\n            (canonical beam search algorithm).",
            "type": "boolean",
            "nullable": true
          },
          "stop_token_ids": {
            "title": "Stop Token Ids",
            "description": "List of tokens that stop the generation when they are\n            generated. The returned output will contain the stop tokens unless\n            the stop tokens are special tokens.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "include_stop_str_in_output": {
            "title": "Include Stop Str In Output",
            "description": "Whether to include the stop strings in\n            output text. Defaults to False.",
            "type": "boolean",
            "nullable": true
          },
          "ignore_eos": {
            "title": "Ignore Eos",
            "description": "Whether to ignore the EOS token and continue generating\n            tokens after the EOS token is generated.",
            "type": "boolean",
            "nullable": true
          },
          "min_tokens": {
            "title": "Min Tokens",
            "description": "Minimum number of tokens to generate per output sequence\n            before EOS or stop_token_ids can be generated",
            "type": "integer",
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "description": "Whether to skip special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "spaces_between_special_tokens": {
            "title": "Spaces Between Special Tokens",
            "description": "Whether to add spaces between special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "echo": {
            "title": "Echo",
            "description": "If true, the new message will be prepended with the last message if they belong to the same role.",
            "type": "boolean",
            "nullable": true
          },
          "add_generation_prompt": {
            "title": "Add Generation Prompt",
            "description": "If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
            "type": "boolean",
            "nullable": true
          },
          "continue_final_message": {
            "title": "Continue Final Message",
            "description": "If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model's response for it. Cannot be used at the same time as `add_generation_prompt`.",
            "type": "boolean",
            "nullable": true
          },
          "add_special_tokens": {
            "title": "Add Special Tokens",
            "description": "If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
            "type": "boolean",
            "nullable": true
          },
          "documents": {
            "title": "Documents",
            "description": "A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.",
            "items": {
              "additionalProperties": {
                "type": "string"
              },
              "type": "object"
            },
            "type": "array",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the model's tokenizer does not define one and no override template is given",
            "type": "string",
            "nullable": true
          },
          "chat_template_kwargs": {
            "title": "Chat Template Kwargs",
            "description": "Additional kwargs to pass to the template renderer. Will be accessible by the chat template.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_json": {
            "title": "Guided Json",
            "description": "JSON schema for guided decoding. Only supported in vllm.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_regex": {
            "title": "Guided Regex",
            "description": "Regex for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_choice": {
            "title": "Guided Choice",
            "description": "Choices for guided decoding. Only supported in vllm.",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "guided_grammar": {
            "title": "Guided Grammar",
            "description": "Context-free grammar for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_decoding_backend": {
            "title": "Guided Decoding Backend",
            "description": "If specified, will override the default guided decoding backend of the server for this specific request. If set, must be either 'outlines' / 'lm-format-enforcer'",
            "type": "string",
            "nullable": true
          },
          "guided_whitespace_pattern": {
            "title": "Guided Whitespace Pattern",
            "description": "If specified, will override the default whitespace pattern for guided json decoding.",
            "type": "string",
            "nullable": true
          },
          "priority": {
            "title": "Priority",
            "description": "The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
            "type": "integer",
            "nullable": true
          },
          "metadata": {
            "$ref": "#/components/schemas/Metadata",
            "nullable": true
          },
          "temperature": {
            "title": "Temperature",
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "description": "An alternative to sampling with temperature, called nucleus sampling,\nwhere the model considers the results of the tokens with top_p probability\nmass. So 0.1 means only the tokens comprising the top 10% probability mass\nare considered.\n\nWe generally recommend altering this or `temperature` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "nullable": true
          },
          "user": {
            "title": "User",
            "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).\n",
            "examples": [
              "user-1234"
            ],
            "type": "string",
            "nullable": true
          },
          "service_tier": {
            "$ref": "#/components/schemas/ServiceTier",
            "nullable": true
          },
          "messages": {
            "items": {
              "$ref": "#/components/schemas/ChatCompletionRequestMessage"
            },
            "type": "array",
            "minItems": 1,
            "title": "Messages",
            "description": "A list of messages comprising the conversation so far. Depending on the\n[model](/docs/models) you use, different message types (modalities) are\nsupported, like [text](/docs/guides/text-generation),\n[images](/docs/guides/vision), and [audio](/docs/guides/audio).\n"
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "ID of the model to use.",
            "examples": [
              "mixtral-8x7b-instruct"
            ]
          },
          "modalities": {
            "$ref": "#/components/schemas/ResponseModalities",
            "nullable": true
          },
          "reasoning_effort": {
            "$ref": "#/components/schemas/ReasoningEffort",
            "nullable": true
          },
          "max_completion_tokens": {
            "title": "Max Completion Tokens",
            "description": "An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).\n",
            "type": "integer",
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on\ntheir existing frequency in the text so far, decreasing the model's\nlikelihood to repeat the same line verbatim.\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on\nwhether they appear in the text so far, increasing the model's likelihood\nto talk about new topics.\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "web_search_options": {
            "title": "Web search",
            "description": "This tool searches the web for relevant results to use in a response.\nLearn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).\n",
            "$ref": "#/components/schemas/WebSearchOptions",
            "nullable": true
          },
          "top_logprobs": {
            "title": "Top Logprobs",
            "description": "An integer between 0 and 20 specifying the number of most likely tokens to\nreturn at each token position, each with an associated log probability.\n`logprobs` must be set to `true` if this parameter is used.\n",
            "type": "integer",
            "maximum": 20.0,
            "minimum": 0.0,
            "nullable": true
          },
          "response_format": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/ResponseFormatText"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonSchema"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonObject"
              }
            ],
            "title": "Response Format",
            "description": "An object specifying the format that the model must output.\n\nSetting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables\nStructured Outputs which ensures the model will match your supplied JSON\nschema. Learn more in the [Structured Outputs\nguide](/docs/guides/structured-outputs).\n\nSetting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\nensures the message the model generates is valid JSON. Using `json_schema`\nis preferred for models that support it.\n",
            "nullable": true
          },
          "audio": {
            "description": "Parameters for audio output. Required when audio output is requested with\n`modalities: [\"audio\"]`. [Learn more](/docs/guides/audio).\n",
            "$ref": "#/components/schemas/Audio2",
            "nullable": true
          },
          "store": {
            "title": "Store",
            "description": "Whether or not to store the output of this chat completion request for \nuse in our [model distillation](/docs/guides/distillation) or\n[evals](/docs/guides/evals) products.\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "stream": {
            "title": "Stream",
            "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "stop": {
            "$ref": "#/components/schemas/StopConfiguration",
            "nullable": true
          },
          "logit_bias": {
            "title": "Logit Bias",
            "description": "Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the\ntokenizer) to an associated bias value from -100 to 100. Mathematically,\nthe bias is added to the logits generated by the model prior to sampling.\nThe exact effect will vary per model, but values between -1 and 1 should\ndecrease or increase likelihood of selection; values like -100 or 100\nshould result in a ban or exclusive selection of the relevant token.\n",
            "additionalProperties": {
              "type": "integer"
            },
            "type": "object",
            "nullable": true
          },
          "logprobs": {
            "title": "Logprobs",
            "description": "Whether to return log probabilities of the output tokens or not. If true,\nreturns the log probabilities of each output token returned in the\n`content` of `message`.\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "max_tokens": {
            "title": "Max Tokens",
            "description": "The maximum number of [tokens](/tokenizer) that can be generated in the\nchat completion. This value can be used to control\n[costs](https://openai.com/api/pricing/) for text generated via API.\n\nThis value is now deprecated in favor of `max_completion_tokens`, and is\nnot compatible with [o-series models](/docs/guides/reasoning).\n",
            "type": "integer",
            "nullable": true
          },
          "n": {
            "title": "N",
            "description": "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.",
            "default": 1,
            "examples": [
              1
            ],
            "type": "integer",
            "maximum": 128.0,
            "minimum": 1.0,
            "nullable": true
          },
          "prediction": {
            "description": "Configuration for a [Predicted Output](/docs/guides/predicted-outputs),\nwhich can greatly improve response times when large parts of the model\nresponse are known ahead of time. This is most common when you are\nregenerating a file with only minor changes to most of the content.\n",
            "$ref": "#/components/schemas/PredictionContent",
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "This feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n",
            "type": "integer",
            "maximum": 9.223372036854776e+18,
            "minimum": -9.223372036854776e+18,
            "nullable": true
          },
          "stream_options": {
            "$ref": "#/components/schemas/ChatCompletionStreamOptions",
            "nullable": true
          },
          "tools": {
            "title": "Tools",
            "description": "A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionTool"
            },
            "type": "array",
            "nullable": true
          },
          "tool_choice": {
            "$ref": "#/components/schemas/ChatCompletionToolChoiceOption",
            "nullable": true
          },
          "parallel_tool_calls": {
            "$ref": "#/components/schemas/ParallelToolCalls",
            "nullable": true
          },
          "function_call": {
            "anyOf": [
              {
                "type": "string",
                "enum": [
                  "none",
                  "auto"
                ]
              },
              {
                "$ref": "#/components/schemas/ChatCompletionFunctionCallOption"
              }
            ],
            "title": "Function Call",
            "description": "Deprecated in favor of `tool_choice`.\n\nControls which (if any) function is called by the model.\n\n`none` means the model will not call a function and instead generates a\nmessage.\n\n`auto` means the model can pick between generating a message or calling a\nfunction.\n\nSpecifying a particular function via `{\"name\": \"my_function\"}` forces the\nmodel to call that function.\n\n`none` is the default when no functions are present. `auto` is the default\nif functions are present.\n",
            "nullable": true
          },
          "functions": {
            "title": "Functions",
            "description": "Deprecated in favor of `tools`.\n\nA list of functions the model may generate JSON inputs for.\n",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionFunctions"
            },
            "type": "array",
            "maxItems": 128,
            "minItems": 1,
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "messages",
          "model"
        ],
        "title": "ChatCompletionV2Request"
      },
      "ChatCompletionV2StreamErrorChunk": {
        "properties": {
          "error": {
            "$ref": "#/components/schemas/StreamError"
          }
        },
        "type": "object",
        "required": [
          "error"
        ],
        "title": "ChatCompletionV2StreamErrorChunk"
      },
      "Choice": {
        "properties": {
          "finish_reason": {
            "type": "string",
            "enum": [
              "stop",
              "length",
              "tool_calls",
              "content_filter",
              "function_call"
            ],
            "title": "Finish Reason",
            "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,\n`length` if the maximum number of tokens specified in the request was reached,\n`content_filter` if content was omitted due to a flag from our content filters,\n`tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.\n"
          },
          "index": {
            "type": "integer",
            "title": "Index",
            "description": "The index of the choice in the list of choices."
          },
          "message": {
            "$ref": "#/components/schemas/ChatCompletionResponseMessage"
          },
          "logprobs": {
            "description": "Log probability information for the choice.",
            "$ref": "#/components/schemas/Logprobs",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "finish_reason",
          "index",
          "message",
          "logprobs"
        ],
        "title": "Choice"
      },
      "Choice1": {
        "properties": {
          "delta": {
            "$ref": "#/components/schemas/ChatCompletionStreamResponseDelta"
          },
          "logprobs": {
            "description": "Log probability information for the choice.",
            "$ref": "#/components/schemas/Logprobs",
            "nullable": true
          },
          "finish_reason": {
            "title": "Finish Reason",
            "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,\n`length` if the maximum number of tokens specified in the request was reached,\n`content_filter` if content was omitted due to a flag from our content filters,\n`tool_calls` if the model called a tool, or `function_call` (deprecated) if the model called a function.\n",
            "type": "string",
            "enum": [
              "stop",
              "length",
              "tool_calls",
              "content_filter",
              "function_call"
            ],
            "nullable": true
          },
          "index": {
            "type": "integer",
            "title": "Index",
            "description": "The index of the choice in the list of choices."
          }
        },
        "type": "object",
        "required": [
          "delta",
          "finish_reason",
          "index"
        ],
        "title": "Choice1"
      },
      "Choice2": {
        "properties": {
          "finish_reason": {
            "type": "string",
            "enum": [
              "stop",
              "length",
              "content_filter"
            ],
            "title": "Finish Reason",
            "description": "The reason the model stopped generating tokens. This will be `stop` if the model hit a natural stop point or a provided stop sequence,\n`length` if the maximum number of tokens specified in the request was reached,\nor `content_filter` if content was omitted due to a flag from our content filters.\n"
          },
          "index": {
            "type": "integer",
            "title": "Index"
          },
          "logprobs": {
            "$ref": "#/components/schemas/Logprobs2",
            "nullable": true
          },
          "text": {
            "type": "string",
            "title": "Text"
          }
        },
        "type": "object",
        "required": [
          "finish_reason",
          "index",
          "logprobs",
          "text"
        ],
        "title": "Choice2"
      },
      "CloneModelBundleV1Request": {
        "properties": {
          "original_model_bundle_id": {
            "type": "string",
            "title": "Original Model Bundle Id"
          },
          "new_app_config": {
            "title": "New App Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "original_model_bundle_id"
        ],
        "title": "CloneModelBundleV1Request",
        "description": "Request object for cloning a Model Bundle from another one."
      },
      "CloneModelBundleV2Request": {
        "properties": {
          "original_model_bundle_id": {
            "type": "string",
            "title": "Original Model Bundle Id"
          },
          "new_app_config": {
            "title": "New App Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "original_model_bundle_id"
        ],
        "title": "CloneModelBundleV2Request",
        "description": "Request object for cloning a Model Bundle from another one."
      },
      "CloudpickleArtifactFlavor": {
        "properties": {
          "requirements": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Requirements"
          },
          "framework": {
            "oneOf": [
              {
                "$ref": "#/components/schemas/PytorchFramework"
              },
              {
                "$ref": "#/components/schemas/TensorflowFramework"
              },
              {
                "$ref": "#/components/schemas/CustomFramework"
              }
            ],
            "title": "Framework",
            "discriminator": {
              "propertyName": "framework_type",
              "mapping": {
                "custom_base_image": "#/components/schemas/CustomFramework",
                "pytorch": "#/components/schemas/PytorchFramework",
                "tensorflow": "#/components/schemas/TensorflowFramework"
              }
            }
          },
          "app_config": {
            "title": "App Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "location": {
            "type": "string",
            "title": "Location"
          },
          "flavor": {
            "type": "string",
            "const": "cloudpickle_artifact",
            "title": "Flavor"
          },
          "load_predict_fn": {
            "type": "string",
            "title": "Load Predict Fn"
          },
          "load_model_fn": {
            "type": "string",
            "title": "Load Model Fn"
          }
        },
        "type": "object",
        "required": [
          "requirements",
          "framework",
          "location",
          "flavor",
          "load_predict_fn",
          "load_model_fn"
        ],
        "title": "CloudpickleArtifactFlavor",
        "description": "This is the entity-layer class for the Model Bundle flavor of a cloudpickle artifact."
      },
      "CompletionOutput": {
        "properties": {
          "text": {
            "type": "string",
            "title": "Text"
          },
          "num_prompt_tokens": {
            "title": "Num Prompt Tokens",
            "type": "integer",
            "nullable": true
          },
          "num_completion_tokens": {
            "type": "integer",
            "title": "Num Completion Tokens"
          },
          "tokens": {
            "title": "Tokens",
            "items": {
              "$ref": "#/components/schemas/TokenOutput"
            },
            "type": "array",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "text",
          "num_completion_tokens"
        ],
        "title": "CompletionOutput",
        "description": "Represents the output of a completion request to a model."
      },
      "CompletionStreamOutput": {
        "properties": {
          "text": {
            "type": "string",
            "title": "Text"
          },
          "finished": {
            "type": "boolean",
            "title": "Finished"
          },
          "num_prompt_tokens": {
            "title": "Num Prompt Tokens",
            "type": "integer",
            "nullable": true
          },
          "num_completion_tokens": {
            "title": "Num Completion Tokens",
            "type": "integer",
            "nullable": true
          },
          "token": {
            "$ref": "#/components/schemas/TokenOutput",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "text",
          "finished"
        ],
        "title": "CompletionStreamOutput"
      },
      "CompletionStreamV1Request": {
        "properties": {
          "prompt": {
            "type": "string",
            "title": "Prompt"
          },
          "max_new_tokens": {
            "type": "integer",
            "title": "Max New Tokens"
          },
          "temperature": {
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "title": "Temperature"
          },
          "stop_sequences": {
            "title": "Stop Sequences",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "return_token_log_probs": {
            "title": "Return Token Log Probs",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "type": "number",
            "maximum": 1.0,
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "include_stop_str_in_output": {
            "title": "Include Stop Str In Output",
            "type": "boolean",
            "nullable": true
          },
          "guided_json": {
            "title": "Guided Json",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_regex": {
            "title": "Guided Regex",
            "type": "string",
            "nullable": true
          },
          "guided_choice": {
            "title": "Guided Choice",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "guided_grammar": {
            "title": "Guided Grammar",
            "type": "string",
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "default": true,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "prompt",
          "max_new_tokens",
          "temperature"
        ],
        "title": "CompletionStreamV1Request",
        "description": "Request object for a stream prompt completion task."
      },
      "CompletionStreamV1Response": {
        "properties": {
          "request_id": {
            "title": "Request Id",
            "type": "string",
            "nullable": true
          },
          "output": {
            "$ref": "#/components/schemas/CompletionStreamOutput",
            "nullable": true
          },
          "error": {
            "$ref": "#/components/schemas/StreamError",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "request_id"
        ],
        "title": "CompletionStreamV1Response",
        "description": "Error of the response (if any)."
      },
      "CompletionSyncV1Request": {
        "properties": {
          "prompt": {
            "type": "string",
            "title": "Prompt"
          },
          "max_new_tokens": {
            "type": "integer",
            "title": "Max New Tokens"
          },
          "temperature": {
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "title": "Temperature"
          },
          "stop_sequences": {
            "title": "Stop Sequences",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "return_token_log_probs": {
            "title": "Return Token Log Probs",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "type": "number",
            "maximum": 1.0,
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "include_stop_str_in_output": {
            "title": "Include Stop Str In Output",
            "type": "boolean",
            "nullable": true
          },
          "guided_json": {
            "title": "Guided Json",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_regex": {
            "title": "Guided Regex",
            "type": "string",
            "nullable": true
          },
          "guided_choice": {
            "title": "Guided Choice",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "guided_grammar": {
            "title": "Guided Grammar",
            "type": "string",
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "default": true,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "prompt",
          "max_new_tokens",
          "temperature"
        ],
        "title": "CompletionSyncV1Request",
        "description": "Request object for a synchronous prompt completion task."
      },
      "CompletionSyncV1Response": {
        "properties": {
          "request_id": {
            "title": "Request Id",
            "type": "string",
            "nullable": true
          },
          "output": {
            "$ref": "#/components/schemas/CompletionOutput",
            "nullable": true
          }
        },
        "type": "object",
        "title": "CompletionSyncV1Response",
        "description": "Response object for a synchronous prompt completion."
      },
      "CompletionTokensDetails": {
        "properties": {
          "accepted_prediction_tokens": {
            "type": "integer",
            "title": "Accepted Prediction Tokens",
            "description": "When using Predicted Outputs, the number of tokens in the\nprediction that appeared in the completion.\n",
            "default": 0
          },
          "audio_tokens": {
            "type": "integer",
            "title": "Audio Tokens",
            "description": "Audio input tokens generated by the model.",
            "default": 0
          },
          "reasoning_tokens": {
            "type": "integer",
            "title": "Reasoning Tokens",
            "description": "Tokens generated by the model for reasoning.",
            "default": 0
          },
          "rejected_prediction_tokens": {
            "type": "integer",
            "title": "Rejected Prediction Tokens",
            "description": "When using Predicted Outputs, the number of tokens in the\nprediction that did not appear in the completion. However, like\nreasoning tokens, these tokens are still counted in the total\ncompletion tokens for purposes of billing, output, and context window\nlimits.\n",
            "default": 0
          }
        },
        "type": "object",
        "title": "CompletionTokensDetails"
      },
      "CompletionUsage": {
        "properties": {
          "completion_tokens": {
            "type": "integer",
            "title": "Completion Tokens",
            "description": "Number of tokens in the generated completion."
          },
          "prompt_tokens": {
            "type": "integer",
            "title": "Prompt Tokens",
            "description": "Number of tokens in the prompt."
          },
          "total_tokens": {
            "type": "integer",
            "title": "Total Tokens",
            "description": "Total number of tokens used in the request (prompt + completion)."
          },
          "completion_tokens_details": {
            "description": "Breakdown of tokens used in a completion.",
            "$ref": "#/components/schemas/CompletionTokensDetails",
            "nullable": true
          },
          "prompt_tokens_details": {
            "description": "Breakdown of tokens used in the prompt.",
            "$ref": "#/components/schemas/PromptTokensDetails",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "completion_tokens",
          "prompt_tokens",
          "total_tokens"
        ],
        "title": "CompletionUsage"
      },
      "CompletionV2Request": {
        "properties": {
          "best_of": {
            "title": "Best Of",
            "description": "Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\n\nWhen used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return \u2013 `best_of` must be greater than `n`.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n",
            "default": 1,
            "type": "integer",
            "maximum": 20.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "description": "Controls the number of top tokens to consider. -1 means consider all tokens.",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "min_p": {
            "title": "Min P",
            "description": "Float that represents the minimum probability for a token to be\n            considered, relative to the probability of the most likely token.\n            Must be in [0, 1]. Set to 0 to disable this.",
            "type": "number",
            "nullable": true
          },
          "use_beam_search": {
            "title": "Use Beam Search",
            "description": "Whether to use beam search for sampling.",
            "type": "boolean",
            "nullable": true
          },
          "length_penalty": {
            "title": "Length Penalty",
            "description": "Float that penalizes sequences based on their length.\n            Used in beam search.",
            "type": "number",
            "nullable": true
          },
          "repetition_penalty": {
            "title": "Repetition Penalty",
            "description": "Float that penalizes new tokens based on whether\n            they appear in the prompt and the generated text so far. Values > 1\n            encourage the model to use new tokens, while values < 1 encourage\n            the model to repeat tokens.",
            "type": "number",
            "nullable": true
          },
          "early_stopping": {
            "title": "Early Stopping",
            "description": "Controls the stopping condition for beam search. It\n            accepts the following values: `True`, where the generation stops as\n            soon as there are `best_of` complete candidates; `False`, where an\n            heuristic is applied and the generation stops when is it very\n            unlikely to find better candidates; `\"never\"`, where the beam search\n            procedure only stops when there cannot be better candidates\n            (canonical beam search algorithm).",
            "type": "boolean",
            "nullable": true
          },
          "stop_token_ids": {
            "title": "Stop Token Ids",
            "description": "List of tokens that stop the generation when they are\n            generated. The returned output will contain the stop tokens unless\n            the stop tokens are special tokens.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "include_stop_str_in_output": {
            "title": "Include Stop Str In Output",
            "description": "Whether to include the stop strings in output text.",
            "type": "boolean",
            "nullable": true
          },
          "ignore_eos": {
            "title": "Ignore Eos",
            "description": "Whether to ignore the EOS token and continue generating\n            tokens after the EOS token is generated.",
            "type": "boolean",
            "nullable": true
          },
          "min_tokens": {
            "title": "Min Tokens",
            "description": "Minimum number of tokens to generate per output sequence\n            before EOS or stop_token_ids can be generated",
            "type": "integer",
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "description": "Whether to skip special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "spaces_between_special_tokens": {
            "title": "Spaces Between Special Tokens",
            "description": "Whether to add spaces between special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "add_special_tokens": {
            "title": "Add Special Tokens",
            "description": "If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
            "type": "boolean",
            "nullable": true
          },
          "response_format": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/ResponseFormatText"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonSchema"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonObject"
              }
            ],
            "title": "Response Format",
            "description": "Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'} or {'type': 'text' } is supported.",
            "nullable": true
          },
          "guided_json": {
            "title": "Guided Json",
            "description": "JSON schema for guided decoding. Only supported in vllm.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_regex": {
            "title": "Guided Regex",
            "description": "Regex for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_choice": {
            "title": "Guided Choice",
            "description": "Choices for guided decoding. Only supported in vllm.",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "guided_grammar": {
            "title": "Guided Grammar",
            "description": "Context-free grammar for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_decoding_backend": {
            "title": "Guided Decoding Backend",
            "description": "If specified, will override the default guided decoding backend of the server for this specific request. If set, must be either 'outlines' / 'lm-format-enforcer'",
            "type": "string",
            "nullable": true
          },
          "guided_whitespace_pattern": {
            "title": "Guided Whitespace Pattern",
            "description": "If specified, will override the default whitespace pattern for guided json decoding.",
            "type": "string",
            "nullable": true
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "ID of the model to use.",
            "examples": [
              "mixtral-8x7b-instruct"
            ]
          },
          "prompt": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "$ref": "#/components/schemas/Prompt"
              },
              {
                "$ref": "#/components/schemas/Prompt1"
              }
            ],
            "title": "Prompt",
            "description": "The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n",
            "nullable": true
          },
          "echo": {
            "title": "Echo",
            "description": "Echo back the prompt in addition to the completion\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation)\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "logit_bias": {
            "title": "Logit Bias",
            "description": "Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nAs an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.\n",
            "additionalProperties": {
              "type": "integer"
            },
            "type": "object",
            "nullable": true
          },
          "logprobs": {
            "title": "Logprobs",
            "description": "Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nThe maximum value for `logprobs` is 5.\n",
            "type": "integer",
            "maximum": 5.0,
            "minimum": 0.0,
            "nullable": true
          },
          "max_tokens": {
            "title": "Max Tokens",
            "description": "The maximum number of [tokens](/tokenizer) that can be generated in the completion.\n\nThe token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n",
            "default": 16,
            "examples": [
              16
            ],
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          },
          "n": {
            "title": "N",
            "description": "How many completions to generate for each prompt.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "integer",
            "maximum": 128.0,
            "minimum": 1.0,
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation)\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\n\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n",
            "type": "integer",
            "nullable": true
          },
          "stop": {
            "$ref": "#/components/schemas/StopConfiguration",
            "nullable": true
          },
          "stream": {
            "title": "Stream",
            "description": "If set, partial message deltas will be sent. Tokens will be sent as data-only [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) as they become available, with the stream terminated by a `data: [DONE]` message. [Example Python code](https://cookbook.openai.com/examples/how_to_stream_completions).\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "stream_options": {
            "$ref": "#/components/schemas/ChatCompletionStreamOptions",
            "nullable": true
          },
          "suffix": {
            "title": "Suffix",
            "description": "The suffix that comes after a completion of inserted text.\n\nThis parameter is only supported for `gpt-3.5-turbo-instruct`.\n",
            "examples": [
              "test."
            ],
            "type": "string",
            "nullable": true
          },
          "temperature": {
            "title": "Temperature",
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "nullable": true
          },
          "user": {
            "title": "User",
            "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).\n",
            "examples": [
              "user-1234"
            ],
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "model",
          "prompt"
        ],
        "title": "CompletionV2Request"
      },
      "CompletionV2StreamErrorChunk": {
        "properties": {
          "error": {
            "$ref": "#/components/schemas/StreamError"
          }
        },
        "type": "object",
        "required": [
          "error"
        ],
        "title": "CompletionV2StreamErrorChunk"
      },
      "Content": {
        "title": "Content",
        "description": "An array of content parts with a defined type. Can be one or more of type `text`, or exactly one of type `refusal`.",
        "items": {
          "$ref": "#/components/schemas/ChatCompletionRequestAssistantMessageContentPart"
        },
        "type": "array",
        "minItems": 1,
        "nullable": true
      },
      "Content1": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        },
        "type": "array",
        "minItems": 1,
        "title": "Content1",
        "description": "An array of content parts with a defined type. For developer messages, only type `text` is supported."
      },
      "Content2": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionRequestSystemMessageContentPart"
        },
        "type": "array",
        "minItems": 1,
        "title": "Content2",
        "description": "An array of content parts with a defined type. For system messages, only type `text` is supported."
      },
      "Content3": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionRequestToolMessageContentPart"
        },
        "type": "array",
        "minItems": 1,
        "title": "Content3",
        "description": "An array of content parts with a defined type. For tool messages, only type `text` is supported."
      },
      "Content4": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionRequestUserMessageContentPart"
        },
        "type": "array",
        "minItems": 1,
        "title": "Content4",
        "description": "An array of content parts with a defined type. Supported options differ based on the [model](/docs/models) being used to generate the response. Can contain text, image, or audio inputs."
      },
      "Content8": {
        "items": {
          "$ref": "#/components/schemas/ChatCompletionRequestMessageContentPartText"
        },
        "type": "array",
        "minItems": 1,
        "title": "Content8",
        "description": "An array of content parts with a defined type. Supported options differ based on the [model](/docs/models) being used to generate the response. Can contain text inputs."
      },
      "CreateAsyncTaskV1Response": {
        "properties": {
          "task_id": {
            "type": "string",
            "title": "Task Id"
          }
        },
        "type": "object",
        "required": [
          "task_id"
        ],
        "title": "CreateAsyncTaskV1Response"
      },
      "CreateBatchCompletionsV1ModelConfig": {
        "properties": {
          "max_model_len": {
            "title": "Max Model Len",
            "description": "Model context length, If unspecified, will be automatically derived from the model config",
            "type": "integer",
            "nullable": true
          },
          "max_num_seqs": {
            "title": "Max Num Seqs",
            "description": "Maximum number of sequences per iteration",
            "type": "integer",
            "nullable": true
          },
          "enforce_eager": {
            "title": "Enforce Eager",
            "description": "Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal perforamnce and flexibility",
            "type": "boolean",
            "nullable": true
          },
          "trust_remote_code": {
            "title": "Trust Remote Code",
            "description": "Whether to trust remote code from Hugging face hub. This is only applicable to models whose code is not supported natively by the transformers library (e.g. deepseek). Default to False.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "pipeline_parallel_size": {
            "title": "Pipeline Parallel Size",
            "description": "Number of pipeline stages. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "tensor_parallel_size": {
            "title": "Tensor Parallel Size",
            "description": "Number of tensor parallel replicas. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "quantization": {
            "title": "Quantization",
            "description": "Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.",
            "type": "string",
            "nullable": true
          },
          "disable_log_requests": {
            "title": "Disable Log Requests",
            "description": "Disable logging requests. Default to None.",
            "type": "boolean",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "tool_call_parser": {
            "title": "Tool Call Parser",
            "description": "Tool call parser",
            "type": "string",
            "nullable": true
          },
          "enable_auto_tool_choice": {
            "title": "Enable Auto Tool Choice",
            "description": "Enable auto tool choice",
            "type": "boolean",
            "nullable": true
          },
          "load_format": {
            "title": "Load Format",
            "description": "The format of the model weights to load.\n\n* \"auto\" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\n* \"pt\" will load the weights in the pytorch bin format.\n* \"safetensors\" will load the weights in the safetensors format.\n* \"npcache\" will load the weights in pytorch format and store a numpy cache to speed up the loading.\n* \"dummy\" will initialize the weights with random values, which is mainly for profiling.\n* \"tensorizer\" will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information.\n* \"bitsandbytes\" will load the weights using bitsandbytes quantization.\n",
            "type": "string",
            "nullable": true
          },
          "config_format": {
            "title": "Config Format",
            "description": "The config format which shall be loaded.  Defaults to 'auto' which defaults to 'hf'.",
            "type": "string",
            "nullable": true
          },
          "tokenizer_mode": {
            "title": "Tokenizer Mode",
            "description": "Tokenizer mode. 'auto' will use the fast tokenizer ifavailable, 'slow' will always use the slow tokenizer, and'mistral' will always use the tokenizer from `mistral_common`.",
            "type": "string",
            "nullable": true
          },
          "limit_mm_per_prompt": {
            "title": "Limit Mm Per Prompt",
            "description": "Maximum number of data instances per modality per prompt. Only applicable for multimodal models.",
            "type": "string",
            "nullable": true
          },
          "max_num_batched_tokens": {
            "title": "Max Num Batched Tokens",
            "description": "Maximum number of batched tokens per iteration",
            "type": "integer",
            "nullable": true
          },
          "tokenizer": {
            "title": "Tokenizer",
            "description": "Name or path of the huggingface tokenizer to use.",
            "type": "string",
            "nullable": true
          },
          "dtype": {
            "title": "Dtype",
            "description": "Data type for model weights and activations. The 'auto' option will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.",
            "type": "string",
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "Random seed for the model.",
            "type": "integer",
            "nullable": true
          },
          "revision": {
            "title": "Revision",
            "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "code_revision": {
            "title": "Code Revision",
            "description": "The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "rope_scaling": {
            "title": "Rope Scaling",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. When using this flag, don't update `max_position_embeddings` to the expected new maximum.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "tokenizer_revision": {
            "title": "Tokenizer Revision",
            "description": "The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "quantization_param_path": {
            "title": "Quantization Param Path",
            "description": "Path to JSON file containing scaling factors. Used to load KV cache scaling factors into the model when KV cache type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also be used to load activation and weight scaling factors when the model dtype is FP8_E4M3 on ROCm.",
            "type": "string",
            "nullable": true
          },
          "max_seq_len_to_capture": {
            "title": "Max Seq Len To Capture",
            "description": "Maximum sequence len covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. Additionally for encoder-decoder models, if the sequence length of the encoder input is larger than this, we fall back to the eager mode.",
            "type": "integer",
            "nullable": true
          },
          "disable_sliding_window": {
            "title": "Disable Sliding Window",
            "description": "Whether to disable sliding window. If True, we will disable the sliding window functionality of the model. If the model does not support sliding window, this argument is ignored.",
            "type": "boolean",
            "nullable": true
          },
          "skip_tokenizer_init": {
            "title": "Skip Tokenizer Init",
            "description": "If true, skip initialization of tokenizer and detokenizer.",
            "type": "boolean",
            "nullable": true
          },
          "served_model_name": {
            "title": "Served Model Name",
            "description": "The model name used in metrics tag `model_name`, matches the model name exposed via the APIs. If multiple model names provided, the first name will be used. If not specified, the model name will be the same as `model`.",
            "type": "string",
            "nullable": true
          },
          "override_neuron_config": {
            "title": "Override Neuron Config",
            "description": "Initialize non default neuron config or override default neuron config that are specific to Neuron devices, this argument will be used to configure the neuron config that can not be gathered from the vllm arguments.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "mm_processor_kwargs": {
            "title": "Mm Processor Kwargs",
            "description": "Arguments to be forwarded to the model's processor for multi-modal data, e.g., image processor.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "block_size": {
            "title": "Block Size",
            "description": "Size of a cache block in number of tokens.",
            "type": "integer",
            "nullable": true
          },
          "gpu_memory_utilization": {
            "title": "Gpu Memory Utilization",
            "description": "Fraction of GPU memory to use for the vLLM execution.",
            "type": "number",
            "nullable": true
          },
          "swap_space": {
            "title": "Swap Space",
            "description": "Size of the CPU swap space per GPU (in GiB).",
            "type": "number",
            "nullable": true
          },
          "cache_dtype": {
            "title": "Cache Dtype",
            "description": "Data type for kv cache storage.",
            "type": "string",
            "nullable": true
          },
          "num_gpu_blocks_override": {
            "title": "Num Gpu Blocks Override",
            "description": "Number of GPU blocks to use. This overrides the profiled num_gpu_blocks if specified. Does nothing if None.",
            "type": "integer",
            "nullable": true
          },
          "enable_prefix_caching": {
            "title": "Enable Prefix Caching",
            "description": "Enables automatic prefix caching.",
            "type": "boolean",
            "nullable": true
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "ID of the model to use.",
            "examples": [
              "mixtral-8x7b-instruct"
            ]
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "description": "Path to the checkpoint to load the model from.",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "description": "\nSuggested number of shards to distribute the model. When not specified, will infer the number of shards based on model config.\nSystem may decide to use a different number than the given value.\n",
            "default": 1,
            "type": "integer",
            "minimum": 1.0,
            "nullable": true
          },
          "max_context_length": {
            "title": "Max Context Length",
            "description": "Maximum context length to use for the model. Defaults to the max allowed by the model. Deprecated in favor of max_model_len.",
            "type": "integer",
            "minimum": 1.0,
            "nullable": true
          },
          "response_role": {
            "title": "Response Role",
            "description": "Role of the response in the conversation. Only supported in chat completions.",
            "type": "string",
            "nullable": true
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels",
            "description": "Labels to attach to the batch inference job.",
            "default": {}
          }
        },
        "type": "object",
        "required": [
          "model"
        ],
        "title": "CreateBatchCompletionsV1ModelConfig"
      },
      "CreateBatchCompletionsV1Request": {
        "properties": {
          "input_data_path": {
            "title": "Input Data Path",
            "description": "Path to the input file. The input file should be a JSON file of type List[CreateBatchCompletionsRequestContent].",
            "type": "string",
            "nullable": true
          },
          "output_data_path": {
            "type": "string",
            "title": "Output Data Path",
            "description": "Path to the output file. The output file will be a JSON file of type List[CompletionOutput]."
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels",
            "description": "Labels to attach to the batch inference job.",
            "default": {}
          },
          "data_parallelism": {
            "title": "Data Parallelism",
            "description": "Number of replicas to run the batch inference. More replicas are slower to schedule but faster to inference.",
            "default": 1,
            "type": "integer",
            "maximum": 64.0,
            "minimum": 1.0,
            "nullable": true
          },
          "max_runtime_sec": {
            "title": "Max Runtime Sec",
            "description": "Maximum runtime of the batch inference in seconds. Default to one day.",
            "default": 86400,
            "type": "integer",
            "maximum": 172800.0,
            "minimum": 1.0,
            "nullable": true
          },
          "priority": {
            "title": "Priority",
            "description": "Priority of the batch inference job. Default to None.",
            "type": "string",
            "nullable": true
          },
          "tool_config": {
            "description": "\nConfiguration for tool use.\nNOTE: this config is highly experimental and signature will change significantly in future iterations.",
            "$ref": "#/components/schemas/ToolConfig",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "description": "CPUs to use for the batch inference.",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "description": "Number of GPUs to use for the batch inference.",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "description": "Amount of memory to use for the batch inference.",
            "nullable": true
          },
          "gpu_type": {
            "description": "GPU type to use for the batch inference.",
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "description": "Storage to use for the batch inference.",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "description": "Number of nodes per worker for the batch inference.",
            "type": "integer",
            "nullable": true
          },
          "content": {
            "$ref": "#/components/schemas/CreateBatchCompletionsV1RequestContent",
            "nullable": true
          },
          "model_config": {
            "$ref": "#/components/schemas/CreateBatchCompletionsV1ModelConfig"
          }
        },
        "type": "object",
        "required": [
          "output_data_path",
          "model_config"
        ],
        "title": "CreateBatchCompletionsV1Request",
        "description": "Request object for batch completions."
      },
      "CreateBatchCompletionsV1RequestContent": {
        "properties": {
          "prompts": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Prompts"
          },
          "max_new_tokens": {
            "type": "integer",
            "title": "Max New Tokens"
          },
          "temperature": {
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "title": "Temperature"
          },
          "stop_sequences": {
            "title": "Stop Sequences",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "return_token_log_probs": {
            "title": "Return Token Log Probs",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "type": "number",
            "maximum": 1.0,
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "default": true,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "prompts",
          "max_new_tokens",
          "temperature"
        ],
        "title": "CreateBatchCompletionsV1RequestContent"
      },
      "CreateBatchCompletionsV1Response": {
        "properties": {
          "job_id": {
            "type": "string",
            "title": "Job Id"
          }
        },
        "type": "object",
        "required": [
          "job_id"
        ],
        "title": "CreateBatchCompletionsV1Response"
      },
      "CreateBatchCompletionsV2Request": {
        "properties": {
          "input_data_path": {
            "title": "Input Data Path",
            "description": "Path to the input file. The input file should be a JSON file of type List[CreateBatchCompletionsRequestContent].",
            "type": "string",
            "nullable": true
          },
          "output_data_path": {
            "type": "string",
            "title": "Output Data Path",
            "description": "Path to the output file. The output file will be a JSON file of type List[CompletionOutput]."
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels",
            "description": "Labels to attach to the batch inference job.",
            "default": {}
          },
          "data_parallelism": {
            "title": "Data Parallelism",
            "description": "Number of replicas to run the batch inference. More replicas are slower to schedule but faster to inference.",
            "default": 1,
            "type": "integer",
            "maximum": 64.0,
            "minimum": 1.0,
            "nullable": true
          },
          "max_runtime_sec": {
            "title": "Max Runtime Sec",
            "description": "Maximum runtime of the batch inference in seconds. Default to one day.",
            "default": 86400,
            "type": "integer",
            "maximum": 172800.0,
            "minimum": 1.0,
            "nullable": true
          },
          "priority": {
            "title": "Priority",
            "description": "Priority of the batch inference job. Default to None.",
            "type": "string",
            "nullable": true
          },
          "tool_config": {
            "description": "\nConfiguration for tool use.\nNOTE: this config is highly experimental and signature will change significantly in future iterations.",
            "$ref": "#/components/schemas/ToolConfig",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "description": "CPUs to use for the batch inference.",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "description": "Number of GPUs to use for the batch inference.",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "description": "Amount of memory to use for the batch inference.",
            "nullable": true
          },
          "gpu_type": {
            "description": "GPU type to use for the batch inference.",
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "description": "Storage to use for the batch inference.",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "description": "Number of nodes per worker for the batch inference.",
            "type": "integer",
            "nullable": true
          },
          "content": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/CreateBatchCompletionsV1RequestContent"
              },
              {
                "items": {
                  "$ref": "#/components/schemas/FilteredCompletionV2Request"
                },
                "type": "array"
              },
              {
                "items": {
                  "$ref": "#/components/schemas/FilteredChatCompletionV2Request"
                },
                "type": "array"
              }
            ],
            "title": "Content",
            "description": "\nEither `input_data_path` or `content` needs to be provided.\nWhen input_data_path is provided, the input file should be a JSON file of type List[CreateBatchCompletionsRequestContent].\n",
            "nullable": true
          },
          "model_config": {
            "$ref": "#/components/schemas/BatchCompletionsModelConfig",
            "description": "Model configuration for the batch inference. Hardware configurations are inferred."
          }
        },
        "type": "object",
        "required": [
          "output_data_path",
          "model_config"
        ],
        "title": "CreateBatchCompletionsV2Request",
        "description": "Request object for batch completions."
      },
      "CreateBatchJobResourceRequests": {
        "properties": {
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "max_workers": {
            "title": "Max Workers",
            "type": "integer",
            "nullable": true
          },
          "per_worker": {
            "title": "Per Worker",
            "type": "integer",
            "nullable": true
          },
          "concurrent_requests_per_worker": {
            "title": "Concurrent Requests Per Worker",
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "title": "CreateBatchJobResourceRequests"
      },
      "CreateBatchJobV1Request": {
        "properties": {
          "model_bundle_id": {
            "type": "string",
            "title": "Model Bundle Id"
          },
          "input_path": {
            "type": "string",
            "title": "Input Path"
          },
          "serialization_format": {
            "$ref": "#/components/schemas/BatchJobSerializationFormat"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "resource_requests": {
            "$ref": "#/components/schemas/CreateBatchJobResourceRequests"
          },
          "timeout_seconds": {
            "type": "number",
            "title": "Timeout Seconds",
            "default": 43200.0
          }
        },
        "type": "object",
        "required": [
          "model_bundle_id",
          "input_path",
          "serialization_format",
          "labels",
          "resource_requests"
        ],
        "title": "CreateBatchJobV1Request"
      },
      "CreateBatchJobV1Response": {
        "properties": {
          "job_id": {
            "type": "string",
            "title": "Job Id"
          }
        },
        "type": "object",
        "required": [
          "job_id"
        ],
        "title": "CreateBatchJobV1Response"
      },
      "CreateChatCompletionResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "A unique identifier for the chat completion."
          },
          "choices": {
            "items": {
              "$ref": "#/components/schemas/Choice"
            },
            "type": "array",
            "title": "Choices",
            "description": "A list of chat completion choices. Can be more than one if `n` is greater than 1."
          },
          "created": {
            "type": "integer",
            "title": "Created",
            "description": "The Unix timestamp (in seconds) of when the chat completion was created."
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "The model used for the chat completion."
          },
          "service_tier": {
            "$ref": "#/components/schemas/ServiceTier",
            "nullable": true
          },
          "system_fingerprint": {
            "title": "System Fingerprint",
            "description": "This fingerprint represents the backend configuration that the model runs with.\n\nCan be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.\n",
            "type": "string",
            "nullable": true
          },
          "object": {
            "type": "string",
            "const": "chat.completion",
            "title": "Object",
            "description": "The object type, which is always `chat.completion`."
          },
          "usage": {
            "$ref": "#/components/schemas/CompletionUsage",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "choices",
          "created",
          "model",
          "object"
        ],
        "title": "CreateChatCompletionResponse"
      },
      "CreateChatCompletionStreamResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "A unique identifier for the chat completion. Each chunk has the same ID."
          },
          "choices": {
            "items": {
              "$ref": "#/components/schemas/Choice1"
            },
            "type": "array",
            "title": "Choices",
            "description": "A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the\nlast chunk if you set `stream_options: {\"include_usage\": true}`.\n"
          },
          "created": {
            "type": "integer",
            "title": "Created",
            "description": "The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp."
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "The model to generate the completion."
          },
          "service_tier": {
            "$ref": "#/components/schemas/ServiceTier",
            "nullable": true
          },
          "system_fingerprint": {
            "title": "System Fingerprint",
            "description": "This fingerprint represents the backend configuration that the model runs with.\nCan be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.\n",
            "type": "string",
            "nullable": true
          },
          "object": {
            "type": "string",
            "const": "chat.completion.chunk",
            "title": "Object",
            "description": "The object type, which is always `chat.completion.chunk`."
          },
          "usage": {
            "description": "An optional field that will only be present when you set\n`stream_options: {\"include_usage\": true}` in your request. When present, it\ncontains a null value **except for the last chunk** which contains the\ntoken usage statistics for the entire request.\n\n**NOTE:** If the stream is interrupted or cancelled, you may not\nreceive the final usage chunk which contains the total token usage for\nthe request.\n",
            "$ref": "#/components/schemas/CompletionUsage",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "choices",
          "created",
          "model",
          "object"
        ],
        "title": "CreateChatCompletionStreamResponse"
      },
      "CreateCompletionResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "A unique identifier for the completion."
          },
          "choices": {
            "items": {
              "$ref": "#/components/schemas/Choice2"
            },
            "type": "array",
            "title": "Choices",
            "description": "The list of completion choices the model generated for the input prompt."
          },
          "created": {
            "type": "integer",
            "title": "Created",
            "description": "The Unix timestamp (in seconds) of when the completion was created."
          },
          "model": {
            "type": "string",
            "title": "Model",
            "description": "The model used for completion."
          },
          "system_fingerprint": {
            "title": "System Fingerprint",
            "description": "This fingerprint represents the backend configuration that the model runs with.\n\nCan be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.\n",
            "type": "string",
            "nullable": true
          },
          "object": {
            "type": "string",
            "const": "text_completion",
            "title": "Object",
            "description": "The object type, which is always \"text_completion\""
          },
          "usage": {
            "$ref": "#/components/schemas/CompletionUsage",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "choices",
          "created",
          "model",
          "object"
        ],
        "title": "CreateCompletionResponse"
      },
      "CreateDeepSpeedModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "min_workers": {
            "type": "integer",
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "title": "Per Worker"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "default": "hugging_face"
          },
          "inference_framework_image_tag": {
            "type": "string",
            "title": "Inference Framework Image Tag",
            "default": "latest"
          },
          "num_shards": {
            "type": "integer",
            "title": "Num Shards",
            "default": 1
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType",
            "default": "sync"
          },
          "inference_framework": {
            "type": "string",
            "const": "deepspeed",
            "title": "Inference Framework",
            "default": "deepspeed"
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_name",
          "metadata",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateDeepSpeedModelEndpointRequest"
      },
      "CreateDockerImageBatchJobBundleV1Request": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name"
          },
          "image_repository": {
            "type": "string",
            "title": "Image Repository"
          },
          "image_tag": {
            "type": "string",
            "title": "Image Tag"
          },
          "command": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Command"
          },
          "env": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Env",
            "default": {}
          },
          "mount_location": {
            "title": "Mount Location",
            "type": "string",
            "nullable": true
          },
          "resource_requests": {
            "$ref": "#/components/schemas/CreateDockerImageBatchJobResourceRequests",
            "default": {}
          },
          "public": {
            "title": "Public",
            "default": false,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name",
          "image_repository",
          "image_tag",
          "command"
        ],
        "title": "CreateDockerImageBatchJobBundleV1Request"
      },
      "CreateDockerImageBatchJobBundleV1Response": {
        "properties": {
          "docker_image_batch_job_bundle_id": {
            "type": "string",
            "title": "Docker Image Batch Job Bundle Id"
          }
        },
        "type": "object",
        "required": [
          "docker_image_batch_job_bundle_id"
        ],
        "title": "CreateDockerImageBatchJobBundleV1Response"
      },
      "CreateDockerImageBatchJobResourceRequests": {
        "properties": {
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "title": "CreateDockerImageBatchJobResourceRequests"
      },
      "CreateDockerImageBatchJobV1Request": {
        "properties": {
          "docker_image_batch_job_bundle_name": {
            "title": "Docker Image Batch Job Bundle Name",
            "type": "string",
            "nullable": true
          },
          "docker_image_batch_job_bundle_id": {
            "title": "Docker Image Batch Job Bundle Id",
            "type": "string",
            "nullable": true
          },
          "job_config": {
            "title": "Job Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "resource_requests": {
            "$ref": "#/components/schemas/CreateDockerImageBatchJobResourceRequests",
            "default": {}
          },
          "override_job_max_runtime_s": {
            "title": "Override Job Max Runtime S",
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "labels"
        ],
        "title": "CreateDockerImageBatchJobV1Request"
      },
      "CreateDockerImageBatchJobV1Response": {
        "properties": {
          "job_id": {
            "type": "string",
            "title": "Job Id"
          }
        },
        "type": "object",
        "required": [
          "job_id"
        ],
        "title": "CreateDockerImageBatchJobV1Response"
      },
      "CreateFineTuneRequest": {
        "properties": {
          "model": {
            "type": "string",
            "title": "Model"
          },
          "training_file": {
            "type": "string",
            "title": "Training File"
          },
          "validation_file": {
            "title": "Validation File",
            "type": "string",
            "nullable": true
          },
          "hyperparameters": {
            "additionalProperties": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "integer"
                },
                {
                  "type": "number"
                },
                {
                  "additionalProperties": true,
                  "type": "object"
                }
              ]
            },
            "type": "object",
            "title": "Hyperparameters"
          },
          "suffix": {
            "title": "Suffix",
            "type": "string",
            "nullable": true
          },
          "wandb_config": {
            "title": "Wandb Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "model",
          "training_file",
          "hyperparameters"
        ],
        "title": "CreateFineTuneRequest"
      },
      "CreateFineTuneResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          }
        },
        "type": "object",
        "required": [
          "id"
        ],
        "title": "CreateFineTuneResponse"
      },
      "CreateLLMModelEndpointV1Response": {
        "properties": {
          "endpoint_creation_task_id": {
            "type": "string",
            "title": "Endpoint Creation Task Id"
          }
        },
        "type": "object",
        "required": [
          "endpoint_creation_task_id"
        ],
        "title": "CreateLLMModelEndpointV1Response"
      },
      "CreateLightLLMModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "min_workers": {
            "type": "integer",
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "title": "Per Worker"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "default": "hugging_face"
          },
          "inference_framework_image_tag": {
            "type": "string",
            "title": "Inference Framework Image Tag",
            "default": "latest"
          },
          "num_shards": {
            "type": "integer",
            "title": "Num Shards",
            "default": 1
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType",
            "default": "sync"
          },
          "inference_framework": {
            "type": "string",
            "const": "lightllm",
            "title": "Inference Framework",
            "default": "lightllm"
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_name",
          "metadata",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateLightLLMModelEndpointRequest"
      },
      "CreateModelBundleV1Request": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name"
          },
          "location": {
            "type": "string",
            "title": "Location"
          },
          "requirements": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Requirements"
          },
          "env_params": {
            "$ref": "#/components/schemas/ModelBundleEnvironmentParams"
          },
          "packaging_type": {
            "$ref": "#/components/schemas/ModelBundlePackagingType"
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "app_config": {
            "title": "App Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "schema_location": {
            "title": "Schema Location",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name",
          "location",
          "requirements",
          "env_params",
          "packaging_type"
        ],
        "title": "CreateModelBundleV1Request",
        "description": "Request object for creating a Model Bundle."
      },
      "CreateModelBundleV1Response": {
        "properties": {
          "model_bundle_id": {
            "type": "string",
            "title": "Model Bundle Id"
          }
        },
        "type": "object",
        "required": [
          "model_bundle_id"
        ],
        "title": "CreateModelBundleV1Response",
        "description": "Response object for creating a Model Bundle."
      },
      "CreateModelBundleV2Request": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name"
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "schema_location": {
            "type": "string",
            "title": "Schema Location"
          },
          "flavor": {
            "oneOf": [
              {
                "$ref": "#/components/schemas/CloudpickleArtifactFlavor"
              },
              {
                "$ref": "#/components/schemas/ZipArtifactFlavor"
              },
              {
                "$ref": "#/components/schemas/RunnableImageFlavor"
              },
              {
                "$ref": "#/components/schemas/StreamingEnhancedRunnableImageFlavor"
              },
              {
                "$ref": "#/components/schemas/TritonEnhancedRunnableImageFlavor"
              }
            ],
            "title": "Flavor",
            "discriminator": {
              "propertyName": "flavor",
              "mapping": {
                "cloudpickle_artifact": "#/components/schemas/CloudpickleArtifactFlavor",
                "runnable_image": "#/components/schemas/RunnableImageFlavor",
                "streaming_enhanced_runnable_image": "#/components/schemas/StreamingEnhancedRunnableImageFlavor",
                "triton_enhanced_runnable_image": "#/components/schemas/TritonEnhancedRunnableImageFlavor",
                "zip_artifact": "#/components/schemas/ZipArtifactFlavor"
              }
            }
          }
        },
        "type": "object",
        "required": [
          "name",
          "schema_location",
          "flavor"
        ],
        "title": "CreateModelBundleV2Request",
        "description": "Request object for creating a Model Bundle."
      },
      "CreateModelBundleV2Response": {
        "properties": {
          "model_bundle_id": {
            "type": "string",
            "title": "Model Bundle Id"
          }
        },
        "type": "object",
        "required": [
          "model_bundle_id"
        ],
        "title": "CreateModelBundleV2Response",
        "description": "Response object for creating a Model Bundle."
      },
      "CreateModelEndpointV1Request": {
        "properties": {
          "name": {
            "type": "string",
            "maxLength": 63,
            "title": "Name"
          },
          "model_bundle_id": {
            "type": "string",
            "title": "Model Bundle Id"
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus"
          },
          "gpus": {
            "type": "integer",
            "minimum": 0.0,
            "title": "Gpus"
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory"
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage"
          },
          "nodes_per_worker": {
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "title": "Nodes Per Worker",
            "default": 1
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "min_workers": {
            "type": "integer",
            "minimum": 0.0,
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "minimum": 0.0,
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "title": "Per Worker"
          },
          "concurrent_requests_per_worker": {
            "title": "Concurrent Requests Per Worker",
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": false,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_bundle_id",
          "endpoint_type",
          "metadata",
          "cpus",
          "gpus",
          "memory",
          "storage",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateModelEndpointV1Request"
      },
      "CreateModelEndpointV1Response": {
        "properties": {
          "endpoint_creation_task_id": {
            "type": "string",
            "title": "Endpoint Creation Task Id"
          }
        },
        "type": "object",
        "required": [
          "endpoint_creation_task_id"
        ],
        "title": "CreateModelEndpointV1Response"
      },
      "CreateSGLangModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "min_workers": {
            "type": "integer",
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "title": "Per Worker"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "default": "hugging_face"
          },
          "inference_framework_image_tag": {
            "type": "string",
            "title": "Inference Framework Image Tag",
            "default": "latest"
          },
          "num_shards": {
            "type": "integer",
            "title": "Num Shards",
            "default": 1
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType",
            "default": "sync"
          },
          "trust_remote_code": {
            "title": "Trust Remote Code",
            "description": "Whether to trust remote code from Hugging face hub. This is only applicable to models whose code is not supported natively by the transformers library (e.g. deepseek). Default to False.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "tp_size": {
            "title": "Tp Size",
            "description": "The tensor parallel size.",
            "type": "integer",
            "nullable": true
          },
          "skip_tokenizer_init": {
            "title": "Skip Tokenizer Init",
            "description": "If set, skip init tokenizer and pass input_ids in generate request",
            "type": "boolean",
            "nullable": true
          },
          "load_format": {
            "title": "Load Format",
            "description": "The format of the model weights to load.",
            "type": "string",
            "nullable": true
          },
          "dtype": {
            "title": "Dtype",
            "description": "Data type for model weights and activations.",
            "type": "string",
            "nullable": true
          },
          "kv_cache_dtype": {
            "title": "Kv Cache Dtype",
            "description": "Data type for kv cache storage. \"auto\" will use model data type.",
            "type": "string",
            "nullable": true
          },
          "quantization_param_path": {
            "title": "Quantization Param Path",
            "description": "Path to the JSON file containing the KV cache scaling factors.",
            "type": "string",
            "nullable": true
          },
          "quantization": {
            "title": "Quantization",
            "description": "The quantization method.",
            "type": "string",
            "nullable": true
          },
          "context_length": {
            "title": "Context Length",
            "description": "The model's maximum context length.",
            "type": "integer",
            "nullable": true
          },
          "device": {
            "title": "Device",
            "description": "The device type.",
            "type": "string",
            "nullable": true
          },
          "served_model_name": {
            "title": "Served Model Name",
            "description": "Override the model name returned by the v1/models endpoint in OpenAI API server.",
            "type": "string",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "The builtin chat template name or path of the chat template file.",
            "type": "string",
            "nullable": true
          },
          "is_embedding": {
            "title": "Is Embedding",
            "description": "Whether to use a CausalLM as an embedding model.",
            "type": "boolean",
            "nullable": true
          },
          "revision": {
            "title": "Revision",
            "description": "The specific model version to use.",
            "type": "string",
            "nullable": true
          },
          "mem_fraction_static": {
            "title": "Mem Fraction Static",
            "description": "The fraction of the memory used for static allocation.",
            "type": "number",
            "nullable": true
          },
          "max_running_requests": {
            "title": "Max Running Requests",
            "description": "The maximum number of running requests.",
            "type": "integer",
            "nullable": true
          },
          "max_total_tokens": {
            "title": "Max Total Tokens",
            "description": "The maximum number of tokens in the memory pool.",
            "type": "integer",
            "nullable": true
          },
          "chunked_prefill_size": {
            "title": "Chunked Prefill Size",
            "description": "The maximum number of tokens in a chunk for the chunked prefill.",
            "type": "integer",
            "nullable": true
          },
          "max_prefill_tokens": {
            "title": "Max Prefill Tokens",
            "description": "The maximum number of tokens in a prefill batch.",
            "type": "integer",
            "nullable": true
          },
          "schedule_policy": {
            "title": "Schedule Policy",
            "description": "The scheduling policy of the requests.",
            "type": "string",
            "nullable": true
          },
          "schedule_conservativeness": {
            "title": "Schedule Conservativeness",
            "description": "How conservative the schedule policy is.",
            "type": "number",
            "nullable": true
          },
          "cpu_offload_gb": {
            "title": "Cpu Offload Gb",
            "description": "How many GBs of RAM to reserve for CPU offloading",
            "type": "integer",
            "nullable": true
          },
          "prefill_only_one_req": {
            "title": "Prefill Only One Req",
            "description": "If true, we only prefill one request at one prefill batch",
            "type": "boolean",
            "nullable": true
          },
          "stream_interval": {
            "title": "Stream Interval",
            "description": "The interval for streaming in terms of the token length.",
            "type": "integer",
            "nullable": true
          },
          "random_seed": {
            "title": "Random Seed",
            "description": "The random seed.",
            "type": "integer",
            "nullable": true
          },
          "constrained_json_whitespace_pattern": {
            "title": "Constrained Json Whitespace Pattern",
            "description": "Regex pattern for syntactic whitespaces allowed in JSON constrained output.",
            "type": "string",
            "nullable": true
          },
          "watchdog_timeout": {
            "title": "Watchdog Timeout",
            "description": "Set watchdog timeout in seconds.",
            "type": "number",
            "nullable": true
          },
          "download_dir": {
            "title": "Download Dir",
            "description": "Model download directory.",
            "type": "string",
            "nullable": true
          },
          "base_gpu_id": {
            "title": "Base Gpu Id",
            "description": "The base GPU ID to start allocating GPUs from.",
            "type": "integer",
            "nullable": true
          },
          "log_level": {
            "title": "Log Level",
            "description": "The logging level of all loggers.",
            "type": "string",
            "nullable": true
          },
          "log_level_http": {
            "title": "Log Level Http",
            "description": "The logging level of HTTP server.",
            "type": "string",
            "nullable": true
          },
          "log_requests": {
            "title": "Log Requests",
            "description": "Log the inputs and outputs of all requests.",
            "type": "boolean",
            "nullable": true
          },
          "show_time_cost": {
            "title": "Show Time Cost",
            "description": "Show time cost of custom marks.",
            "type": "boolean",
            "nullable": true
          },
          "enable_metrics": {
            "title": "Enable Metrics",
            "description": "Enable log prometheus metrics.",
            "type": "boolean",
            "nullable": true
          },
          "decode_log_interval": {
            "title": "Decode Log Interval",
            "description": "The log interval of decode batch.",
            "type": "integer",
            "nullable": true
          },
          "api_key": {
            "title": "Api Key",
            "description": "Set API key of the server.",
            "type": "string",
            "nullable": true
          },
          "file_storage_pth": {
            "title": "File Storage Pth",
            "description": "The path of the file storage in backend.",
            "type": "string",
            "nullable": true
          },
          "enable_cache_report": {
            "title": "Enable Cache Report",
            "description": "Return number of cached tokens in usage.prompt_tokens_details.",
            "type": "boolean",
            "nullable": true
          },
          "data_parallel_size": {
            "title": "Data Parallel Size",
            "description": "The data parallelism size.",
            "type": "integer",
            "nullable": true
          },
          "load_balance_method": {
            "title": "Load Balance Method",
            "description": "The load balancing strategy for data parallelism.",
            "type": "string",
            "nullable": true
          },
          "expert_parallel_size": {
            "title": "Expert Parallel Size",
            "description": "The expert parallelism size.",
            "type": "integer",
            "nullable": true
          },
          "dist_init_addr": {
            "title": "Dist Init Addr",
            "description": "The host address for initializing distributed backend.",
            "type": "string",
            "nullable": true
          },
          "nnodes": {
            "title": "Nnodes",
            "description": "The number of nodes.",
            "type": "integer",
            "nullable": true
          },
          "node_rank": {
            "title": "Node Rank",
            "description": "The node rank.",
            "type": "integer",
            "nullable": true
          },
          "json_model_override_args": {
            "title": "Json Model Override Args",
            "description": "A dictionary in JSON string format used to override default model configurations.",
            "type": "string",
            "nullable": true
          },
          "lora_paths": {
            "title": "Lora Paths",
            "description": "The list of LoRA adapters.",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "max_loras_per_batch": {
            "title": "Max Loras Per Batch",
            "description": "Maximum number of adapters for a running batch.",
            "type": "integer",
            "nullable": true
          },
          "attention_backend": {
            "title": "Attention Backend",
            "description": "Choose the kernels for attention layers.",
            "type": "string",
            "nullable": true
          },
          "sampling_backend": {
            "title": "Sampling Backend",
            "description": "Choose the kernels for sampling layers.",
            "type": "string",
            "nullable": true
          },
          "grammar_backend": {
            "title": "Grammar Backend",
            "description": "Choose the backend for grammar-guided decoding.",
            "type": "string",
            "nullable": true
          },
          "speculative_algorithm": {
            "title": "Speculative Algorithm",
            "description": "Speculative algorithm.",
            "type": "string",
            "nullable": true
          },
          "speculative_draft_model_path": {
            "title": "Speculative Draft Model Path",
            "description": "The path of the draft model weights.",
            "type": "string",
            "nullable": true
          },
          "speculative_num_steps": {
            "title": "Speculative Num Steps",
            "description": "The number of steps sampled from draft model in Speculative Decoding.",
            "type": "integer",
            "nullable": true
          },
          "speculative_num_draft_tokens": {
            "title": "Speculative Num Draft Tokens",
            "description": "The number of token sampled from draft model in Speculative Decoding.",
            "type": "integer",
            "nullable": true
          },
          "speculative_eagle_topk": {
            "title": "Speculative Eagle Topk",
            "description": "The number of token sampled from draft model in eagle2 each step.",
            "type": "integer",
            "nullable": true
          },
          "enable_double_sparsity": {
            "title": "Enable Double Sparsity",
            "description": "Enable double sparsity attention",
            "type": "boolean",
            "nullable": true
          },
          "ds_channel_config_path": {
            "title": "Ds Channel Config Path",
            "description": "The path of the double sparsity channel config",
            "type": "string",
            "nullable": true
          },
          "ds_heavy_channel_num": {
            "title": "Ds Heavy Channel Num",
            "description": "The number of heavy channels in double sparsity attention",
            "type": "integer",
            "nullable": true
          },
          "ds_heavy_token_num": {
            "title": "Ds Heavy Token Num",
            "description": "The number of heavy tokens in double sparsity attention",
            "type": "integer",
            "nullable": true
          },
          "ds_heavy_channel_type": {
            "title": "Ds Heavy Channel Type",
            "description": "The type of heavy channels in double sparsity attention",
            "type": "string",
            "nullable": true
          },
          "ds_sparse_decode_threshold": {
            "title": "Ds Sparse Decode Threshold",
            "description": "The threshold for sparse decoding in double sparsity attention",
            "type": "integer",
            "nullable": true
          },
          "disable_radix_cache": {
            "title": "Disable Radix Cache",
            "description": "Disable RadixAttention for prefix caching.",
            "type": "boolean",
            "nullable": true
          },
          "disable_jump_forward": {
            "title": "Disable Jump Forward",
            "description": "Disable jump-forward for grammar-guided decoding.",
            "type": "boolean",
            "nullable": true
          },
          "disable_cuda_graph": {
            "title": "Disable Cuda Graph",
            "description": "Disable cuda graph.",
            "type": "boolean",
            "nullable": true
          },
          "disable_cuda_graph_padding": {
            "title": "Disable Cuda Graph Padding",
            "description": "Disable cuda graph when padding is needed.",
            "type": "boolean",
            "nullable": true
          },
          "disable_outlines_disk_cache": {
            "title": "Disable Outlines Disk Cache",
            "description": "Disable disk cache of outlines.",
            "type": "boolean",
            "nullable": true
          },
          "disable_custom_all_reduce": {
            "title": "Disable Custom All Reduce",
            "description": "Disable the custom all-reduce kernel.",
            "type": "boolean",
            "nullable": true
          },
          "disable_mla": {
            "title": "Disable Mla",
            "description": "Disable Multi-head Latent Attention (MLA) for DeepSeek-V2.",
            "type": "boolean",
            "nullable": true
          },
          "disable_overlap_schedule": {
            "title": "Disable Overlap Schedule",
            "description": "Disable the overlap scheduler.",
            "type": "boolean",
            "nullable": true
          },
          "enable_mixed_chunk": {
            "title": "Enable Mixed Chunk",
            "description": "Enable mixing prefill and decode in a batch when using chunked prefill.",
            "type": "boolean",
            "nullable": true
          },
          "enable_dp_attention": {
            "title": "Enable Dp Attention",
            "description": "Enable data parallelism for attention and tensor parallelism for FFN.",
            "type": "boolean",
            "nullable": true
          },
          "enable_ep_moe": {
            "title": "Enable Ep Moe",
            "description": "Enable expert parallelism for moe.",
            "type": "boolean",
            "nullable": true
          },
          "enable_torch_compile": {
            "title": "Enable Torch Compile",
            "description": "Optimize the model with torch.compile.",
            "type": "boolean",
            "nullable": true
          },
          "torch_compile_max_bs": {
            "title": "Torch Compile Max Bs",
            "description": "Set the maximum batch size when using torch compile.",
            "type": "integer",
            "nullable": true
          },
          "cuda_graph_max_bs": {
            "title": "Cuda Graph Max Bs",
            "description": "Set the maximum batch size for cuda graph.",
            "type": "integer",
            "nullable": true
          },
          "cuda_graph_bs": {
            "title": "Cuda Graph Bs",
            "description": "Set the list of batch sizes for cuda graph.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "torchao_config": {
            "title": "Torchao Config",
            "description": "Optimize the model with torchao.",
            "type": "string",
            "nullable": true
          },
          "enable_nan_detection": {
            "title": "Enable Nan Detection",
            "description": "Enable the NaN detection for debugging purposes.",
            "type": "boolean",
            "nullable": true
          },
          "enable_p2p_check": {
            "title": "Enable P2P Check",
            "description": "Enable P2P check for GPU access.",
            "type": "boolean",
            "nullable": true
          },
          "triton_attention_reduce_in_fp32": {
            "title": "Triton Attention Reduce In Fp32",
            "description": "Cast the intermediate attention results to fp32.",
            "type": "boolean",
            "nullable": true
          },
          "triton_attention_num_kv_splits": {
            "title": "Triton Attention Num Kv Splits",
            "description": "The number of KV splits in flash decoding Triton kernel.",
            "type": "integer",
            "nullable": true
          },
          "num_continuous_decode_steps": {
            "title": "Num Continuous Decode Steps",
            "description": "Run multiple continuous decoding steps to reduce scheduling overhead.",
            "type": "integer",
            "nullable": true
          },
          "delete_ckpt_after_loading": {
            "title": "Delete Ckpt After Loading",
            "description": "Delete the model checkpoint after loading the model.",
            "type": "boolean",
            "nullable": true
          },
          "enable_memory_saver": {
            "title": "Enable Memory Saver",
            "description": "Allow saving memory using release_memory_occupation and resume_memory_occupation",
            "type": "boolean",
            "nullable": true
          },
          "allow_auto_truncate": {
            "title": "Allow Auto Truncate",
            "description": "Allow automatically truncating requests that exceed the maximum input length.",
            "type": "boolean",
            "nullable": true
          },
          "enable_custom_logit_processor": {
            "title": "Enable Custom Logit Processor",
            "description": "Enable users to pass custom logit processors to the server.",
            "type": "boolean",
            "nullable": true
          },
          "tool_call_parser": {
            "title": "Tool Call Parser",
            "description": "Specify the parser for handling tool-call interactions.",
            "type": "string",
            "nullable": true
          },
          "huggingface_repo": {
            "title": "Huggingface Repo",
            "description": "The Hugging Face repository ID.",
            "type": "string",
            "nullable": true
          },
          "inference_framework": {
            "type": "string",
            "const": "sglang",
            "title": "Inference Framework",
            "default": "sglang"
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_name",
          "metadata",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateSGLangModelEndpointRequest"
      },
      "CreateTensorRTLLMModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "min_workers": {
            "type": "integer",
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "title": "Per Worker"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "default": "hugging_face"
          },
          "inference_framework_image_tag": {
            "type": "string",
            "title": "Inference Framework Image Tag",
            "default": "latest"
          },
          "num_shards": {
            "type": "integer",
            "title": "Num Shards",
            "default": 1
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType",
            "default": "sync"
          },
          "inference_framework": {
            "type": "string",
            "const": "tensorrt_llm",
            "title": "Inference Framework",
            "default": "tensorrt_llm"
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_name",
          "metadata",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateTensorRTLLMModelEndpointRequest"
      },
      "CreateTextGenerationInferenceModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "min_workers": {
            "type": "integer",
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "title": "Per Worker"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "default": "hugging_face"
          },
          "inference_framework_image_tag": {
            "type": "string",
            "title": "Inference Framework Image Tag",
            "default": "latest"
          },
          "num_shards": {
            "type": "integer",
            "title": "Num Shards",
            "default": 1
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType",
            "default": "sync"
          },
          "inference_framework": {
            "type": "string",
            "const": "text_generation_inference",
            "title": "Inference Framework",
            "default": "text_generation_inference"
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_name",
          "metadata",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateTextGenerationInferenceModelEndpointRequest"
      },
      "CreateTriggerV1Request": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name"
          },
          "cron_schedule": {
            "type": "string",
            "title": "Cron Schedule"
          },
          "bundle_id": {
            "type": "string",
            "title": "Bundle Id"
          },
          "default_job_config": {
            "title": "Default Job Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_job_metadata": {
            "title": "Default Job Metadata",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name",
          "cron_schedule",
          "bundle_id"
        ],
        "title": "CreateTriggerV1Request"
      },
      "CreateTriggerV1Response": {
        "properties": {
          "trigger_id": {
            "type": "string",
            "title": "Trigger Id"
          }
        },
        "type": "object",
        "required": [
          "trigger_id"
        ],
        "title": "CreateTriggerV1Response"
      },
      "CreateVLLMModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "min_workers": {
            "type": "integer",
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "title": "Per Worker"
          },
          "labels": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Labels"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "default": "hugging_face"
          },
          "inference_framework_image_tag": {
            "type": "string",
            "title": "Inference Framework Image Tag",
            "default": "latest"
          },
          "num_shards": {
            "type": "integer",
            "title": "Num Shards",
            "default": 1
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType",
            "default": "sync"
          },
          "max_gpu_memory_utilization": {
            "title": "Max Gpu Memory Utilization",
            "description": "Maximum GPU memory utilization for the batch inference. Default to 90%. Deprecated in favor of specifying this in VLLMModelConfig",
            "type": "number",
            "nullable": true
          },
          "attention_backend": {
            "title": "Attention Backend",
            "description": "Attention backend to use for vLLM. Default to None.",
            "type": "string",
            "nullable": true
          },
          "max_model_len": {
            "title": "Max Model Len",
            "description": "Model context length, If unspecified, will be automatically derived from the model config",
            "type": "integer",
            "nullable": true
          },
          "max_num_seqs": {
            "title": "Max Num Seqs",
            "description": "Maximum number of sequences per iteration",
            "type": "integer",
            "nullable": true
          },
          "enforce_eager": {
            "title": "Enforce Eager",
            "description": "Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal perforamnce and flexibility",
            "type": "boolean",
            "nullable": true
          },
          "trust_remote_code": {
            "title": "Trust Remote Code",
            "description": "Whether to trust remote code from Hugging face hub. This is only applicable to models whose code is not supported natively by the transformers library (e.g. deepseek). Default to False.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "pipeline_parallel_size": {
            "title": "Pipeline Parallel Size",
            "description": "Number of pipeline stages. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "tensor_parallel_size": {
            "title": "Tensor Parallel Size",
            "description": "Number of tensor parallel replicas. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "quantization": {
            "title": "Quantization",
            "description": "Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.",
            "type": "string",
            "nullable": true
          },
          "disable_log_requests": {
            "title": "Disable Log Requests",
            "description": "Disable logging requests. Default to None.",
            "type": "boolean",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "tool_call_parser": {
            "title": "Tool Call Parser",
            "description": "Tool call parser",
            "type": "string",
            "nullable": true
          },
          "enable_auto_tool_choice": {
            "title": "Enable Auto Tool Choice",
            "description": "Enable auto tool choice",
            "type": "boolean",
            "nullable": true
          },
          "load_format": {
            "title": "Load Format",
            "description": "The format of the model weights to load.\n\n* \"auto\" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\n* \"pt\" will load the weights in the pytorch bin format.\n* \"safetensors\" will load the weights in the safetensors format.\n* \"npcache\" will load the weights in pytorch format and store a numpy cache to speed up the loading.\n* \"dummy\" will initialize the weights with random values, which is mainly for profiling.\n* \"tensorizer\" will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information.\n* \"bitsandbytes\" will load the weights using bitsandbytes quantization.\n",
            "type": "string",
            "nullable": true
          },
          "config_format": {
            "title": "Config Format",
            "description": "The config format which shall be loaded.  Defaults to 'auto' which defaults to 'hf'.",
            "type": "string",
            "nullable": true
          },
          "tokenizer_mode": {
            "title": "Tokenizer Mode",
            "description": "Tokenizer mode. 'auto' will use the fast tokenizer ifavailable, 'slow' will always use the slow tokenizer, and'mistral' will always use the tokenizer from `mistral_common`.",
            "type": "string",
            "nullable": true
          },
          "limit_mm_per_prompt": {
            "title": "Limit Mm Per Prompt",
            "description": "Maximum number of data instances per modality per prompt. Only applicable for multimodal models.",
            "type": "string",
            "nullable": true
          },
          "max_num_batched_tokens": {
            "title": "Max Num Batched Tokens",
            "description": "Maximum number of batched tokens per iteration",
            "type": "integer",
            "nullable": true
          },
          "tokenizer": {
            "title": "Tokenizer",
            "description": "Name or path of the huggingface tokenizer to use.",
            "type": "string",
            "nullable": true
          },
          "dtype": {
            "title": "Dtype",
            "description": "Data type for model weights and activations. The 'auto' option will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.",
            "type": "string",
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "Random seed for reproducibility.",
            "type": "integer",
            "nullable": true
          },
          "revision": {
            "title": "Revision",
            "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "code_revision": {
            "title": "Code Revision",
            "description": "The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "rope_scaling": {
            "title": "Rope Scaling",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. When using this flag, don't update `max_position_embeddings` to the expected new maximum.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "tokenizer_revision": {
            "title": "Tokenizer Revision",
            "description": "The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "quantization_param_path": {
            "title": "Quantization Param Path",
            "description": "Path to JSON file containing scaling factors. Used to load KV cache scaling factors into the model when KV cache type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also be used to load activation and weight scaling factors when the model dtype is FP8_E4M3 on ROCm.",
            "type": "string",
            "nullable": true
          },
          "max_seq_len_to_capture": {
            "title": "Max Seq Len To Capture",
            "description": "Maximum sequence len covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. Additionally for encoder-decoder models, if the sequence length of the encoder input is larger than this, we fall back to the eager mode.",
            "type": "integer",
            "nullable": true
          },
          "disable_sliding_window": {
            "title": "Disable Sliding Window",
            "description": "Whether to disable sliding window. If True, we will disable the sliding window functionality of the model. If the model does not support sliding window, this argument is ignored.",
            "type": "boolean",
            "nullable": true
          },
          "skip_tokenizer_init": {
            "title": "Skip Tokenizer Init",
            "description": "If true, skip initialization of tokenizer and detokenizer.",
            "type": "boolean",
            "nullable": true
          },
          "served_model_name": {
            "title": "Served Model Name",
            "description": "The model name used in metrics tag `model_name`, matches the model name exposed via the APIs. If multiple model names provided, the first name will be used. If not specified, the model name will be the same as `model`.",
            "type": "string",
            "nullable": true
          },
          "override_neuron_config": {
            "title": "Override Neuron Config",
            "description": "Initialize non default neuron config or override default neuron config that are specific to Neuron devices, this argument will be used to configure the neuron config that can not be gathered from the vllm arguments.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "mm_processor_kwargs": {
            "title": "Mm Processor Kwargs",
            "description": "Arguments to be forwarded to the model's processor for multi-modal data, e.g., image processor.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "block_size": {
            "title": "Block Size",
            "description": "Size of a cache block in number of tokens.",
            "type": "integer",
            "nullable": true
          },
          "gpu_memory_utilization": {
            "title": "Gpu Memory Utilization",
            "description": "Fraction of GPU memory to use for the vLLM execution.",
            "type": "number",
            "nullable": true
          },
          "swap_space": {
            "title": "Swap Space",
            "description": "Size of the CPU swap space per GPU (in GiB).",
            "type": "number",
            "nullable": true
          },
          "cache_dtype": {
            "title": "Cache Dtype",
            "description": "Data type for kv cache storage.",
            "type": "string",
            "nullable": true
          },
          "num_gpu_blocks_override": {
            "title": "Num Gpu Blocks Override",
            "description": "Number of GPU blocks to use. This overrides the profiled num_gpu_blocks if specified. Does nothing if None.",
            "type": "integer",
            "nullable": true
          },
          "enable_prefix_caching": {
            "title": "Enable Prefix Caching",
            "description": "Enables automatic prefix caching.",
            "type": "boolean",
            "nullable": true
          },
          "inference_framework": {
            "type": "string",
            "const": "vllm",
            "title": "Inference Framework",
            "default": "vllm"
          }
        },
        "type": "object",
        "required": [
          "name",
          "model_name",
          "metadata",
          "min_workers",
          "max_workers",
          "per_worker",
          "labels"
        ],
        "title": "CreateVLLMModelEndpointRequest"
      },
      "CustomFramework": {
        "properties": {
          "framework_type": {
            "type": "string",
            "const": "custom_base_image",
            "title": "Framework Type"
          },
          "image_repository": {
            "type": "string",
            "title": "Image Repository"
          },
          "image_tag": {
            "type": "string",
            "title": "Image Tag"
          }
        },
        "type": "object",
        "required": [
          "framework_type",
          "image_repository",
          "image_tag"
        ],
        "title": "CustomFramework",
        "description": "This is the entity-layer class for a custom framework specification."
      },
      "DeleteFileResponse": {
        "properties": {
          "deleted": {
            "type": "boolean",
            "title": "Deleted",
            "description": "Whether deletion was successful."
          }
        },
        "type": "object",
        "required": [
          "deleted"
        ],
        "title": "DeleteFileResponse",
        "description": "Response object for deleting a file."
      },
      "DeleteLLMEndpointResponse": {
        "properties": {
          "deleted": {
            "type": "boolean",
            "title": "Deleted"
          }
        },
        "type": "object",
        "required": [
          "deleted"
        ],
        "title": "DeleteLLMEndpointResponse"
      },
      "DeleteModelEndpointV1Response": {
        "properties": {
          "deleted": {
            "type": "boolean",
            "title": "Deleted"
          }
        },
        "type": "object",
        "required": [
          "deleted"
        ],
        "title": "DeleteModelEndpointV1Response"
      },
      "DeleteTriggerV1Response": {
        "properties": {
          "success": {
            "type": "boolean",
            "title": "Success"
          }
        },
        "type": "object",
        "required": [
          "success"
        ],
        "title": "DeleteTriggerV1Response"
      },
      "DockerImageBatchJob": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "created_by": {
            "type": "string",
            "title": "Created By"
          },
          "owner": {
            "type": "string",
            "title": "Owner"
          },
          "created_at": {
            "type": "string",
            "format": "date-time",
            "title": "Created At"
          },
          "completed_at": {
            "title": "Completed At",
            "type": "string",
            "format": "date-time",
            "nullable": true
          },
          "status": {
            "$ref": "#/components/schemas/BatchJobStatus"
          },
          "annotations": {
            "title": "Annotations",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "override_job_max_runtime_s": {
            "title": "Override Job Max Runtime S",
            "type": "integer",
            "nullable": true
          },
          "num_workers": {
            "title": "Num Workers",
            "default": 1,
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "created_by",
          "owner",
          "created_at",
          "status"
        ],
        "title": "DockerImageBatchJob",
        "description": "This is the entity-layer class for a Docker Image Batch Job, i.e. a batch job\ncreated via the \"supply a docker image for a k8s job\" API."
      },
      "DockerImageBatchJobBundleV1Response": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "created_at": {
            "type": "string",
            "format": "date-time",
            "title": "Created At"
          },
          "image_repository": {
            "type": "string",
            "title": "Image Repository"
          },
          "image_tag": {
            "type": "string",
            "title": "Image Tag"
          },
          "command": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Command"
          },
          "env": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Env"
          },
          "mount_location": {
            "title": "Mount Location",
            "type": "string",
            "nullable": true
          },
          "cpus": {
            "title": "Cpus",
            "type": "string",
            "nullable": true
          },
          "memory": {
            "title": "Memory",
            "type": "string",
            "nullable": true
          },
          "storage": {
            "title": "Storage",
            "type": "string",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "gpu_type": {
            "title": "Gpu Type",
            "type": "string",
            "nullable": true
          },
          "public": {
            "title": "Public",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "name",
          "created_at",
          "image_repository",
          "image_tag",
          "command",
          "env"
        ],
        "title": "DockerImageBatchJobBundleV1Response"
      },
      "EndpointPredictV1Request": {
        "properties": {
          "url": {
            "title": "Url",
            "type": "string",
            "nullable": true
          },
          "args": {
            "$ref": "#/components/schemas/RequestSchema",
            "nullable": true
          },
          "cloudpickle": {
            "title": "Cloudpickle",
            "type": "string",
            "nullable": true
          },
          "callback_url": {
            "title": "Callback Url",
            "type": "string",
            "nullable": true
          },
          "callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "return_pickled": {
            "type": "boolean",
            "title": "Return Pickled",
            "default": false
          },
          "destination_path": {
            "title": "Destination Path",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "EndpointPredictV1Request"
      },
      "File": {
        "properties": {
          "filename": {
            "title": "Filename",
            "description": "The name of the file, used when passing the file to the model as a \nstring.\n",
            "type": "string",
            "nullable": true
          },
          "file_data": {
            "title": "File Data",
            "description": "The base64 encoded file data, used when passing the file to the model \nas a string.\n",
            "type": "string",
            "nullable": true
          },
          "file_id": {
            "title": "File Id",
            "description": "The ID of an uploaded file to use as input.\n",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "File"
      },
      "FilteredChatCompletionV2Request": {
        "properties": {
          "best_of": {
            "title": "Best Of",
            "description": "Number of output sequences that are generated from the prompt.\n            From these `best_of` sequences, the top `n` sequences are returned.\n            `best_of` must be greater than or equal to `n`. This is treated as\n            the beam width when `use_beam_search` is True. By default, `best_of`\n            is set to `n`.",
            "type": "integer",
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "description": "Controls the number of top tokens to consider. -1 means consider all tokens.",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "min_p": {
            "title": "Min P",
            "description": "Float that represents the minimum probability for a token to be\n            considered, relative to the probability of the most likely token.\n            Must be in [0, 1]. Set to 0 to disable this.",
            "type": "number",
            "nullable": true
          },
          "use_beam_search": {
            "title": "Use Beam Search",
            "description": "Whether to use beam search for sampling.",
            "type": "boolean",
            "nullable": true
          },
          "length_penalty": {
            "title": "Length Penalty",
            "description": "Float that penalizes sequences based on their length.\n            Used in beam search.",
            "type": "number",
            "nullable": true
          },
          "repetition_penalty": {
            "title": "Repetition Penalty",
            "description": "Float that penalizes new tokens based on whether\n            they appear in the prompt and the generated text so far. Values > 1\n            encourage the model to use new tokens, while values < 1 encourage\n            the model to repeat tokens.",
            "type": "number",
            "nullable": true
          },
          "early_stopping": {
            "title": "Early Stopping",
            "description": "Controls the stopping condition for beam search. It\n            accepts the following values: `True`, where the generation stops as\n            soon as there are `best_of` complete candidates; `False`, where an\n            heuristic is applied and the generation stops when is it very\n            unlikely to find better candidates; `\"never\"`, where the beam search\n            procedure only stops when there cannot be better candidates\n            (canonical beam search algorithm).",
            "type": "boolean",
            "nullable": true
          },
          "stop_token_ids": {
            "title": "Stop Token Ids",
            "description": "List of tokens that stop the generation when they are\n            generated. The returned output will contain the stop tokens unless\n            the stop tokens are special tokens.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "include_stop_str_in_output": {
            "title": "Include Stop Str In Output",
            "description": "Whether to include the stop strings in\n            output text. Defaults to False.",
            "type": "boolean",
            "nullable": true
          },
          "ignore_eos": {
            "title": "Ignore Eos",
            "description": "Whether to ignore the EOS token and continue generating\n            tokens after the EOS token is generated.",
            "type": "boolean",
            "nullable": true
          },
          "min_tokens": {
            "title": "Min Tokens",
            "description": "Minimum number of tokens to generate per output sequence\n            before EOS or stop_token_ids can be generated",
            "type": "integer",
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "description": "Whether to skip special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "spaces_between_special_tokens": {
            "title": "Spaces Between Special Tokens",
            "description": "Whether to add spaces between special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "echo": {
            "title": "Echo",
            "description": "If true, the new message will be prepended with the last message if they belong to the same role.",
            "type": "boolean",
            "nullable": true
          },
          "add_generation_prompt": {
            "title": "Add Generation Prompt",
            "description": "If true, the generation prompt will be added to the chat template. This is a parameter used by chat template in tokenizer config of the model.",
            "type": "boolean",
            "nullable": true
          },
          "continue_final_message": {
            "title": "Continue Final Message",
            "description": "If this is set, the chat will be formatted so that the final message in the chat is open-ended, without any EOS tokens. The model will continue this message rather than starting a new one. This allows you to \"prefill\" part of the model's response for it. Cannot be used at the same time as `add_generation_prompt`.",
            "type": "boolean",
            "nullable": true
          },
          "add_special_tokens": {
            "title": "Add Special Tokens",
            "description": "If true, special tokens (e.g. BOS) will be added to the prompt on top of what is added by the chat template. For most models, the chat template takes care of adding the special tokens so this should be set to false (as is the default).",
            "type": "boolean",
            "nullable": true
          },
          "documents": {
            "title": "Documents",
            "description": "A list of dicts representing documents that will be accessible to the model if it is performing RAG (retrieval-augmented generation). If the template does not support RAG, this argument will have no effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys.",
            "items": {
              "additionalProperties": {
                "type": "string"
              },
              "type": "object"
            },
            "type": "array",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "A Jinja template to use for this conversion. As of transformers v4.44, default chat template is no longer allowed, so you must provide a chat template if the model's tokenizer does not define one and no override template is given",
            "type": "string",
            "nullable": true
          },
          "chat_template_kwargs": {
            "title": "Chat Template Kwargs",
            "description": "Additional kwargs to pass to the template renderer. Will be accessible by the chat template.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_json": {
            "title": "Guided Json",
            "description": "JSON schema for guided decoding. Only supported in vllm.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_regex": {
            "title": "Guided Regex",
            "description": "Regex for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_choice": {
            "title": "Guided Choice",
            "description": "Choices for guided decoding. Only supported in vllm.",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "guided_grammar": {
            "title": "Guided Grammar",
            "description": "Context-free grammar for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_decoding_backend": {
            "title": "Guided Decoding Backend",
            "description": "If specified, will override the default guided decoding backend of the server for this specific request. If set, must be either 'outlines' / 'lm-format-enforcer'",
            "type": "string",
            "nullable": true
          },
          "guided_whitespace_pattern": {
            "title": "Guided Whitespace Pattern",
            "description": "If specified, will override the default whitespace pattern for guided json decoding.",
            "type": "string",
            "nullable": true
          },
          "priority": {
            "title": "Priority",
            "description": "The priority of the request (lower means earlier handling; default: 0). Any priority other than 0 will raise an error if the served model does not use priority scheduling.",
            "type": "integer",
            "nullable": true
          },
          "metadata": {
            "$ref": "#/components/schemas/Metadata",
            "nullable": true
          },
          "temperature": {
            "title": "Temperature",
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or `top_p` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "description": "An alternative to sampling with temperature, called nucleus sampling,\nwhere the model considers the results of the tokens with top_p probability\nmass. So 0.1 means only the tokens comprising the top 10% probability mass\nare considered.\n\nWe generally recommend altering this or `temperature` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "nullable": true
          },
          "user": {
            "title": "User",
            "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).\n",
            "examples": [
              "user-1234"
            ],
            "type": "string",
            "nullable": true
          },
          "service_tier": {
            "$ref": "#/components/schemas/ServiceTier",
            "nullable": true
          },
          "messages": {
            "items": {
              "$ref": "#/components/schemas/ChatCompletionRequestMessage"
            },
            "type": "array",
            "minItems": 1,
            "title": "Messages",
            "description": "A list of messages comprising the conversation so far. Depending on the\n[model](/docs/models) you use, different message types (modalities) are\nsupported, like [text](/docs/guides/text-generation),\n[images](/docs/guides/vision), and [audio](/docs/guides/audio).\n"
          },
          "model": {
            "title": "Model",
            "type": "string",
            "nullable": true
          },
          "modalities": {
            "$ref": "#/components/schemas/ResponseModalities",
            "nullable": true
          },
          "reasoning_effort": {
            "$ref": "#/components/schemas/ReasoningEffort",
            "nullable": true
          },
          "max_completion_tokens": {
            "title": "Max Completion Tokens",
            "description": "An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and [reasoning tokens](/docs/guides/reasoning).\n",
            "type": "integer",
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on\ntheir existing frequency in the text so far, decreasing the model's\nlikelihood to repeat the same line verbatim.\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on\nwhether they appear in the text so far, increasing the model's likelihood\nto talk about new topics.\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "web_search_options": {
            "title": "Web search",
            "description": "This tool searches the web for relevant results to use in a response.\nLearn more about the [web search tool](/docs/guides/tools-web-search?api-mode=chat).\n",
            "$ref": "#/components/schemas/WebSearchOptions",
            "nullable": true
          },
          "top_logprobs": {
            "title": "Top Logprobs",
            "description": "An integer between 0 and 20 specifying the number of most likely tokens to\nreturn at each token position, each with an associated log probability.\n`logprobs` must be set to `true` if this parameter is used.\n",
            "type": "integer",
            "maximum": 20.0,
            "minimum": 0.0,
            "nullable": true
          },
          "response_format": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/ResponseFormatText"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonSchema"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonObject"
              }
            ],
            "title": "Response Format",
            "description": "An object specifying the format that the model must output.\n\nSetting to `{ \"type\": \"json_schema\", \"json_schema\": {...} }` enables\nStructured Outputs which ensures the model will match your supplied JSON\nschema. Learn more in the [Structured Outputs\nguide](/docs/guides/structured-outputs).\n\nSetting to `{ \"type\": \"json_object\" }` enables the older JSON mode, which\nensures the message the model generates is valid JSON. Using `json_schema`\nis preferred for models that support it.\n",
            "nullable": true
          },
          "audio": {
            "description": "Parameters for audio output. Required when audio output is requested with\n`modalities: [\"audio\"]`. [Learn more](/docs/guides/audio).\n",
            "$ref": "#/components/schemas/Audio2",
            "nullable": true
          },
          "store": {
            "title": "Store",
            "description": "Whether or not to store the output of this chat completion request for \nuse in our [model distillation](/docs/guides/distillation) or\n[evals](/docs/guides/evals) products.\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "stream": {
            "title": "Stream",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "stop": {
            "$ref": "#/components/schemas/StopConfiguration",
            "nullable": true
          },
          "logit_bias": {
            "title": "Logit Bias",
            "description": "Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the\ntokenizer) to an associated bias value from -100 to 100. Mathematically,\nthe bias is added to the logits generated by the model prior to sampling.\nThe exact effect will vary per model, but values between -1 and 1 should\ndecrease or increase likelihood of selection; values like -100 or 100\nshould result in a ban or exclusive selection of the relevant token.\n",
            "additionalProperties": {
              "type": "integer"
            },
            "type": "object",
            "nullable": true
          },
          "logprobs": {
            "title": "Logprobs",
            "description": "Whether to return log probabilities of the output tokens or not. If true,\nreturns the log probabilities of each output token returned in the\n`content` of `message`.\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "max_tokens": {
            "title": "Max Tokens",
            "description": "The maximum number of [tokens](/tokenizer) that can be generated in the\nchat completion. This value can be used to control\n[costs](https://openai.com/api/pricing/) for text generated via API.\n\nThis value is now deprecated in favor of `max_completion_tokens`, and is\nnot compatible with [o-series models](/docs/guides/reasoning).\n",
            "type": "integer",
            "nullable": true
          },
          "n": {
            "title": "N",
            "description": "How many chat completion choices to generate for each input message. Note that you will be charged based on the number of generated tokens across all of the choices. Keep `n` as `1` to minimize costs.",
            "default": 1,
            "examples": [
              1
            ],
            "type": "integer",
            "maximum": 128.0,
            "minimum": 1.0,
            "nullable": true
          },
          "prediction": {
            "description": "Configuration for a [Predicted Output](/docs/guides/predicted-outputs),\nwhich can greatly improve response times when large parts of the model\nresponse are known ahead of time. This is most common when you are\nregenerating a file with only minor changes to most of the content.\n",
            "$ref": "#/components/schemas/PredictionContent",
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "This feature is in Beta.\nIf specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n",
            "type": "integer",
            "maximum": 9.223372036854776e+18,
            "minimum": -9.223372036854776e+18,
            "nullable": true
          },
          "stream_options": {
            "$ref": "#/components/schemas/ChatCompletionStreamOptions",
            "nullable": true
          },
          "tools": {
            "title": "Tools",
            "description": "A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.\n",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionTool"
            },
            "type": "array",
            "nullable": true
          },
          "tool_choice": {
            "$ref": "#/components/schemas/ChatCompletionToolChoiceOption",
            "nullable": true
          },
          "parallel_tool_calls": {
            "$ref": "#/components/schemas/ParallelToolCalls",
            "nullable": true
          },
          "function_call": {
            "anyOf": [
              {
                "type": "string",
                "enum": [
                  "none",
                  "auto"
                ]
              },
              {
                "$ref": "#/components/schemas/ChatCompletionFunctionCallOption"
              }
            ],
            "title": "Function Call",
            "description": "Deprecated in favor of `tool_choice`.\n\nControls which (if any) function is called by the model.\n\n`none` means the model will not call a function and instead generates a\nmessage.\n\n`auto` means the model can pick between generating a message or calling a\nfunction.\n\nSpecifying a particular function via `{\"name\": \"my_function\"}` forces the\nmodel to call that function.\n\n`none` is the default when no functions are present. `auto` is the default\nif functions are present.\n",
            "nullable": true
          },
          "functions": {
            "title": "Functions",
            "description": "Deprecated in favor of `tools`.\n\nA list of functions the model may generate JSON inputs for.\n",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionFunctions"
            },
            "type": "array",
            "maxItems": 128,
            "minItems": 1,
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "messages"
        ],
        "title": "FilteredChatCompletionV2Request"
      },
      "FilteredCompletionV2Request": {
        "properties": {
          "best_of": {
            "title": "Best Of",
            "description": "Generates `best_of` completions server-side and returns the \"best\" (the one with the highest log probability per token). Results cannot be streamed.\n\nWhen used with `n`, `best_of` controls the number of candidate completions and `n` specifies how many to return \u2013 `best_of` must be greater than `n`.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n",
            "default": 1,
            "type": "integer",
            "maximum": 20.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_k": {
            "title": "Top K",
            "description": "Controls the number of top tokens to consider. -1 means consider all tokens.",
            "type": "integer",
            "minimum": -1.0,
            "nullable": true
          },
          "min_p": {
            "title": "Min P",
            "description": "Float that represents the minimum probability for a token to be\n            considered, relative to the probability of the most likely token.\n            Must be in [0, 1]. Set to 0 to disable this.",
            "type": "number",
            "nullable": true
          },
          "use_beam_search": {
            "title": "Use Beam Search",
            "description": "Whether to use beam search for sampling.",
            "type": "boolean",
            "nullable": true
          },
          "length_penalty": {
            "title": "Length Penalty",
            "description": "Float that penalizes sequences based on their length.\n            Used in beam search.",
            "type": "number",
            "nullable": true
          },
          "repetition_penalty": {
            "title": "Repetition Penalty",
            "description": "Float that penalizes new tokens based on whether\n            they appear in the prompt and the generated text so far. Values > 1\n            encourage the model to use new tokens, while values < 1 encourage\n            the model to repeat tokens.",
            "type": "number",
            "nullable": true
          },
          "early_stopping": {
            "title": "Early Stopping",
            "description": "Controls the stopping condition for beam search. It\n            accepts the following values: `True`, where the generation stops as\n            soon as there are `best_of` complete candidates; `False`, where an\n            heuristic is applied and the generation stops when is it very\n            unlikely to find better candidates; `\"never\"`, where the beam search\n            procedure only stops when there cannot be better candidates\n            (canonical beam search algorithm).",
            "type": "boolean",
            "nullable": true
          },
          "stop_token_ids": {
            "title": "Stop Token Ids",
            "description": "List of tokens that stop the generation when they are\n            generated. The returned output will contain the stop tokens unless\n            the stop tokens are special tokens.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "include_stop_str_in_output": {
            "title": "Include Stop Str In Output",
            "description": "Whether to include the stop strings in output text.",
            "type": "boolean",
            "nullable": true
          },
          "ignore_eos": {
            "title": "Ignore Eos",
            "description": "Whether to ignore the EOS token and continue generating\n            tokens after the EOS token is generated.",
            "type": "boolean",
            "nullable": true
          },
          "min_tokens": {
            "title": "Min Tokens",
            "description": "Minimum number of tokens to generate per output sequence\n            before EOS or stop_token_ids can be generated",
            "type": "integer",
            "nullable": true
          },
          "skip_special_tokens": {
            "title": "Skip Special Tokens",
            "description": "Whether to skip special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "spaces_between_special_tokens": {
            "title": "Spaces Between Special Tokens",
            "description": "Whether to add spaces between special tokens in the output. Only supported in vllm.",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "add_special_tokens": {
            "title": "Add Special Tokens",
            "description": "If true (the default), special tokens (e.g. BOS) will be added to the prompt.",
            "type": "boolean",
            "nullable": true
          },
          "response_format": {
            "anyOf": [
              {
                "$ref": "#/components/schemas/ResponseFormatText"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonSchema"
              },
              {
                "$ref": "#/components/schemas/ResponseFormatJsonObject"
              }
            ],
            "title": "Response Format",
            "description": "Similar to chat completion, this parameter specifies the format of output. Only {'type': 'json_object'} or {'type': 'text' } is supported.",
            "nullable": true
          },
          "guided_json": {
            "title": "Guided Json",
            "description": "JSON schema for guided decoding. Only supported in vllm.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "guided_regex": {
            "title": "Guided Regex",
            "description": "Regex for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_choice": {
            "title": "Guided Choice",
            "description": "Choices for guided decoding. Only supported in vllm.",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "guided_grammar": {
            "title": "Guided Grammar",
            "description": "Context-free grammar for guided decoding. Only supported in vllm.",
            "type": "string",
            "nullable": true
          },
          "guided_decoding_backend": {
            "title": "Guided Decoding Backend",
            "description": "If specified, will override the default guided decoding backend of the server for this specific request. If set, must be either 'outlines' / 'lm-format-enforcer'",
            "type": "string",
            "nullable": true
          },
          "guided_whitespace_pattern": {
            "title": "Guided Whitespace Pattern",
            "description": "If specified, will override the default whitespace pattern for guided json decoding.",
            "type": "string",
            "nullable": true
          },
          "model": {
            "title": "Model",
            "type": "string",
            "nullable": true
          },
          "prompt": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "items": {
                  "type": "string"
                },
                "type": "array"
              },
              {
                "$ref": "#/components/schemas/Prompt"
              },
              {
                "$ref": "#/components/schemas/Prompt1"
              }
            ],
            "title": "Prompt",
            "description": "The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n",
            "nullable": true
          },
          "echo": {
            "title": "Echo",
            "description": "Echo back the prompt in addition to the completion\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "frequency_penalty": {
            "title": "Frequency Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation)\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "logit_bias": {
            "title": "Logit Bias",
            "description": "Modify the likelihood of specified tokens appearing in the completion.\n\nAccepts a JSON object that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100. You can use this [tokenizer tool](/tokenizer?view=bpe) to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n\nAs an example, you can pass `{\"50256\": -100}` to prevent the <|endoftext|> token from being generated.\n",
            "additionalProperties": {
              "type": "integer"
            },
            "type": "object",
            "nullable": true
          },
          "logprobs": {
            "title": "Logprobs",
            "description": "Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the `logprob` of the sampled token, so there may be up to `logprobs+1` elements in the response.\n\nThe maximum value for `logprobs` is 5.\n",
            "type": "integer",
            "maximum": 5.0,
            "minimum": 0.0,
            "nullable": true
          },
          "max_tokens": {
            "title": "Max Tokens",
            "description": "The maximum number of [tokens](/tokenizer) that can be generated in the completion.\n\nThe token count of your prompt plus `max_tokens` cannot exceed the model's context length. [Example Python code](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken) for counting tokens.\n",
            "default": 16,
            "examples": [
              16
            ],
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          },
          "n": {
            "title": "N",
            "description": "How many completions to generate for each prompt.\n\n**Note:** Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for `max_tokens` and `stop`.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "integer",
            "maximum": 128.0,
            "minimum": 1.0,
            "nullable": true
          },
          "presence_penalty": {
            "title": "Presence Penalty",
            "description": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.\n\n[See more information about frequency and presence penalties.](/docs/guides/text-generation)\n",
            "default": 0,
            "type": "number",
            "maximum": 2.0,
            "minimum": -2.0,
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result.\n\nDeterminism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.\n",
            "type": "integer",
            "nullable": true
          },
          "stop": {
            "$ref": "#/components/schemas/StopConfiguration",
            "nullable": true
          },
          "stream": {
            "title": "Stream",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "stream_options": {
            "$ref": "#/components/schemas/ChatCompletionStreamOptions",
            "nullable": true
          },
          "suffix": {
            "title": "Suffix",
            "description": "The suffix that comes after a completion of inserted text.\n\nThis parameter is only supported for `gpt-3.5-turbo-instruct`.\n",
            "examples": [
              "test."
            ],
            "type": "string",
            "nullable": true
          },
          "temperature": {
            "title": "Temperature",
            "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\n\nWe generally recommend altering this or `top_p` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 2.0,
            "minimum": 0.0,
            "nullable": true
          },
          "top_p": {
            "title": "Top P",
            "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nWe generally recommend altering this or `temperature` but not both.\n",
            "default": 1,
            "examples": [
              1
            ],
            "type": "number",
            "maximum": 1.0,
            "minimum": 0.0,
            "nullable": true
          },
          "user": {
            "title": "User",
            "description": "A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices#end-user-ids).\n",
            "examples": [
              "user-1234"
            ],
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "prompt"
        ],
        "title": "FilteredCompletionV2Request"
      },
      "Function1": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to call."
          },
          "arguments": {
            "type": "string",
            "title": "Arguments",
            "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."
          }
        },
        "type": "object",
        "required": [
          "name",
          "arguments"
        ],
        "title": "Function1"
      },
      "Function2": {
        "properties": {
          "name": {
            "title": "Name",
            "description": "The name of the function to call.",
            "type": "string",
            "nullable": true
          },
          "arguments": {
            "title": "Arguments",
            "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "Function2"
      },
      "Function3": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to call."
          }
        },
        "type": "object",
        "required": [
          "name"
        ],
        "title": "Function3"
      },
      "FunctionCall": {
        "properties": {
          "arguments": {
            "type": "string",
            "title": "Arguments",
            "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function."
          },
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to call."
          }
        },
        "type": "object",
        "required": [
          "arguments",
          "name"
        ],
        "title": "FunctionCall"
      },
      "FunctionCall2": {
        "properties": {
          "arguments": {
            "title": "Arguments",
            "description": "The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.",
            "type": "string",
            "nullable": true
          },
          "name": {
            "title": "Name",
            "description": "The name of the function to call.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "FunctionCall2"
      },
      "FunctionObject": {
        "properties": {
          "description": {
            "title": "Description",
            "description": "A description of what the function does, used by the model to choose when and how to call the function.",
            "type": "string",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64."
          },
          "parameters": {
            "$ref": "#/components/schemas/FunctionParameters",
            "nullable": true
          },
          "strict": {
            "title": "Strict",
            "description": "Whether to enable strict schema adherence when generating the function call. If set to true, the model will follow the exact schema defined in the `parameters` field. Only a subset of JSON Schema is supported when `strict` is `true`. Learn more about Structured Outputs in the [function calling guide](docs/guides/function-calling).",
            "default": false,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name"
        ],
        "title": "FunctionObject"
      },
      "FunctionParameters": {
        "properties": {},
        "additionalProperties": true,
        "type": "object",
        "title": "FunctionParameters"
      },
      "GetAsyncTaskV1Response": {
        "properties": {
          "task_id": {
            "type": "string",
            "title": "Task Id"
          },
          "status": {
            "$ref": "#/components/schemas/TaskStatus"
          },
          "result": {
            "$ref": "#/components/schemas/ResponseSchema",
            "nullable": true
          },
          "traceback": {
            "title": "Traceback",
            "type": "string",
            "nullable": true
          },
          "status_code": {
            "title": "Status Code",
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "task_id",
          "status"
        ],
        "title": "GetAsyncTaskV1Response"
      },
      "GetBatchCompletionV2Response": {
        "properties": {
          "job": {
            "$ref": "#/components/schemas/BatchCompletionsJob"
          }
        },
        "type": "object",
        "required": [
          "job"
        ],
        "title": "GetBatchCompletionV2Response"
      },
      "GetBatchJobV1Response": {
        "properties": {
          "status": {
            "$ref": "#/components/schemas/BatchJobStatus"
          },
          "result": {
            "title": "Result",
            "type": "string",
            "nullable": true
          },
          "duration": {
            "type": "string",
            "format": "duration",
            "title": "Duration"
          },
          "num_tasks_pending": {
            "title": "Num Tasks Pending",
            "type": "integer",
            "nullable": true
          },
          "num_tasks_completed": {
            "title": "Num Tasks Completed",
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "status",
          "duration"
        ],
        "title": "GetBatchJobV1Response"
      },
      "GetDockerImageBatchJobV1Response": {
        "properties": {
          "status": {
            "$ref": "#/components/schemas/BatchJobStatus"
          }
        },
        "type": "object",
        "required": [
          "status"
        ],
        "title": "GetDockerImageBatchJobV1Response"
      },
      "GetFileContentResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "ID of the requested file."
          },
          "content": {
            "type": "string",
            "title": "Content",
            "description": "File content."
          }
        },
        "type": "object",
        "required": [
          "id",
          "content"
        ],
        "title": "GetFileContentResponse",
        "description": "Response object for retrieving a file's content."
      },
      "GetFileResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "ID of the requested file."
          },
          "filename": {
            "type": "string",
            "title": "Filename",
            "description": "File name."
          },
          "size": {
            "type": "integer",
            "title": "Size",
            "description": "Length of the file, in characters."
          }
        },
        "type": "object",
        "required": [
          "id",
          "filename",
          "size"
        ],
        "title": "GetFileResponse",
        "description": "Response object for retrieving a file."
      },
      "GetFineTuneEventsResponse": {
        "properties": {
          "events": {
            "items": {
              "$ref": "#/components/schemas/LLMFineTuneEvent"
            },
            "type": "array",
            "title": "Events"
          }
        },
        "type": "object",
        "required": [
          "events"
        ],
        "title": "GetFineTuneEventsResponse"
      },
      "GetFineTuneResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "Unique ID of the fine tune"
          },
          "fine_tuned_model": {
            "title": "Fine Tuned Model",
            "description": "Name of the resulting fine-tuned model. This can be plugged into the Completion API ones the fine-tune is complete",
            "type": "string",
            "nullable": true
          },
          "status": {
            "$ref": "#/components/schemas/BatchJobStatus",
            "description": "Status of the requested fine tune."
          }
        },
        "type": "object",
        "required": [
          "id",
          "status"
        ],
        "title": "GetFineTuneResponse"
      },
      "GetLLMModelEndpointV1Response": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "model_name": {
            "type": "string",
            "title": "Model Name"
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource"
          },
          "status": {
            "$ref": "#/components/schemas/ModelEndpointStatus"
          },
          "inference_framework": {
            "$ref": "#/components/schemas/LLMInferenceFramework"
          },
          "inference_framework_image_tag": {
            "title": "Inference Framework Image Tag",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "type": "integer",
            "nullable": true
          },
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "spec": {
            "$ref": "#/components/schemas/GetModelEndpointV1Response",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "name",
          "model_name",
          "source",
          "status",
          "inference_framework"
        ],
        "title": "GetLLMModelEndpointV1Response"
      },
      "GetModelEndpointV1Response": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "endpoint_type": {
            "$ref": "#/components/schemas/ModelEndpointType"
          },
          "destination": {
            "type": "string",
            "title": "Destination"
          },
          "deployment_name": {
            "title": "Deployment Name",
            "type": "string",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "bundle_name": {
            "type": "string",
            "title": "Bundle Name"
          },
          "status": {
            "$ref": "#/components/schemas/ModelEndpointStatus"
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "labels": {
            "title": "Labels",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "aws_role": {
            "title": "Aws Role",
            "type": "string",
            "nullable": true
          },
          "results_s3_bucket": {
            "title": "Results S3 Bucket",
            "type": "string",
            "nullable": true
          },
          "created_by": {
            "type": "string",
            "title": "Created By"
          },
          "created_at": {
            "type": "string",
            "format": "date-time",
            "title": "Created At"
          },
          "last_updated_at": {
            "type": "string",
            "format": "date-time",
            "title": "Last Updated At"
          },
          "deployment_state": {
            "$ref": "#/components/schemas/ModelEndpointDeploymentState",
            "nullable": true
          },
          "resource_state": {
            "$ref": "#/components/schemas/ModelEndpointResourceState",
            "nullable": true
          },
          "num_queued_items": {
            "title": "Num Queued Items",
            "type": "integer",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "name",
          "endpoint_type",
          "destination",
          "bundle_name",
          "status",
          "created_by",
          "created_at",
          "last_updated_at"
        ],
        "title": "GetModelEndpointV1Response"
      },
      "GetTriggerV1Response": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "owner": {
            "type": "string",
            "title": "Owner"
          },
          "created_by": {
            "type": "string",
            "title": "Created By"
          },
          "created_at": {
            "type": "string",
            "format": "date-time",
            "title": "Created At"
          },
          "cron_schedule": {
            "type": "string",
            "title": "Cron Schedule"
          },
          "docker_image_batch_job_bundle_id": {
            "type": "string",
            "title": "Docker Image Batch Job Bundle Id"
          },
          "default_job_config": {
            "title": "Default Job Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_job_metadata": {
            "title": "Default Job Metadata",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "name",
          "owner",
          "created_by",
          "created_at",
          "cron_schedule",
          "docker_image_batch_job_bundle_id"
        ],
        "title": "GetTriggerV1Response"
      },
      "GpuType": {
        "type": "string",
        "enum": [
          "nvidia-tesla-t4",
          "nvidia-ampere-a10",
          "nvidia-ampere-a100",
          "nvidia-ampere-a100e",
          "nvidia-hopper-h100",
          "nvidia-hopper-h100-1g20gb",
          "nvidia-hopper-h100-3g40gb"
        ],
        "title": "GpuType",
        "description": "Lists allowed GPU types for Launch."
      },
      "HTTPValidationError": {
        "properties": {
          "detail": {
            "items": {
              "$ref": "#/components/schemas/ValidationError"
            },
            "type": "array",
            "title": "Detail"
          }
        },
        "type": "object",
        "title": "HTTPValidationError"
      },
      "ImageUrl": {
        "properties": {
          "url": {
            "type": "string",
            "maxLength": 65536,
            "minLength": 1,
            "format": "uri",
            "title": "Url",
            "description": "Either a URL of the image or the base64 encoded image data.",
            "examples": [
              "http://www.example.com/"
            ]
          },
          "detail": {
            "type": "string",
            "enum": [
              "auto",
              "low",
              "high"
            ],
            "title": "Detail",
            "description": "Specifies the detail level of the image. Learn more in the [Vision guide](/docs/guides/vision#low-or-high-fidelity-image-understanding).",
            "default": "auto"
          }
        },
        "type": "object",
        "required": [
          "url"
        ],
        "title": "ImageUrl"
      },
      "InputAudio": {
        "properties": {
          "data": {
            "type": "string",
            "title": "Data",
            "description": "Base64 encoded audio data."
          },
          "format": {
            "type": "string",
            "enum": [
              "wav",
              "mp3"
            ],
            "title": "Format",
            "description": "The format of the encoded audio data. Currently supports \"wav\" and \"mp3\".\n"
          }
        },
        "type": "object",
        "required": [
          "data",
          "format"
        ],
        "title": "InputAudio"
      },
      "JsonSchema": {
        "properties": {
          "description": {
            "title": "Description",
            "description": "A description of what the response format is for, used by the model to\ndetermine how to respond in the format.\n",
            "type": "string",
            "nullable": true
          },
          "name": {
            "type": "string",
            "title": "Name",
            "description": "The name of the response format. Must be a-z, A-Z, 0-9, or contain\nunderscores and dashes, with a maximum length of 64.\n"
          },
          "schema": {
            "$ref": "#/components/schemas/ResponseFormatJsonSchemaSchema",
            "nullable": true
          },
          "strict": {
            "title": "Strict",
            "description": "Whether to enable strict schema adherence when generating the output.\nIf set to true, the model will always follow the exact schema defined\nin the `schema` field. Only a subset of JSON Schema is supported when\n`strict` is `true`. To learn more, read the [Structured Outputs\nguide](/docs/guides/structured-outputs).\n",
            "default": false,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name"
        ],
        "title": "JsonSchema"
      },
      "LLMFineTuneEvent": {
        "properties": {
          "timestamp": {
            "title": "Timestamp",
            "type": "number",
            "nullable": true
          },
          "message": {
            "type": "string",
            "title": "Message"
          },
          "level": {
            "type": "string",
            "title": "Level"
          }
        },
        "type": "object",
        "required": [
          "message",
          "level"
        ],
        "title": "LLMFineTuneEvent"
      },
      "LLMInferenceFramework": {
        "type": "string",
        "enum": [
          "deepspeed",
          "text_generation_inference",
          "vllm",
          "lightllm",
          "tensorrt_llm",
          "sglang"
        ],
        "title": "LLMInferenceFramework"
      },
      "LLMSource": {
        "type": "string",
        "enum": [
          "hugging_face"
        ],
        "title": "LLMSource"
      },
      "ListDockerImageBatchJobBundleV1Response": {
        "properties": {
          "docker_image_batch_job_bundles": {
            "items": {
              "$ref": "#/components/schemas/DockerImageBatchJobBundleV1Response"
            },
            "type": "array",
            "title": "Docker Image Batch Job Bundles"
          }
        },
        "type": "object",
        "required": [
          "docker_image_batch_job_bundles"
        ],
        "title": "ListDockerImageBatchJobBundleV1Response"
      },
      "ListDockerImageBatchJobsV1Response": {
        "properties": {
          "jobs": {
            "items": {
              "$ref": "#/components/schemas/DockerImageBatchJob"
            },
            "type": "array",
            "title": "Jobs"
          }
        },
        "type": "object",
        "required": [
          "jobs"
        ],
        "title": "ListDockerImageBatchJobsV1Response"
      },
      "ListFilesResponse": {
        "properties": {
          "files": {
            "items": {
              "$ref": "#/components/schemas/GetFileResponse"
            },
            "type": "array",
            "title": "Files",
            "description": "List of file IDs, names, and sizes."
          }
        },
        "type": "object",
        "required": [
          "files"
        ],
        "title": "ListFilesResponse",
        "description": "Response object for listing files."
      },
      "ListFineTunesResponse": {
        "properties": {
          "jobs": {
            "items": {
              "$ref": "#/components/schemas/GetFineTuneResponse"
            },
            "type": "array",
            "title": "Jobs"
          }
        },
        "type": "object",
        "required": [
          "jobs"
        ],
        "title": "ListFineTunesResponse"
      },
      "ListLLMModelEndpointsV1Response": {
        "properties": {
          "model_endpoints": {
            "items": {
              "$ref": "#/components/schemas/GetLLMModelEndpointV1Response"
            },
            "type": "array",
            "title": "Model Endpoints"
          }
        },
        "type": "object",
        "required": [
          "model_endpoints"
        ],
        "title": "ListLLMModelEndpointsV1Response"
      },
      "ListModelBundlesV1Response": {
        "properties": {
          "model_bundles": {
            "items": {
              "$ref": "#/components/schemas/ModelBundleV1Response"
            },
            "type": "array",
            "title": "Model Bundles"
          }
        },
        "type": "object",
        "required": [
          "model_bundles"
        ],
        "title": "ListModelBundlesV1Response",
        "description": "Response object for listing Model Bundles."
      },
      "ListModelBundlesV2Response": {
        "properties": {
          "model_bundles": {
            "items": {
              "$ref": "#/components/schemas/ModelBundleV2Response"
            },
            "type": "array",
            "title": "Model Bundles"
          }
        },
        "type": "object",
        "required": [
          "model_bundles"
        ],
        "title": "ListModelBundlesV2Response",
        "description": "Response object for listing Model Bundles."
      },
      "ListModelEndpointsV1Response": {
        "properties": {
          "model_endpoints": {
            "items": {
              "$ref": "#/components/schemas/GetModelEndpointV1Response"
            },
            "type": "array",
            "title": "Model Endpoints"
          }
        },
        "type": "object",
        "required": [
          "model_endpoints"
        ],
        "title": "ListModelEndpointsV1Response"
      },
      "ListTriggersV1Response": {
        "properties": {
          "triggers": {
            "items": {
              "$ref": "#/components/schemas/GetTriggerV1Response"
            },
            "type": "array",
            "title": "Triggers"
          }
        },
        "type": "object",
        "required": [
          "triggers"
        ],
        "title": "ListTriggersV1Response"
      },
      "Logprobs": {
        "properties": {
          "content": {
            "title": "Content",
            "description": "A list of message content tokens with log probability information.",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionTokenLogprob"
            },
            "type": "array",
            "nullable": true
          },
          "refusal": {
            "title": "Refusal",
            "description": "A list of message refusal tokens with log probability information.",
            "items": {
              "$ref": "#/components/schemas/ChatCompletionTokenLogprob"
            },
            "type": "array",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "content",
          "refusal"
        ],
        "title": "Logprobs"
      },
      "Logprobs2": {
        "properties": {
          "text_offset": {
            "title": "Text Offset",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "token_logprobs": {
            "title": "Token Logprobs",
            "items": {
              "type": "number"
            },
            "type": "array",
            "nullable": true
          },
          "tokens": {
            "title": "Tokens",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "top_logprobs": {
            "title": "Top Logprobs",
            "items": {
              "additionalProperties": {
                "type": "number"
              },
              "type": "object"
            },
            "type": "array",
            "nullable": true
          }
        },
        "type": "object",
        "title": "Logprobs2"
      },
      "Metadata": {
        "title": "Metadata",
        "additionalProperties": {
          "type": "string"
        },
        "type": "object",
        "nullable": true
      },
      "ModelBundleEnvironmentParams": {
        "properties": {
          "framework_type": {
            "$ref": "#/components/schemas/ModelBundleFrameworkType"
          },
          "pytorch_image_tag": {
            "title": "Pytorch Image Tag",
            "type": "string",
            "nullable": true
          },
          "tensorflow_version": {
            "title": "Tensorflow Version",
            "type": "string",
            "nullable": true
          },
          "ecr_repo": {
            "title": "Ecr Repo",
            "type": "string",
            "nullable": true
          },
          "image_tag": {
            "title": "Image Tag",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "framework_type"
        ],
        "title": "ModelBundleEnvironmentParams",
        "description": "This is the entity-layer class for the Model Bundle environment parameters. Being an\nentity-layer class, it should be a plain data object."
      },
      "ModelBundleFrameworkType": {
        "type": "string",
        "enum": [
          "pytorch",
          "tensorflow",
          "custom_base_image"
        ],
        "title": "ModelBundleFrameworkType",
        "description": "The canonical list of possible machine learning frameworks of Model Bundles."
      },
      "ModelBundleOrderBy": {
        "type": "string",
        "enum": [
          "newest",
          "oldest"
        ],
        "title": "ModelBundleOrderBy",
        "description": "The canonical list of possible orderings of Model Bundles."
      },
      "ModelBundlePackagingType": {
        "type": "string",
        "enum": [
          "cloudpickle",
          "zip",
          "lira"
        ],
        "title": "ModelBundlePackagingType",
        "description": "The canonical list of possible packaging types for Model Bundles.\n\nThese values broadly determine how the model endpoint will obtain its code & dependencies."
      },
      "ModelBundleV1Response": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "location": {
            "type": "string",
            "title": "Location"
          },
          "requirements": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Requirements"
          },
          "env_params": {
            "$ref": "#/components/schemas/ModelBundleEnvironmentParams"
          },
          "packaging_type": {
            "$ref": "#/components/schemas/ModelBundlePackagingType"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "app_config": {
            "title": "App Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "created_at": {
            "type": "string",
            "format": "date-time",
            "title": "Created At"
          },
          "model_artifact_ids": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Model Artifact Ids"
          },
          "schema_location": {
            "title": "Schema Location",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "id",
          "name",
          "location",
          "requirements",
          "env_params",
          "packaging_type",
          "metadata",
          "created_at",
          "model_artifact_ids"
        ],
        "title": "ModelBundleV1Response",
        "description": "Response object for a single Model Bundle."
      },
      "ModelBundleV2Response": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id"
          },
          "name": {
            "type": "string",
            "title": "Name"
          },
          "metadata": {
            "additionalProperties": true,
            "type": "object",
            "title": "Metadata"
          },
          "created_at": {
            "type": "string",
            "format": "date-time",
            "title": "Created At"
          },
          "model_artifact_ids": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Model Artifact Ids"
          },
          "schema_location": {
            "title": "Schema Location",
            "type": "string",
            "nullable": true
          },
          "flavor": {
            "oneOf": [
              {
                "$ref": "#/components/schemas/CloudpickleArtifactFlavor"
              },
              {
                "$ref": "#/components/schemas/ZipArtifactFlavor"
              },
              {
                "$ref": "#/components/schemas/RunnableImageFlavor"
              },
              {
                "$ref": "#/components/schemas/StreamingEnhancedRunnableImageFlavor"
              },
              {
                "$ref": "#/components/schemas/TritonEnhancedRunnableImageFlavor"
              }
            ],
            "title": "Flavor",
            "discriminator": {
              "propertyName": "flavor",
              "mapping": {
                "cloudpickle_artifact": "#/components/schemas/CloudpickleArtifactFlavor",
                "runnable_image": "#/components/schemas/RunnableImageFlavor",
                "streaming_enhanced_runnable_image": "#/components/schemas/StreamingEnhancedRunnableImageFlavor",
                "triton_enhanced_runnable_image": "#/components/schemas/TritonEnhancedRunnableImageFlavor",
                "zip_artifact": "#/components/schemas/ZipArtifactFlavor"
              }
            }
          }
        },
        "type": "object",
        "required": [
          "id",
          "name",
          "metadata",
          "created_at",
          "model_artifact_ids",
          "flavor"
        ],
        "title": "ModelBundleV2Response",
        "description": "Response object for a single Model Bundle."
      },
      "ModelDownloadRequest": {
        "properties": {
          "model_name": {
            "type": "string",
            "title": "Model Name",
            "description": "Name of the fine tuned model"
          },
          "download_format": {
            "title": "Download Format",
            "description": "Format that you want the downloaded urls to be compatible with. Currently only supports hugging_face",
            "default": "hugging_face",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "model_name"
        ],
        "title": "ModelDownloadRequest"
      },
      "ModelDownloadResponse": {
        "properties": {
          "urls": {
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "title": "Urls",
            "description": "Dictionary of (file_name, url) pairs to download the model from."
          }
        },
        "type": "object",
        "required": [
          "urls"
        ],
        "title": "ModelDownloadResponse"
      },
      "ModelEndpointDeploymentState": {
        "properties": {
          "min_workers": {
            "type": "integer",
            "minimum": 0.0,
            "title": "Min Workers"
          },
          "max_workers": {
            "type": "integer",
            "minimum": 0.0,
            "title": "Max Workers"
          },
          "per_worker": {
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "title": "Per Worker"
          },
          "concurrent_requests_per_worker": {
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "title": "Concurrent Requests Per Worker"
          },
          "available_workers": {
            "title": "Available Workers",
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          },
          "unavailable_workers": {
            "title": "Unavailable Workers",
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "min_workers",
          "max_workers",
          "per_worker",
          "concurrent_requests_per_worker"
        ],
        "title": "ModelEndpointDeploymentState",
        "description": "This is the entity-layer class for the deployment settings related to a Model Endpoint."
      },
      "ModelEndpointOrderBy": {
        "type": "string",
        "enum": [
          "newest",
          "oldest",
          "alphabetical"
        ],
        "title": "ModelEndpointOrderBy",
        "description": "The canonical list of possible orderings of Model Bundles."
      },
      "ModelEndpointResourceState": {
        "properties": {
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus"
          },
          "gpus": {
            "type": "integer",
            "minimum": 0.0,
            "title": "Gpus"
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory"
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "type": "integer",
            "minimum": 1.0,
            "title": "Nodes Per Worker"
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "cpus",
          "gpus",
          "memory",
          "nodes_per_worker"
        ],
        "title": "ModelEndpointResourceState",
        "description": "This is the entity-layer class for the resource settings per worker of a Model Endpoint.\nNote: in the multinode case, there are multiple \"nodes\" per \"worker\".\n\"Nodes\" is analogous to a single k8s pod that may take up all the GPUs on a single machine.\n\"Workers\" is the smallest unit that a request can be made to, and consists of one leader \"node\" and\nmultiple follower \"nodes\" (named \"worker\" in the k8s LeaderWorkerSet definition).\ncpus/gpus/memory/storage are per-node, thus the total consumption by a \"worker\"\nis cpus/gpus/etc. multiplied by nodes_per_worker."
      },
      "ModelEndpointStatus": {
        "type": "string",
        "enum": [
          "READY",
          "UPDATE_PENDING",
          "UPDATE_IN_PROGRESS",
          "UPDATE_FAILED",
          "DELETE_IN_PROGRESS"
        ],
        "title": "ModelEndpointStatus"
      },
      "ModelEndpointType": {
        "type": "string",
        "enum": [
          "async",
          "sync",
          "streaming"
        ],
        "title": "ModelEndpointType"
      },
      "ParallelToolCalls": {
        "type": "boolean",
        "title": "ParallelToolCalls",
        "description": "Whether to enable [parallel function calling](/docs/guides/function-calling#configuring-parallel-function-calling) during tool use."
      },
      "PredictionContent": {
        "properties": {
          "type": {
            "type": "string",
            "const": "content",
            "title": "Type",
            "description": "The type of the predicted content you want to provide. This type is\ncurrently always `content`.\n"
          },
          "content": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "$ref": "#/components/schemas/Content8"
              }
            ],
            "title": "Content",
            "description": "The content that should be matched when generating a model response.\nIf generated tokens would match this content, the entire model response\ncan be returned much more quickly.\n"
          }
        },
        "type": "object",
        "required": [
          "type",
          "content"
        ],
        "title": "PredictionContent"
      },
      "Prompt": {
        "title": "Prompt",
        "description": "The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n",
        "default": "<|endoftext|>",
        "items": {
          "type": "integer"
        },
        "type": "array",
        "minItems": 1,
        "nullable": true
      },
      "Prompt1": {
        "title": "Prompt1",
        "description": "The prompt(s) to generate completions for, encoded as a string, array of strings, array of tokens, or array of token arrays.\n\nNote that <|endoftext|> is the document separator that the model sees during training, so if a prompt is not specified the model will generate as if from the beginning of a new document.\n",
        "default": "<|endoftext|>",
        "items": {
          "$ref": "#/components/schemas/Prompt1Item"
        },
        "type": "array",
        "minItems": 1,
        "nullable": true
      },
      "Prompt1Item": {
        "items": {
          "type": "integer"
        },
        "type": "array",
        "minItems": 1,
        "title": "Prompt1Item"
      },
      "PromptTokensDetails": {
        "properties": {
          "audio_tokens": {
            "type": "integer",
            "title": "Audio Tokens",
            "description": "Audio input tokens present in the prompt.",
            "default": 0
          },
          "cached_tokens": {
            "type": "integer",
            "title": "Cached Tokens",
            "description": "Cached tokens present in the prompt.",
            "default": 0
          }
        },
        "type": "object",
        "title": "PromptTokensDetails"
      },
      "PytorchFramework": {
        "properties": {
          "framework_type": {
            "type": "string",
            "const": "pytorch",
            "title": "Framework Type"
          },
          "pytorch_image_tag": {
            "type": "string",
            "title": "Pytorch Image Tag"
          }
        },
        "type": "object",
        "required": [
          "framework_type",
          "pytorch_image_tag"
        ],
        "title": "PytorchFramework",
        "description": "This is the entity-layer class for a Pytorch framework specification."
      },
      "Quantization": {
        "type": "string",
        "enum": [
          "bitsandbytes",
          "awq"
        ],
        "title": "Quantization"
      },
      "ReasoningEffort": {
        "title": "ReasoningEffort",
        "description": "**o-series models only** \n\nConstrains effort on reasoning for \n[reasoning models](https://platform.openai.com/docs/guides/reasoning).\nCurrently supported values are `low`, `medium`, and `high`. Reducing\nreasoning effort can result in faster responses and fewer tokens used\non reasoning in a response.\n",
        "default": "medium",
        "type": "string",
        "enum": [
          "low",
          "medium",
          "high"
        ],
        "nullable": true
      },
      "RequestSchema": {
        "title": "RequestSchema"
      },
      "ResponseFormatJsonObject": {
        "properties": {
          "type": {
            "type": "string",
            "const": "json_object",
            "title": "Type",
            "description": "The type of response format being defined. Always `json_object`."
          }
        },
        "type": "object",
        "required": [
          "type"
        ],
        "title": "ResponseFormatJsonObject"
      },
      "ResponseFormatJsonSchema": {
        "properties": {
          "type": {
            "type": "string",
            "const": "json_schema",
            "title": "Type",
            "description": "The type of response format being defined. Always `json_schema`."
          },
          "json_schema": {
            "$ref": "#/components/schemas/JsonSchema",
            "title": "JSON schema",
            "description": "Structured Outputs configuration options, including a JSON Schema.\n"
          }
        },
        "type": "object",
        "required": [
          "type",
          "json_schema"
        ],
        "title": "ResponseFormatJsonSchema"
      },
      "ResponseFormatJsonSchemaSchema": {
        "properties": {},
        "additionalProperties": true,
        "type": "object",
        "title": "ResponseFormatJsonSchemaSchema"
      },
      "ResponseFormatText": {
        "properties": {
          "type": {
            "type": "string",
            "const": "text",
            "title": "Type",
            "description": "The type of response format being defined. Always `text`."
          }
        },
        "type": "object",
        "required": [
          "type"
        ],
        "title": "ResponseFormatText"
      },
      "ResponseModalities": {
        "title": "ResponseModalities",
        "description": "Output types that you would like the model to generate.\nMost models are capable of generating text, which is the default:\n\n`[\"text\"]`\n\nThe `gpt-4o-audio-preview` model can also be used to \n[generate audio](/docs/guides/audio). To request that this model generate \nboth text and audio responses, you can use:\n\n`[\"text\", \"audio\"]`\n",
        "items": {
          "type": "string",
          "enum": [
            "text",
            "audio"
          ]
        },
        "type": "array",
        "nullable": true
      },
      "ResponseSchema": {
        "title": "ResponseSchema"
      },
      "RestartModelEndpointV1Response": {
        "properties": {
          "restarted": {
            "type": "boolean",
            "title": "Restarted"
          }
        },
        "type": "object",
        "required": [
          "restarted"
        ],
        "title": "RestartModelEndpointV1Response"
      },
      "RunnableImageFlavor": {
        "properties": {
          "repository": {
            "type": "string",
            "title": "Repository"
          },
          "tag": {
            "type": "string",
            "title": "Tag"
          },
          "command": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Command"
          },
          "predict_route": {
            "type": "string",
            "title": "Predict Route",
            "default": "/predict"
          },
          "healthcheck_route": {
            "type": "string",
            "title": "Healthcheck Route",
            "default": "/readyz"
          },
          "env": {
            "title": "Env",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "protocol": {
            "type": "string",
            "const": "http",
            "title": "Protocol"
          },
          "readiness_initial_delay_seconds": {
            "type": "integer",
            "title": "Readiness Initial Delay Seconds",
            "default": 120
          },
          "extra_routes": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Extra Routes"
          },
          "routes": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Routes"
          },
          "forwarder_type": {
            "title": "Forwarder Type",
            "default": "default",
            "type": "string",
            "nullable": true
          },
          "worker_command": {
            "title": "Worker Command",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "worker_env": {
            "title": "Worker Env",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "flavor": {
            "type": "string",
            "const": "runnable_image",
            "title": "Flavor"
          }
        },
        "type": "object",
        "required": [
          "repository",
          "tag",
          "command",
          "protocol",
          "flavor"
        ],
        "title": "RunnableImageFlavor",
        "description": "This is the entity-layer class for the Model Bundle flavor of a runnable image."
      },
      "ServiceTier": {
        "title": "ServiceTier",
        "description": "Specifies the latency tier to use for processing the request. This parameter is relevant for customers subscribed to the scale tier service:\n  - If set to 'auto', and the Project is Scale tier enabled, the system\n    will utilize scale tier credits until they are exhausted.\n  - If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\n  - If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.\n  - If set to 'flex', the request will be processed with the Flex Processing service tier. [Learn more](/docs/guides/flex-processing).\n  - When not set, the default behavior is 'auto'.\n\n  When this parameter is set, the response body will include the `service_tier` utilized.\n",
        "default": "auto",
        "type": "string",
        "enum": [
          "auto",
          "default",
          "flex"
        ],
        "nullable": true
      },
      "StopConfiguration": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "$ref": "#/components/schemas/StopConfiguration1"
          }
        ],
        "title": "StopConfiguration",
        "description": "Not supported with latest reasoning models `o3` and `o4-mini`.\n\nUp to 4 sequences where the API will stop generating further tokens. The\nreturned text will not contain the stop sequence.\n",
        "nullable": true
      },
      "StopConfiguration1": {
        "title": "StopConfiguration1",
        "description": "Not supported with latest reasoning models `o3` and `o4-mini`.\n\nUp to 4 sequences where the API will stop generating further tokens. The\nreturned text will not contain the stop sequence.\n",
        "items": {
          "type": "string"
        },
        "type": "array",
        "maxItems": 4,
        "minItems": 1,
        "nullable": true
      },
      "StreamError": {
        "properties": {
          "status_code": {
            "type": "integer",
            "title": "Status Code"
          },
          "content": {
            "$ref": "#/components/schemas/StreamErrorContent"
          }
        },
        "type": "object",
        "required": [
          "status_code",
          "content"
        ],
        "title": "StreamError",
        "description": "Error object for a stream prompt completion task."
      },
      "StreamErrorContent": {
        "properties": {
          "error": {
            "type": "string",
            "title": "Error"
          },
          "timestamp": {
            "type": "string",
            "title": "Timestamp"
          }
        },
        "type": "object",
        "required": [
          "error",
          "timestamp"
        ],
        "title": "StreamErrorContent"
      },
      "StreamingEnhancedRunnableImageFlavor": {
        "properties": {
          "repository": {
            "type": "string",
            "title": "Repository"
          },
          "tag": {
            "type": "string",
            "title": "Tag"
          },
          "command": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Command",
            "default": []
          },
          "predict_route": {
            "type": "string",
            "title": "Predict Route",
            "default": "/predict"
          },
          "healthcheck_route": {
            "type": "string",
            "title": "Healthcheck Route",
            "default": "/readyz"
          },
          "env": {
            "title": "Env",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "protocol": {
            "type": "string",
            "const": "http",
            "title": "Protocol"
          },
          "readiness_initial_delay_seconds": {
            "type": "integer",
            "title": "Readiness Initial Delay Seconds",
            "default": 120
          },
          "extra_routes": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Extra Routes"
          },
          "routes": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Routes"
          },
          "forwarder_type": {
            "title": "Forwarder Type",
            "default": "default",
            "type": "string",
            "nullable": true
          },
          "worker_command": {
            "title": "Worker Command",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "worker_env": {
            "title": "Worker Env",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "flavor": {
            "type": "string",
            "const": "streaming_enhanced_runnable_image",
            "title": "Flavor"
          },
          "streaming_command": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Streaming Command"
          },
          "streaming_predict_route": {
            "type": "string",
            "title": "Streaming Predict Route",
            "default": "/stream"
          }
        },
        "type": "object",
        "required": [
          "repository",
          "tag",
          "protocol",
          "flavor",
          "streaming_command"
        ],
        "title": "StreamingEnhancedRunnableImageFlavor",
        "description": "For deployments that expose a streaming route in a container."
      },
      "SyncEndpointPredictV1Request": {
        "properties": {
          "url": {
            "title": "Url",
            "type": "string",
            "nullable": true
          },
          "args": {
            "$ref": "#/components/schemas/RequestSchema",
            "nullable": true
          },
          "cloudpickle": {
            "title": "Cloudpickle",
            "type": "string",
            "nullable": true
          },
          "callback_url": {
            "title": "Callback Url",
            "type": "string",
            "nullable": true
          },
          "callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "return_pickled": {
            "type": "boolean",
            "title": "Return Pickled",
            "default": false
          },
          "destination_path": {
            "title": "Destination Path",
            "type": "string",
            "nullable": true
          },
          "timeout_seconds": {
            "title": "Timeout Seconds",
            "type": "number",
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "num_retries": {
            "title": "Num Retries",
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          }
        },
        "type": "object",
        "title": "SyncEndpointPredictV1Request"
      },
      "SyncEndpointPredictV1Response": {
        "properties": {
          "status": {
            "$ref": "#/components/schemas/TaskStatus"
          },
          "result": {
            "title": "Result",
            "nullable": true
          },
          "traceback": {
            "title": "Traceback",
            "type": "string",
            "nullable": true
          },
          "status_code": {
            "title": "Status Code",
            "type": "integer",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "status"
        ],
        "title": "SyncEndpointPredictV1Response"
      },
      "TaskStatus": {
        "type": "string",
        "enum": [
          "PENDING",
          "STARTED",
          "SUCCESS",
          "FAILURE",
          "UNDEFINED"
        ],
        "title": "TaskStatus"
      },
      "TensorflowFramework": {
        "properties": {
          "framework_type": {
            "type": "string",
            "const": "tensorflow",
            "title": "Framework Type"
          },
          "tensorflow_version": {
            "type": "string",
            "title": "Tensorflow Version"
          }
        },
        "type": "object",
        "required": [
          "framework_type",
          "tensorflow_version"
        ],
        "title": "TensorflowFramework",
        "description": "This is the entity-layer class for a Tensorflow framework specification."
      },
      "TokenOutput": {
        "properties": {
          "token": {
            "type": "string",
            "title": "Token"
          },
          "log_prob": {
            "type": "number",
            "title": "Log Prob"
          }
        },
        "type": "object",
        "required": [
          "token",
          "log_prob"
        ],
        "title": "TokenOutput",
        "description": "Detailed token information."
      },
      "ToolConfig": {
        "properties": {
          "name": {
            "type": "string",
            "title": "Name"
          },
          "max_iterations": {
            "title": "Max Iterations",
            "default": 10,
            "type": "integer",
            "nullable": true
          },
          "execution_timeout_seconds": {
            "title": "Execution Timeout Seconds",
            "default": 60,
            "type": "integer",
            "nullable": true
          },
          "should_retry_on_error": {
            "title": "Should Retry On Error",
            "default": true,
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "name"
        ],
        "title": "ToolConfig",
        "description": "Configuration for tool use.\nNOTE: this config is highly experimental and signature will change significantly in future iterations."
      },
      "TopLogprob": {
        "properties": {
          "token": {
            "type": "string",
            "title": "Token",
            "description": "The token."
          },
          "logprob": {
            "type": "number",
            "title": "Logprob",
            "description": "The log probability of this token, if it is within the top 20 most likely tokens. Otherwise, the value `-9999.0` is used to signify that the token is very unlikely."
          },
          "bytes": {
            "title": "Bytes",
            "description": "A list of integers representing the UTF-8 bytes representation of the token. Useful in instances where characters are represented by multiple tokens and their byte representations must be combined to generate the correct text representation. Can be `null` if there is no bytes representation for the token.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "token",
          "logprob",
          "bytes"
        ],
        "title": "TopLogprob"
      },
      "TritonEnhancedRunnableImageFlavor": {
        "properties": {
          "repository": {
            "type": "string",
            "title": "Repository"
          },
          "tag": {
            "type": "string",
            "title": "Tag"
          },
          "command": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Command"
          },
          "predict_route": {
            "type": "string",
            "title": "Predict Route",
            "default": "/predict"
          },
          "healthcheck_route": {
            "type": "string",
            "title": "Healthcheck Route",
            "default": "/readyz"
          },
          "env": {
            "title": "Env",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "protocol": {
            "type": "string",
            "const": "http",
            "title": "Protocol"
          },
          "readiness_initial_delay_seconds": {
            "type": "integer",
            "title": "Readiness Initial Delay Seconds",
            "default": 120
          },
          "extra_routes": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Extra Routes"
          },
          "routes": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Routes"
          },
          "forwarder_type": {
            "title": "Forwarder Type",
            "default": "default",
            "type": "string",
            "nullable": true
          },
          "worker_command": {
            "title": "Worker Command",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "worker_env": {
            "title": "Worker Env",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "flavor": {
            "type": "string",
            "const": "triton_enhanced_runnable_image",
            "title": "Flavor"
          },
          "triton_model_repository": {
            "type": "string",
            "title": "Triton Model Repository"
          },
          "triton_model_replicas": {
            "title": "Triton Model Replicas",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "triton_num_cpu": {
            "type": "number",
            "title": "Triton Num Cpu"
          },
          "triton_commit_tag": {
            "type": "string",
            "title": "Triton Commit Tag"
          },
          "triton_storage": {
            "title": "Triton Storage",
            "type": "string",
            "nullable": true
          },
          "triton_memory": {
            "title": "Triton Memory",
            "type": "string",
            "nullable": true
          },
          "triton_readiness_initial_delay_seconds": {
            "type": "integer",
            "title": "Triton Readiness Initial Delay Seconds",
            "default": 300
          }
        },
        "type": "object",
        "required": [
          "repository",
          "tag",
          "command",
          "protocol",
          "flavor",
          "triton_model_repository",
          "triton_num_cpu",
          "triton_commit_tag"
        ],
        "title": "TritonEnhancedRunnableImageFlavor",
        "description": "For deployments that require tritonserver running in a container."
      },
      "UpdateBatchCompletionsV2Request": {
        "properties": {
          "job_id": {
            "type": "string",
            "title": "Job Id",
            "description": "ID of the batch completions job"
          },
          "priority": {
            "title": "Priority",
            "description": "Priority of the batch inference job. Default to None.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "required": [
          "job_id"
        ],
        "title": "UpdateBatchCompletionsV2Request"
      },
      "UpdateBatchCompletionsV2Response": {
        "properties": {
          "job_id": {
            "type": "string",
            "title": "Job Id"
          },
          "input_data_path": {
            "title": "Input Data Path",
            "description": "Path to the input file. The input file should be a JSON file of type List[CreateBatchCompletionsRequestContent].",
            "type": "string",
            "nullable": true
          },
          "output_data_path": {
            "type": "string",
            "title": "Output Data Path",
            "description": "Path to the output file. The output file will be a JSON file of type List[CompletionOutput]."
          },
          "model_config": {
            "$ref": "#/components/schemas/BatchCompletionsModelConfig",
            "description": "Model configuration for the batch inference. Hardware configurations are inferred."
          },
          "priority": {
            "title": "Priority",
            "description": "Priority of the batch inference job. Default to None.",
            "type": "string",
            "nullable": true
          },
          "status": {
            "$ref": "#/components/schemas/BatchCompletionsJobStatus"
          },
          "created_at": {
            "type": "string",
            "title": "Created At"
          },
          "expires_at": {
            "type": "string",
            "title": "Expires At"
          },
          "completed_at": {
            "title": "Completed At",
            "type": "string",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "success": {
            "type": "boolean",
            "title": "Success",
            "description": "Whether the update was successful"
          }
        },
        "type": "object",
        "required": [
          "job_id",
          "output_data_path",
          "model_config",
          "status",
          "created_at",
          "expires_at",
          "completed_at",
          "metadata",
          "success"
        ],
        "title": "UpdateBatchCompletionsV2Response"
      },
      "UpdateBatchJobV1Request": {
        "properties": {
          "cancel": {
            "type": "boolean",
            "title": "Cancel"
          }
        },
        "type": "object",
        "required": [
          "cancel"
        ],
        "title": "UpdateBatchJobV1Request"
      },
      "UpdateBatchJobV1Response": {
        "properties": {
          "success": {
            "type": "boolean",
            "title": "Success"
          }
        },
        "type": "object",
        "required": [
          "success"
        ],
        "title": "UpdateBatchJobV1Response"
      },
      "UpdateDeepSpeedModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "model_name": {
            "title": "Model Name",
            "type": "string",
            "nullable": true
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "nullable": true
          },
          "inference_framework": {
            "type": "string",
            "const": "deepspeed",
            "title": "Inference Framework",
            "default": "deepspeed"
          },
          "inference_framework_image_tag": {
            "title": "Inference Framework Image Tag",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "type": "integer",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "force_bundle_recreation": {
            "title": "Force Bundle Recreation",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "min_workers": {
            "title": "Min Workers",
            "type": "integer",
            "nullable": true
          },
          "max_workers": {
            "title": "Max Workers",
            "type": "integer",
            "nullable": true
          },
          "per_worker": {
            "title": "Per Worker",
            "type": "integer",
            "nullable": true
          },
          "labels": {
            "title": "Labels",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "title": "UpdateDeepSpeedModelEndpointRequest"
      },
      "UpdateDockerImageBatchJobV1Request": {
        "properties": {
          "cancel": {
            "type": "boolean",
            "title": "Cancel"
          }
        },
        "type": "object",
        "required": [
          "cancel"
        ],
        "title": "UpdateDockerImageBatchJobV1Request"
      },
      "UpdateDockerImageBatchJobV1Response": {
        "properties": {
          "success": {
            "type": "boolean",
            "title": "Success"
          }
        },
        "type": "object",
        "required": [
          "success"
        ],
        "title": "UpdateDockerImageBatchJobV1Response"
      },
      "UpdateLLMModelEndpointV1Response": {
        "properties": {
          "endpoint_creation_task_id": {
            "type": "string",
            "title": "Endpoint Creation Task Id"
          }
        },
        "type": "object",
        "required": [
          "endpoint_creation_task_id"
        ],
        "title": "UpdateLLMModelEndpointV1Response"
      },
      "UpdateModelEndpointV1Request": {
        "properties": {
          "model_bundle_id": {
            "title": "Model Bundle Id",
            "type": "string",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "min_workers": {
            "title": "Min Workers",
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          },
          "max_workers": {
            "title": "Max Workers",
            "type": "integer",
            "minimum": 0.0,
            "nullable": true
          },
          "per_worker": {
            "title": "Per Worker",
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "concurrent_requests_per_worker": {
            "title": "Concurrent Requests Per Worker",
            "type": "integer",
            "exclusiveMinimum": 0.0,
            "nullable": true
          },
          "labels": {
            "title": "Labels",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "title": "UpdateModelEndpointV1Request"
      },
      "UpdateModelEndpointV1Response": {
        "properties": {
          "endpoint_creation_task_id": {
            "type": "string",
            "title": "Endpoint Creation Task Id"
          }
        },
        "type": "object",
        "required": [
          "endpoint_creation_task_id"
        ],
        "title": "UpdateModelEndpointV1Response"
      },
      "UpdateSGLangModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "model_name": {
            "title": "Model Name",
            "type": "string",
            "nullable": true
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "nullable": true
          },
          "inference_framework": {
            "type": "string",
            "const": "sglang",
            "title": "Inference Framework",
            "default": "sglang"
          },
          "inference_framework_image_tag": {
            "title": "Inference Framework Image Tag",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "type": "integer",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "force_bundle_recreation": {
            "title": "Force Bundle Recreation",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "min_workers": {
            "title": "Min Workers",
            "type": "integer",
            "nullable": true
          },
          "max_workers": {
            "title": "Max Workers",
            "type": "integer",
            "nullable": true
          },
          "per_worker": {
            "title": "Per Worker",
            "type": "integer",
            "nullable": true
          },
          "labels": {
            "title": "Labels",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "trust_remote_code": {
            "title": "Trust Remote Code",
            "description": "Whether to trust remote code from Hugging face hub. This is only applicable to models whose code is not supported natively by the transformers library (e.g. deepseek). Default to False.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "tp_size": {
            "title": "Tp Size",
            "description": "The tensor parallel size.",
            "type": "integer",
            "nullable": true
          },
          "skip_tokenizer_init": {
            "title": "Skip Tokenizer Init",
            "description": "If set, skip init tokenizer and pass input_ids in generate request",
            "type": "boolean",
            "nullable": true
          },
          "load_format": {
            "title": "Load Format",
            "description": "The format of the model weights to load.",
            "type": "string",
            "nullable": true
          },
          "dtype": {
            "title": "Dtype",
            "description": "Data type for model weights and activations.",
            "type": "string",
            "nullable": true
          },
          "kv_cache_dtype": {
            "title": "Kv Cache Dtype",
            "description": "Data type for kv cache storage. \"auto\" will use model data type.",
            "type": "string",
            "nullable": true
          },
          "quantization_param_path": {
            "title": "Quantization Param Path",
            "description": "Path to the JSON file containing the KV cache scaling factors.",
            "type": "string",
            "nullable": true
          },
          "quantization": {
            "title": "Quantization",
            "description": "The quantization method.",
            "type": "string",
            "nullable": true
          },
          "context_length": {
            "title": "Context Length",
            "description": "The model's maximum context length.",
            "type": "integer",
            "nullable": true
          },
          "device": {
            "title": "Device",
            "description": "The device type.",
            "type": "string",
            "nullable": true
          },
          "served_model_name": {
            "title": "Served Model Name",
            "description": "Override the model name returned by the v1/models endpoint in OpenAI API server.",
            "type": "string",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "The builtin chat template name or path of the chat template file.",
            "type": "string",
            "nullable": true
          },
          "is_embedding": {
            "title": "Is Embedding",
            "description": "Whether to use a CausalLM as an embedding model.",
            "type": "boolean",
            "nullable": true
          },
          "revision": {
            "title": "Revision",
            "description": "The specific model version to use.",
            "type": "string",
            "nullable": true
          },
          "mem_fraction_static": {
            "title": "Mem Fraction Static",
            "description": "The fraction of the memory used for static allocation.",
            "type": "number",
            "nullable": true
          },
          "max_running_requests": {
            "title": "Max Running Requests",
            "description": "The maximum number of running requests.",
            "type": "integer",
            "nullable": true
          },
          "max_total_tokens": {
            "title": "Max Total Tokens",
            "description": "The maximum number of tokens in the memory pool.",
            "type": "integer",
            "nullable": true
          },
          "chunked_prefill_size": {
            "title": "Chunked Prefill Size",
            "description": "The maximum number of tokens in a chunk for the chunked prefill.",
            "type": "integer",
            "nullable": true
          },
          "max_prefill_tokens": {
            "title": "Max Prefill Tokens",
            "description": "The maximum number of tokens in a prefill batch.",
            "type": "integer",
            "nullable": true
          },
          "schedule_policy": {
            "title": "Schedule Policy",
            "description": "The scheduling policy of the requests.",
            "type": "string",
            "nullable": true
          },
          "schedule_conservativeness": {
            "title": "Schedule Conservativeness",
            "description": "How conservative the schedule policy is.",
            "type": "number",
            "nullable": true
          },
          "cpu_offload_gb": {
            "title": "Cpu Offload Gb",
            "description": "How many GBs of RAM to reserve for CPU offloading",
            "type": "integer",
            "nullable": true
          },
          "prefill_only_one_req": {
            "title": "Prefill Only One Req",
            "description": "If true, we only prefill one request at one prefill batch",
            "type": "boolean",
            "nullable": true
          },
          "stream_interval": {
            "title": "Stream Interval",
            "description": "The interval for streaming in terms of the token length.",
            "type": "integer",
            "nullable": true
          },
          "random_seed": {
            "title": "Random Seed",
            "description": "The random seed.",
            "type": "integer",
            "nullable": true
          },
          "constrained_json_whitespace_pattern": {
            "title": "Constrained Json Whitespace Pattern",
            "description": "Regex pattern for syntactic whitespaces allowed in JSON constrained output.",
            "type": "string",
            "nullable": true
          },
          "watchdog_timeout": {
            "title": "Watchdog Timeout",
            "description": "Set watchdog timeout in seconds.",
            "type": "number",
            "nullable": true
          },
          "download_dir": {
            "title": "Download Dir",
            "description": "Model download directory.",
            "type": "string",
            "nullable": true
          },
          "base_gpu_id": {
            "title": "Base Gpu Id",
            "description": "The base GPU ID to start allocating GPUs from.",
            "type": "integer",
            "nullable": true
          },
          "log_level": {
            "title": "Log Level",
            "description": "The logging level of all loggers.",
            "type": "string",
            "nullable": true
          },
          "log_level_http": {
            "title": "Log Level Http",
            "description": "The logging level of HTTP server.",
            "type": "string",
            "nullable": true
          },
          "log_requests": {
            "title": "Log Requests",
            "description": "Log the inputs and outputs of all requests.",
            "type": "boolean",
            "nullable": true
          },
          "show_time_cost": {
            "title": "Show Time Cost",
            "description": "Show time cost of custom marks.",
            "type": "boolean",
            "nullable": true
          },
          "enable_metrics": {
            "title": "Enable Metrics",
            "description": "Enable log prometheus metrics.",
            "type": "boolean",
            "nullable": true
          },
          "decode_log_interval": {
            "title": "Decode Log Interval",
            "description": "The log interval of decode batch.",
            "type": "integer",
            "nullable": true
          },
          "api_key": {
            "title": "Api Key",
            "description": "Set API key of the server.",
            "type": "string",
            "nullable": true
          },
          "file_storage_pth": {
            "title": "File Storage Pth",
            "description": "The path of the file storage in backend.",
            "type": "string",
            "nullable": true
          },
          "enable_cache_report": {
            "title": "Enable Cache Report",
            "description": "Return number of cached tokens in usage.prompt_tokens_details.",
            "type": "boolean",
            "nullable": true
          },
          "data_parallel_size": {
            "title": "Data Parallel Size",
            "description": "The data parallelism size.",
            "type": "integer",
            "nullable": true
          },
          "load_balance_method": {
            "title": "Load Balance Method",
            "description": "The load balancing strategy for data parallelism.",
            "type": "string",
            "nullable": true
          },
          "expert_parallel_size": {
            "title": "Expert Parallel Size",
            "description": "The expert parallelism size.",
            "type": "integer",
            "nullable": true
          },
          "dist_init_addr": {
            "title": "Dist Init Addr",
            "description": "The host address for initializing distributed backend.",
            "type": "string",
            "nullable": true
          },
          "nnodes": {
            "title": "Nnodes",
            "description": "The number of nodes.",
            "type": "integer",
            "nullable": true
          },
          "node_rank": {
            "title": "Node Rank",
            "description": "The node rank.",
            "type": "integer",
            "nullable": true
          },
          "json_model_override_args": {
            "title": "Json Model Override Args",
            "description": "A dictionary in JSON string format used to override default model configurations.",
            "type": "string",
            "nullable": true
          },
          "lora_paths": {
            "title": "Lora Paths",
            "description": "The list of LoRA adapters.",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "max_loras_per_batch": {
            "title": "Max Loras Per Batch",
            "description": "Maximum number of adapters for a running batch.",
            "type": "integer",
            "nullable": true
          },
          "attention_backend": {
            "title": "Attention Backend",
            "description": "Choose the kernels for attention layers.",
            "type": "string",
            "nullable": true
          },
          "sampling_backend": {
            "title": "Sampling Backend",
            "description": "Choose the kernels for sampling layers.",
            "type": "string",
            "nullable": true
          },
          "grammar_backend": {
            "title": "Grammar Backend",
            "description": "Choose the backend for grammar-guided decoding.",
            "type": "string",
            "nullable": true
          },
          "speculative_algorithm": {
            "title": "Speculative Algorithm",
            "description": "Speculative algorithm.",
            "type": "string",
            "nullable": true
          },
          "speculative_draft_model_path": {
            "title": "Speculative Draft Model Path",
            "description": "The path of the draft model weights.",
            "type": "string",
            "nullable": true
          },
          "speculative_num_steps": {
            "title": "Speculative Num Steps",
            "description": "The number of steps sampled from draft model in Speculative Decoding.",
            "type": "integer",
            "nullable": true
          },
          "speculative_num_draft_tokens": {
            "title": "Speculative Num Draft Tokens",
            "description": "The number of token sampled from draft model in Speculative Decoding.",
            "type": "integer",
            "nullable": true
          },
          "speculative_eagle_topk": {
            "title": "Speculative Eagle Topk",
            "description": "The number of token sampled from draft model in eagle2 each step.",
            "type": "integer",
            "nullable": true
          },
          "enable_double_sparsity": {
            "title": "Enable Double Sparsity",
            "description": "Enable double sparsity attention",
            "type": "boolean",
            "nullable": true
          },
          "ds_channel_config_path": {
            "title": "Ds Channel Config Path",
            "description": "The path of the double sparsity channel config",
            "type": "string",
            "nullable": true
          },
          "ds_heavy_channel_num": {
            "title": "Ds Heavy Channel Num",
            "description": "The number of heavy channels in double sparsity attention",
            "type": "integer",
            "nullable": true
          },
          "ds_heavy_token_num": {
            "title": "Ds Heavy Token Num",
            "description": "The number of heavy tokens in double sparsity attention",
            "type": "integer",
            "nullable": true
          },
          "ds_heavy_channel_type": {
            "title": "Ds Heavy Channel Type",
            "description": "The type of heavy channels in double sparsity attention",
            "type": "string",
            "nullable": true
          },
          "ds_sparse_decode_threshold": {
            "title": "Ds Sparse Decode Threshold",
            "description": "The threshold for sparse decoding in double sparsity attention",
            "type": "integer",
            "nullable": true
          },
          "disable_radix_cache": {
            "title": "Disable Radix Cache",
            "description": "Disable RadixAttention for prefix caching.",
            "type": "boolean",
            "nullable": true
          },
          "disable_jump_forward": {
            "title": "Disable Jump Forward",
            "description": "Disable jump-forward for grammar-guided decoding.",
            "type": "boolean",
            "nullable": true
          },
          "disable_cuda_graph": {
            "title": "Disable Cuda Graph",
            "description": "Disable cuda graph.",
            "type": "boolean",
            "nullable": true
          },
          "disable_cuda_graph_padding": {
            "title": "Disable Cuda Graph Padding",
            "description": "Disable cuda graph when padding is needed.",
            "type": "boolean",
            "nullable": true
          },
          "disable_outlines_disk_cache": {
            "title": "Disable Outlines Disk Cache",
            "description": "Disable disk cache of outlines.",
            "type": "boolean",
            "nullable": true
          },
          "disable_custom_all_reduce": {
            "title": "Disable Custom All Reduce",
            "description": "Disable the custom all-reduce kernel.",
            "type": "boolean",
            "nullable": true
          },
          "disable_mla": {
            "title": "Disable Mla",
            "description": "Disable Multi-head Latent Attention (MLA) for DeepSeek-V2.",
            "type": "boolean",
            "nullable": true
          },
          "disable_overlap_schedule": {
            "title": "Disable Overlap Schedule",
            "description": "Disable the overlap scheduler.",
            "type": "boolean",
            "nullable": true
          },
          "enable_mixed_chunk": {
            "title": "Enable Mixed Chunk",
            "description": "Enable mixing prefill and decode in a batch when using chunked prefill.",
            "type": "boolean",
            "nullable": true
          },
          "enable_dp_attention": {
            "title": "Enable Dp Attention",
            "description": "Enable data parallelism for attention and tensor parallelism for FFN.",
            "type": "boolean",
            "nullable": true
          },
          "enable_ep_moe": {
            "title": "Enable Ep Moe",
            "description": "Enable expert parallelism for moe.",
            "type": "boolean",
            "nullable": true
          },
          "enable_torch_compile": {
            "title": "Enable Torch Compile",
            "description": "Optimize the model with torch.compile.",
            "type": "boolean",
            "nullable": true
          },
          "torch_compile_max_bs": {
            "title": "Torch Compile Max Bs",
            "description": "Set the maximum batch size when using torch compile.",
            "type": "integer",
            "nullable": true
          },
          "cuda_graph_max_bs": {
            "title": "Cuda Graph Max Bs",
            "description": "Set the maximum batch size for cuda graph.",
            "type": "integer",
            "nullable": true
          },
          "cuda_graph_bs": {
            "title": "Cuda Graph Bs",
            "description": "Set the list of batch sizes for cuda graph.",
            "items": {
              "type": "integer"
            },
            "type": "array",
            "nullable": true
          },
          "torchao_config": {
            "title": "Torchao Config",
            "description": "Optimize the model with torchao.",
            "type": "string",
            "nullable": true
          },
          "enable_nan_detection": {
            "title": "Enable Nan Detection",
            "description": "Enable the NaN detection for debugging purposes.",
            "type": "boolean",
            "nullable": true
          },
          "enable_p2p_check": {
            "title": "Enable P2P Check",
            "description": "Enable P2P check for GPU access.",
            "type": "boolean",
            "nullable": true
          },
          "triton_attention_reduce_in_fp32": {
            "title": "Triton Attention Reduce In Fp32",
            "description": "Cast the intermediate attention results to fp32.",
            "type": "boolean",
            "nullable": true
          },
          "triton_attention_num_kv_splits": {
            "title": "Triton Attention Num Kv Splits",
            "description": "The number of KV splits in flash decoding Triton kernel.",
            "type": "integer",
            "nullable": true
          },
          "num_continuous_decode_steps": {
            "title": "Num Continuous Decode Steps",
            "description": "Run multiple continuous decoding steps to reduce scheduling overhead.",
            "type": "integer",
            "nullable": true
          },
          "delete_ckpt_after_loading": {
            "title": "Delete Ckpt After Loading",
            "description": "Delete the model checkpoint after loading the model.",
            "type": "boolean",
            "nullable": true
          },
          "enable_memory_saver": {
            "title": "Enable Memory Saver",
            "description": "Allow saving memory using release_memory_occupation and resume_memory_occupation",
            "type": "boolean",
            "nullable": true
          },
          "allow_auto_truncate": {
            "title": "Allow Auto Truncate",
            "description": "Allow automatically truncating requests that exceed the maximum input length.",
            "type": "boolean",
            "nullable": true
          },
          "enable_custom_logit_processor": {
            "title": "Enable Custom Logit Processor",
            "description": "Enable users to pass custom logit processors to the server.",
            "type": "boolean",
            "nullable": true
          },
          "tool_call_parser": {
            "title": "Tool Call Parser",
            "description": "Specify the parser for handling tool-call interactions.",
            "type": "string",
            "nullable": true
          },
          "huggingface_repo": {
            "title": "Huggingface Repo",
            "description": "The Hugging Face repository ID.",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "UpdateSGLangModelEndpointRequest"
      },
      "UpdateTextGenerationInferenceModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "model_name": {
            "title": "Model Name",
            "type": "string",
            "nullable": true
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "nullable": true
          },
          "inference_framework": {
            "type": "string",
            "const": "text_generation_inference",
            "title": "Inference Framework",
            "default": "text_generation_inference"
          },
          "inference_framework_image_tag": {
            "title": "Inference Framework Image Tag",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "type": "integer",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "force_bundle_recreation": {
            "title": "Force Bundle Recreation",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "min_workers": {
            "title": "Min Workers",
            "type": "integer",
            "nullable": true
          },
          "max_workers": {
            "title": "Max Workers",
            "type": "integer",
            "nullable": true
          },
          "per_worker": {
            "title": "Per Worker",
            "type": "integer",
            "nullable": true
          },
          "labels": {
            "title": "Labels",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          }
        },
        "type": "object",
        "title": "UpdateTextGenerationInferenceModelEndpointRequest"
      },
      "UpdateTriggerV1Request": {
        "properties": {
          "cron_schedule": {
            "title": "Cron Schedule",
            "type": "string",
            "nullable": true
          },
          "suspend": {
            "title": "Suspend",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "title": "UpdateTriggerV1Request"
      },
      "UpdateTriggerV1Response": {
        "properties": {
          "success": {
            "type": "boolean",
            "title": "Success"
          }
        },
        "type": "object",
        "required": [
          "success"
        ],
        "title": "UpdateTriggerV1Response"
      },
      "UpdateVLLMModelEndpointRequest": {
        "properties": {
          "quantize": {
            "$ref": "#/components/schemas/Quantization",
            "nullable": true
          },
          "checkpoint_path": {
            "title": "Checkpoint Path",
            "type": "string",
            "nullable": true
          },
          "post_inference_hooks": {
            "title": "Post Inference Hooks",
            "items": {
              "type": "string"
            },
            "type": "array",
            "nullable": true
          },
          "cpus": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Cpus",
            "nullable": true
          },
          "gpus": {
            "title": "Gpus",
            "type": "integer",
            "nullable": true
          },
          "memory": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Memory",
            "nullable": true
          },
          "gpu_type": {
            "$ref": "#/components/schemas/GpuType",
            "nullable": true
          },
          "storage": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "integer"
              },
              {
                "type": "number"
              }
            ],
            "title": "Storage",
            "nullable": true
          },
          "nodes_per_worker": {
            "title": "Nodes Per Worker",
            "type": "integer",
            "nullable": true
          },
          "optimize_costs": {
            "title": "Optimize Costs",
            "type": "boolean",
            "nullable": true
          },
          "prewarm": {
            "title": "Prewarm",
            "type": "boolean",
            "nullable": true
          },
          "high_priority": {
            "title": "High Priority",
            "type": "boolean",
            "nullable": true
          },
          "billing_tags": {
            "title": "Billing Tags",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "default_callback_url": {
            "title": "Default Callback Url",
            "type": "string",
            "nullable": true
          },
          "default_callback_auth": {
            "$ref": "#/components/schemas/CallbackAuth",
            "nullable": true
          },
          "public_inference": {
            "title": "Public Inference",
            "default": true,
            "type": "boolean",
            "nullable": true
          },
          "chat_template_override": {
            "title": "Chat Template Override",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "enable_startup_metrics": {
            "title": "Enable Startup Metrics",
            "description": "Enable startup metrics collection via OpenTelemetry. When enabled, emits traces and metrics for download, Python init, and vLLM init phases.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "model_name": {
            "title": "Model Name",
            "type": "string",
            "nullable": true
          },
          "source": {
            "$ref": "#/components/schemas/LLMSource",
            "nullable": true
          },
          "inference_framework": {
            "type": "string",
            "const": "vllm",
            "title": "Inference Framework",
            "default": "vllm"
          },
          "inference_framework_image_tag": {
            "title": "Inference Framework Image Tag",
            "type": "string",
            "nullable": true
          },
          "num_shards": {
            "title": "Num Shards",
            "type": "integer",
            "nullable": true
          },
          "metadata": {
            "title": "Metadata",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "force_bundle_recreation": {
            "title": "Force Bundle Recreation",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "min_workers": {
            "title": "Min Workers",
            "type": "integer",
            "nullable": true
          },
          "max_workers": {
            "title": "Max Workers",
            "type": "integer",
            "nullable": true
          },
          "per_worker": {
            "title": "Per Worker",
            "type": "integer",
            "nullable": true
          },
          "labels": {
            "title": "Labels",
            "additionalProperties": {
              "type": "string"
            },
            "type": "object",
            "nullable": true
          },
          "max_gpu_memory_utilization": {
            "title": "Max Gpu Memory Utilization",
            "description": "Maximum GPU memory utilization for the batch inference. Default to 90%. Deprecated in favor of specifying this in VLLMModelConfig",
            "type": "number",
            "nullable": true
          },
          "attention_backend": {
            "title": "Attention Backend",
            "description": "Attention backend to use for vLLM. Default to None.",
            "type": "string",
            "nullable": true
          },
          "max_model_len": {
            "title": "Max Model Len",
            "description": "Model context length, If unspecified, will be automatically derived from the model config",
            "type": "integer",
            "nullable": true
          },
          "max_num_seqs": {
            "title": "Max Num Seqs",
            "description": "Maximum number of sequences per iteration",
            "type": "integer",
            "nullable": true
          },
          "enforce_eager": {
            "title": "Enforce Eager",
            "description": "Always use eager-mode PyTorch. If False, will use eager mode and CUDA graph in hybrid for maximal perforamnce and flexibility",
            "type": "boolean",
            "nullable": true
          },
          "trust_remote_code": {
            "title": "Trust Remote Code",
            "description": "Whether to trust remote code from Hugging face hub. This is only applicable to models whose code is not supported natively by the transformers library (e.g. deepseek). Default to False.",
            "default": false,
            "type": "boolean",
            "nullable": true
          },
          "pipeline_parallel_size": {
            "title": "Pipeline Parallel Size",
            "description": "Number of pipeline stages. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "tensor_parallel_size": {
            "title": "Tensor Parallel Size",
            "description": "Number of tensor parallel replicas. Default to None.",
            "type": "integer",
            "nullable": true
          },
          "quantization": {
            "title": "Quantization",
            "description": "Method used to quantize the weights. If None, we first check the `quantization_config` attribute in the model config file. If that is None, we assume the model weights are not quantized and use `dtype` to determine the data type of the weights.",
            "type": "string",
            "nullable": true
          },
          "disable_log_requests": {
            "title": "Disable Log Requests",
            "description": "Disable logging requests. Default to None.",
            "type": "boolean",
            "nullable": true
          },
          "chat_template": {
            "title": "Chat Template",
            "description": "A Jinja template to use for this endpoint. If not provided, will use the chat template from the checkpoint",
            "type": "string",
            "nullable": true
          },
          "tool_call_parser": {
            "title": "Tool Call Parser",
            "description": "Tool call parser",
            "type": "string",
            "nullable": true
          },
          "enable_auto_tool_choice": {
            "title": "Enable Auto Tool Choice",
            "description": "Enable auto tool choice",
            "type": "boolean",
            "nullable": true
          },
          "load_format": {
            "title": "Load Format",
            "description": "The format of the model weights to load.\n\n* \"auto\" will try to load the weights in the safetensors format and fall back to the pytorch bin format if safetensors format is not available.\n* \"pt\" will load the weights in the pytorch bin format.\n* \"safetensors\" will load the weights in the safetensors format.\n* \"npcache\" will load the weights in pytorch format and store a numpy cache to speed up the loading.\n* \"dummy\" will initialize the weights with random values, which is mainly for profiling.\n* \"tensorizer\" will load the weights using tensorizer from CoreWeave. See the Tensorize vLLM Model script in the Examples section for more information.\n* \"bitsandbytes\" will load the weights using bitsandbytes quantization.\n",
            "type": "string",
            "nullable": true
          },
          "config_format": {
            "title": "Config Format",
            "description": "The config format which shall be loaded.  Defaults to 'auto' which defaults to 'hf'.",
            "type": "string",
            "nullable": true
          },
          "tokenizer_mode": {
            "title": "Tokenizer Mode",
            "description": "Tokenizer mode. 'auto' will use the fast tokenizer ifavailable, 'slow' will always use the slow tokenizer, and'mistral' will always use the tokenizer from `mistral_common`.",
            "type": "string",
            "nullable": true
          },
          "limit_mm_per_prompt": {
            "title": "Limit Mm Per Prompt",
            "description": "Maximum number of data instances per modality per prompt. Only applicable for multimodal models.",
            "type": "string",
            "nullable": true
          },
          "max_num_batched_tokens": {
            "title": "Max Num Batched Tokens",
            "description": "Maximum number of batched tokens per iteration",
            "type": "integer",
            "nullable": true
          },
          "tokenizer": {
            "title": "Tokenizer",
            "description": "Name or path of the huggingface tokenizer to use.",
            "type": "string",
            "nullable": true
          },
          "dtype": {
            "title": "Dtype",
            "description": "Data type for model weights and activations. The 'auto' option will use FP16 precision for FP32 and FP16 models, and BF16 precision for BF16 models.",
            "type": "string",
            "nullable": true
          },
          "seed": {
            "title": "Seed",
            "description": "Random seed for reproducibility.",
            "type": "integer",
            "nullable": true
          },
          "revision": {
            "title": "Revision",
            "description": "The specific model version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "code_revision": {
            "title": "Code Revision",
            "description": "The specific revision to use for the model code on Hugging Face Hub. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "rope_scaling": {
            "title": "Rope Scaling",
            "description": "Dictionary containing the scaling configuration for the RoPE embeddings. When using this flag, don't update `max_position_embeddings` to the expected new maximum.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "tokenizer_revision": {
            "title": "Tokenizer Revision",
            "description": "The specific tokenizer version to use. It can be a branch name, a tag name, or a commit id. If unspecified, will use the default version.",
            "type": "string",
            "nullable": true
          },
          "quantization_param_path": {
            "title": "Quantization Param Path",
            "description": "Path to JSON file containing scaling factors. Used to load KV cache scaling factors into the model when KV cache type is FP8_E4M3 on ROCm (AMD GPU). In the future these will also be used to load activation and weight scaling factors when the model dtype is FP8_E4M3 on ROCm.",
            "type": "string",
            "nullable": true
          },
          "max_seq_len_to_capture": {
            "title": "Max Seq Len To Capture",
            "description": "Maximum sequence len covered by CUDA graphs. When a sequence has context length larger than this, we fall back to eager mode. Additionally for encoder-decoder models, if the sequence length of the encoder input is larger than this, we fall back to the eager mode.",
            "type": "integer",
            "nullable": true
          },
          "disable_sliding_window": {
            "title": "Disable Sliding Window",
            "description": "Whether to disable sliding window. If True, we will disable the sliding window functionality of the model. If the model does not support sliding window, this argument is ignored.",
            "type": "boolean",
            "nullable": true
          },
          "skip_tokenizer_init": {
            "title": "Skip Tokenizer Init",
            "description": "If true, skip initialization of tokenizer and detokenizer.",
            "type": "boolean",
            "nullable": true
          },
          "served_model_name": {
            "title": "Served Model Name",
            "description": "The model name used in metrics tag `model_name`, matches the model name exposed via the APIs. If multiple model names provided, the first name will be used. If not specified, the model name will be the same as `model`.",
            "type": "string",
            "nullable": true
          },
          "override_neuron_config": {
            "title": "Override Neuron Config",
            "description": "Initialize non default neuron config or override default neuron config that are specific to Neuron devices, this argument will be used to configure the neuron config that can not be gathered from the vllm arguments.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "mm_processor_kwargs": {
            "title": "Mm Processor Kwargs",
            "description": "Arguments to be forwarded to the model's processor for multi-modal data, e.g., image processor.",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "block_size": {
            "title": "Block Size",
            "description": "Size of a cache block in number of tokens.",
            "type": "integer",
            "nullable": true
          },
          "gpu_memory_utilization": {
            "title": "Gpu Memory Utilization",
            "description": "Fraction of GPU memory to use for the vLLM execution.",
            "type": "number",
            "nullable": true
          },
          "swap_space": {
            "title": "Swap Space",
            "description": "Size of the CPU swap space per GPU (in GiB).",
            "type": "number",
            "nullable": true
          },
          "cache_dtype": {
            "title": "Cache Dtype",
            "description": "Data type for kv cache storage.",
            "type": "string",
            "nullable": true
          },
          "num_gpu_blocks_override": {
            "title": "Num Gpu Blocks Override",
            "description": "Number of GPU blocks to use. This overrides the profiled num_gpu_blocks if specified. Does nothing if None.",
            "type": "integer",
            "nullable": true
          },
          "enable_prefix_caching": {
            "title": "Enable Prefix Caching",
            "description": "Enables automatic prefix caching.",
            "type": "boolean",
            "nullable": true
          }
        },
        "type": "object",
        "title": "UpdateVLLMModelEndpointRequest"
      },
      "UploadFileResponse": {
        "properties": {
          "id": {
            "type": "string",
            "title": "Id",
            "description": "ID of the uploaded file."
          }
        },
        "type": "object",
        "required": [
          "id"
        ],
        "title": "UploadFileResponse",
        "description": "Response object for uploading a file."
      },
      "UrlCitation": {
        "properties": {
          "end_index": {
            "type": "integer",
            "title": "End Index",
            "description": "The index of the last character of the URL citation in the message."
          },
          "start_index": {
            "type": "integer",
            "title": "Start Index",
            "description": "The index of the first character of the URL citation in the message."
          },
          "url": {
            "type": "string",
            "title": "Url",
            "description": "The URL of the web resource."
          },
          "title": {
            "type": "string",
            "title": "Title",
            "description": "The title of the web resource."
          }
        },
        "type": "object",
        "required": [
          "end_index",
          "start_index",
          "url",
          "title"
        ],
        "title": "UrlCitation"
      },
      "UserLocation": {
        "properties": {
          "type": {
            "type": "string",
            "const": "approximate",
            "title": "Type",
            "description": "The type of location approximation. Always `approximate`.\n"
          },
          "approximate": {
            "$ref": "#/components/schemas/WebSearchLocation"
          }
        },
        "type": "object",
        "required": [
          "type",
          "approximate"
        ],
        "title": "UserLocation"
      },
      "ValidationError": {
        "properties": {
          "loc": {
            "items": {
              "anyOf": [
                {
                  "type": "string"
                },
                {
                  "type": "integer"
                }
              ]
            },
            "type": "array",
            "title": "Location"
          },
          "msg": {
            "type": "string",
            "title": "Message"
          },
          "type": {
            "type": "string",
            "title": "Error Type"
          }
        },
        "type": "object",
        "required": [
          "loc",
          "msg",
          "type"
        ],
        "title": "ValidationError"
      },
      "VoiceIdsShared": {
        "anyOf": [
          {
            "type": "string"
          },
          {
            "type": "string",
            "enum": [
              "alloy",
              "ash",
              "ballad",
              "coral",
              "echo",
              "fable",
              "onyx",
              "nova",
              "sage",
              "shimmer",
              "verse"
            ]
          }
        ],
        "title": "VoiceIdsShared"
      },
      "WebSearchContextSize": {
        "type": "string",
        "enum": [
          "low",
          "medium",
          "high"
        ],
        "title": "WebSearchContextSize",
        "description": "High level guidance for the amount of context window space to use for the \nsearch. One of `low`, `medium`, or `high`. `medium` is the default.\n"
      },
      "WebSearchLocation": {
        "properties": {
          "country": {
            "title": "Country",
            "description": "The two-letter \n[ISO country code](https://en.wikipedia.org/wiki/ISO_3166-1) of the user,\ne.g. `US`.\n",
            "type": "string",
            "nullable": true
          },
          "region": {
            "title": "Region",
            "description": "Free text input for the region of the user, e.g. `California`.\n",
            "type": "string",
            "nullable": true
          },
          "city": {
            "title": "City",
            "description": "Free text input for the city of the user, e.g. `San Francisco`.\n",
            "type": "string",
            "nullable": true
          },
          "timezone": {
            "title": "Timezone",
            "description": "The [IANA timezone](https://timeapi.io/documentation/iana-timezones) \nof the user, e.g. `America/Los_Angeles`.\n",
            "type": "string",
            "nullable": true
          }
        },
        "type": "object",
        "title": "WebSearchLocation"
      },
      "WebSearchOptions": {
        "properties": {
          "user_location": {
            "description": "Approximate location parameters for the search.\n",
            "$ref": "#/components/schemas/UserLocation",
            "nullable": true
          },
          "search_context_size": {
            "$ref": "#/components/schemas/WebSearchContextSize",
            "nullable": true
          }
        },
        "type": "object",
        "title": "WebSearchOptions"
      },
      "ZipArtifactFlavor": {
        "properties": {
          "requirements": {
            "items": {
              "type": "string"
            },
            "type": "array",
            "title": "Requirements"
          },
          "framework": {
            "oneOf": [
              {
                "$ref": "#/components/schemas/PytorchFramework"
              },
              {
                "$ref": "#/components/schemas/TensorflowFramework"
              },
              {
                "$ref": "#/components/schemas/CustomFramework"
              }
            ],
            "title": "Framework",
            "discriminator": {
              "propertyName": "framework_type",
              "mapping": {
                "custom_base_image": "#/components/schemas/CustomFramework",
                "pytorch": "#/components/schemas/PytorchFramework",
                "tensorflow": "#/components/schemas/TensorflowFramework"
              }
            }
          },
          "app_config": {
            "title": "App Config",
            "additionalProperties": true,
            "type": "object",
            "nullable": true
          },
          "location": {
            "type": "string",
            "title": "Location"
          },
          "flavor": {
            "type": "string",
            "const": "zip_artifact",
            "title": "Flavor"
          },
          "load_predict_fn_module_path": {
            "type": "string",
            "title": "Load Predict Fn Module Path"
          },
          "load_model_fn_module_path": {
            "type": "string",
            "title": "Load Model Fn Module Path"
          }
        },
        "type": "object",
        "required": [
          "requirements",
          "framework",
          "location",
          "flavor",
          "load_predict_fn_module_path",
          "load_model_fn_module_path"
        ],
        "title": "ZipArtifactFlavor",
        "description": "This is the entity-layer class for the Model Bundle flavor of a zip artifact."
      },
      "CreateLLMModelEndpointV1Request": {
        "oneOf": [
          {
            "$ref": "#/components/schemas/CreateVLLMModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/CreateSGLangModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/CreateDeepSpeedModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/CreateTextGenerationInferenceModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/CreateLightLLMModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/CreateTensorRTLLMModelEndpointRequest"
          }
        ],
        "title": "RootModel[Annotated[Union[Annotated[CreateVLLMModelEndpointRequest, Tag], Annotated[CreateSGLangModelEndpointRequest, Tag], Annotated[CreateDeepSpeedModelEndpointRequest, Tag], Annotated[CreateTextGenerationInferenceModelEndpointRequest, Tag], Annotated[CreateLightLLMModelEndpointRequest, Tag], Annotated[CreateTensorRTLLMModelEndpointRequest, Tag]], Discriminator]]"
      },
      "UpdateLLMModelEndpointV1Request": {
        "oneOf": [
          {
            "$ref": "#/components/schemas/UpdateVLLMModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/UpdateSGLangModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/UpdateDeepSpeedModelEndpointRequest"
          },
          {
            "$ref": "#/components/schemas/UpdateTextGenerationInferenceModelEndpointRequest"
          }
        ],
        "title": "RootModel[Annotated[Union[Annotated[UpdateVLLMModelEndpointRequest, Tag], Annotated[UpdateSGLangModelEndpointRequest, Tag], Annotated[UpdateDeepSpeedModelEndpointRequest, Tag], Annotated[UpdateTextGenerationInferenceModelEndpointRequest, Tag]], Discriminator]]"
      }
    },
    "securitySchemes": {
      "HTTPBasic": {
        "type": "http",
        "scheme": "basic"
      },
      "OAuth2PasswordBearer": {
        "type": "oauth2",
        "flows": {
          "password": {
            "scopes": {},
            "tokenUrl": "token"
          }
        }
      }
    }
  }
}