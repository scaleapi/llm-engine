# Config to know where model-engine is running
gateway_namespace: default

# Config for Model Engine running in CircleCI
model_primitive_host: "none"

# Endpoint config
# K8s namespace the endpoints will be created in
endpoint_namespace: model-engine

# Asynchronous endpoints
# TODO: Try out localstack once e2e tests have been updated to use sqs as a broker_type
sqs_profile: nonexistent_sqs_profile
sqs_queue_policy_template: >
  {
    "Version": "2012-10-17",
    "Id": "__default_policy_ID",
    "Statement": [
      {
        "Sid": "__owner_statement",
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::000000000000:root"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:000000000000:${queue_name}"
      },
      {
        "Effect": "Allow",
        "Principal": {
          "AWS": "arn:aws:iam::000000000000:role/default"
        },
        "Action": "sqs:*",
        "Resource": "arn:aws:sqs:us-west-2:000000000000:${queue_name}"
      }
    ]
  }

sqs_queue_tag_template: >
  {
    "infra.scale.com/product": "MLInfraLaunchSQS",
    "infra.scale.com/team": "${team}",
    "infra.scale.com/contact": "yi.xu@scale.com",
    "infra.scale.com/customer": "AllCustomers",
    "infra.scale.com/financialOwner": "yi.xu@scale.com",
    "Launch-Endpoint-Id": "${endpoint_id}",
    "Launch-Endpoint-Name": "${endpoint_name}",
    "Launch-Endpoint-Created-By": "${endpoint_created_by}"
  }

# Billing
billing_queue_arn: none
# There's a separate piece of infra that caches k8s state onto redis, so we need a url to it
cache_redis_aws_url: redis://127.0.0.1:6379/15

cloud_file_llm_fine_tune_repository: "s3://model-engine-integration-tests/fine_tune_repository/circleci"

dd_trace_enabled: false
istio_enabled: true
sensitive_log_mode: false
tgi_repository: "text-generation-inference"
vllm_repository: "vllm"
lightllm_repository: "lightllm"
tensorrt_llm_repository: "tensorrt-llm"
batch_inference_vllm_repository: "llm-engine/batch-infer-vllm"
user_inference_base_repository: "launch/inference"
user_inference_pytorch_repository: "hosted-model-inference/async-pytorch"
user_inference_tensorflow_repository: "hosted-model-inference/async-tensorflow-cpu"
docker_image_layer_cache_repository: "kaniko-cache"

# S3 access
hf_user_fine_tuned_weights_prefix: "s3://test-bucket/model-weights"
